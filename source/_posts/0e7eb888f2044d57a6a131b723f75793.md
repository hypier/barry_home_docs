
---
categories: äººå·¥æ™ºèƒ½
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DGO3O6q34wYR4mrCnKIpUg.jpeg
date: '2024-06-19 04:01:35'
tags:
  - LangChain
  - GenAI
  - æœºå™¨å­¦ä¹ 
title: LangGraph FastAPI å’Œ StreamlitGradioLLM é©±åŠ¨åº”ç”¨çš„å®Œç¾ä¸‰é‡å¥

---


*åœ¨æˆ‘å¼€å§‹ä¹‹å‰ï¼Œå¿«é€Ÿæ’å…¥ä¸€ä¸ªğŸ”Œ :-)è¿™æ˜¯å…³äº LangChain åˆå­¦è€…ç³»åˆ—çš„ç¬¬ 5 ç¯‡åšå®¢ã€‚æˆ‘é€šè¿‡è¿™äº›å¸–å­è®°å½•æˆ‘çš„ GenAI å­¦ä¹ ï¼ˆé¡ºä¾¿æä¸€ä¸‹ï¼Œæˆ‘çš„èƒŒæ™¯æ˜¯åœŸæœ¨å·¥ç¨‹å’Œæœºå™¨å­¦ä¹ ï¼‰ã€‚æˆ‘æ‰¿è¯ºä¼šè®©å†…å®¹ç®€å•æ˜“æ‡‚ï¼Œé€‚åˆåƒæˆ‘ä¸€æ ·çš„åˆå­¦è€…ï¼Œå¹¶éšç€æ¯ä¸€ç¯‡æ–°å¸–å­é€æ­¥æ·±å…¥ã€‚é€šè¿‡è¿™ä¸ªç³»åˆ—ï¼Œä½ å°†èƒ½å¤Ÿä¸€æ­¥ä¸€æ­¥æ„å»ºè‡ªå·±çš„ GenAI åº”ç”¨ã€‚ç¥å­¦ä¹ æ„‰å¿«ï¼ğŸ¤—*



åœ¨æˆ‘å¼€å§‹ä¹‹å‰ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªèƒŒæ™¯æ•…äº‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ¥è‡ªåä¸º **Beyond the Speckleverse** çš„é»‘å®¢é©¬æ‹‰æ¾çš„é¡¹ç›®ï¼Œæˆ‘æœ¬æ‰“ç®—å‚åŠ ã€‚è¿™ä¸ªæ´»åŠ¨ç”± [Speckle](https://speckle.systems) ç»„ç»‡ï¼Œè¯¥å…¬å¸ä¸ºè½¯ä»¶å¼€å‘äººå‘˜å’Œå·¥ç¨‹å¸ˆ/å»ºç­‘å¸ˆæä¾›ä¸€ä¸ªå¹³å°ï¼Œä»¥ä¾¿ä»–ä»¬èƒ½å¤Ÿåä½œå¹¶ä¸º AECï¼ˆå»ºç­‘ã€å·¥ç¨‹å’Œæ–½å·¥ï¼‰è¡Œä¸šæ„å»ºå·¥å…·ã€‚æˆ‘ç›´åˆ°æœ€åä¸€æ™šæ‰å¼€å§‹ï¼Œå› ä¸ºæˆ‘æ— æ³•æƒ³å‡ºä¸€ä¸ªå¯ä»¥ä¸ Speckle å¹³å°é›†æˆçš„é…·é¡¹ç›®ã€‚å½“æˆ‘è¯•å›¾å­¦ä¹ å¼€å‘è€…æ–‡æ¡£æ—¶ï¼Œæˆ‘æ„è¯†åˆ°â€¦â€¦

> å¦‚æœæˆ‘ä»¬èƒ½åˆ›å»ºä¸€ä¸ª **AI ä»£ç åŠ©æ‰‹**ï¼Œèƒ½å¤Ÿæµè§ˆæ–‡æ¡£å¹¶æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢æ£€ç´¢ç­”æ¡ˆï¼Œé‚£å°†æ˜¯å¤šä¹ˆé…·å•Šã€‚è¿™ä¸ªé¡¹ç›®è¿˜å¯ä»¥è¿›ä¸€æ­¥å‘å±•æˆä¸€ä¸ªè°ƒè¯•åŠ©æ‰‹ï¼Œé€šè¿‡æŸ¥çœ‹ç¤¾åŒºè®ºå›æ¥å®ç°ã€‚

äºæ˜¯æˆ‘è¯•å›¾åœ¨ä¸€å¤œä¹‹é—´å®Œæˆæ•´ä¸ªè®¾ç½® ğŸ˜„ï¼å½“ç„¶æˆ‘æ²¡èƒ½å®Œæˆã€‚æˆ‘ä½ä¼°äº†æ‰‹å¤´çš„ä»»åŠ¡ï¼Œè¿‡é«˜ä¼°è®¡äº†æˆ‘çš„ç¼–ç èƒ½åŠ›ã€‚ä½†æ˜¯ï¼Œåœ¨æ¥ä¸‹æ¥çš„ä¸¤å¤©é‡Œï¼Œæˆ‘æˆåŠŸæ„å»ºäº†ä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„æœåŠ¡å™¨ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªç®€å•çš„ç”¨æˆ·ç•Œé¢æ¥ä¸æ–‡æ¡£è¿›è¡ŒèŠå¤©ã€‚è¿˜æœ‰æ›´å¤šçš„å·¥ä½œè¦åšï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ˜¯æˆ‘ç¬¬äº”ç¯‡åšå®¢çš„å®Œç¾å†…å®¹ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ ğŸ¬

# æ„å»ºæœåŠ¡å™¨-å®¢æˆ·ç«¯äº¤äº’

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hdFq10Hqz7iV2hvFXoZTRg.jpeg)

## TL;DR

è¿™æ˜¯ä¸€ç¯‡ç¨é•¿çš„æ–‡ç« ï¼Œå› ä¸ºå®ƒåŒ…å«ä¸€ä¸ªç«¯åˆ°ç«¯çš„é¡¹ç›®ï¼›åˆ›å»ºå›¾å½¢ç®¡é“ã€å¯åŠ¨æœåŠ¡å™¨ä»¥åŠåˆ›å»ºä¸æœåŠ¡å™¨äº¤äº’çš„å®¢æˆ·ç«¯ã€‚é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¹‹å‰åœ¨æœ¬åœ°æµ‹è¯•æ‚¨çš„é¡¹ç›®ã€‚æˆ‘ä»¬è¿˜å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ Streamlit å’Œ Gradio ä¸ºæˆ‘ä»¬çš„ä»£ç åŠ©æ‰‹åˆ›å»ºä¸€ä¸ªç®€å•çš„ç”¨æˆ·ç•Œé¢ã€‚é‚£ä¹ˆï¼ŒåºŸè¯ä¸å¤šè¯´ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼

# ç›®å½•

1. **å¯¼å…¥ API å¯†é’¥**
2. **åŠ è½½æ–‡æ¡£**
3. **åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨**
4. **åˆ›å»ºå“åº”ç”Ÿæˆçš„æ£€ç´¢é“¾**
5. **åˆ›å»ºè¯„åˆ†å™¨**
6. **åˆ›å»ºå›¾å½¢**
7. **ä½¿ç”¨ FastAPI å¯åŠ¨æœåŠ¡å™¨**
8. **åˆ›å»ºå¸¦æœ‰ Streamlit/Gradio UI çš„å®¢æˆ·ç«¯**

## ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥ API å¯†é’¥

è®©æˆ‘ä»¬å¼€å§‹ä» `.env` æ–‡ä»¶ä¸­å¯¼å…¥ API å¯†é’¥ã€‚å¯é€‰åœ°ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ Langsmith è®¾ç½®è¿½è¸ªã€‚

```python
import os
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv()) # important line if cannot load api key

## Getting the api keys from the .env file

os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')
os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')

# Langsmith Tracing
os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2')
os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')
os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')

# Fire Crawl API
os.environ['FIRE_API_KEY']=os.getenv('FIRE_API_KEY')
```
è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ `.env` æ–‡ä»¶ã€‚å¦‚æœæ‚¨æ²¡æœ‰ API å¯†é’¥ï¼Œè¯·è·å–ä¸€ä¸ªï¼Œå¹¶å°†å…¶ç²˜è´´åœ¨å­—ç¬¦ä¸²ä¹‹é—´ã€‚æˆ‘åœ¨æˆ‘çš„ [ç¬¬ä¸€ç¯‡åšå®¢æ–‡ç« ](https://readmedium.com/chatbots-with-langchain-a-gentle-introduction-with-python-62348fc2e5f1) ä¸­è¯¦ç»†æè¿°äº†è¿™ä¸€ç‚¹ã€‚

```python
OPENAI_API_KEY=''
LANGCHAIN_API_KEY=''
LANGCHAIN_TRACING_V2='true'
LANGCHAIN_ENDPOINT='https://api.smith.langchain.com'
LANGCHAIN_PROJECT=''
```

## ç¬¬2æ­¥ï¼šåŠ è½½æ–‡æ¡£

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåä¸º [FireCrawl](https://www.firecrawl.dev) çš„äº§å“ï¼Œå®ƒç”± [Mendable.ai](https://www.mendable.ai) åˆ›å»ºï¼Œå¯ä»¥å°†ç½‘ç«™è½¬æ¢ä¸ºé€‚åˆ LLM çš„æ–‡æ¡£ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„ã€‚æˆ‘ä»¬å°†çˆ¬å– Speckle çš„å¼€å‘è€…æ–‡æ¡£ï¼Œå¹¶å°†æ‰€æœ‰é¡µé¢å’Œå­é¡µé¢è½¬æ¢ä¸ºæ–‡æ¡£åˆ—è¡¨ã€‚æ‚¨éœ€è¦ä¸€ä¸ª API å¯†é’¥æ¥åœ¨åŠ è½½å‡½æ•°ä¸­ä½¿ç”¨ã€‚ä»…ä¾›å‚è€ƒï¼šæ‚¨å°†è·å¾— 500 ä¸ªå…è´¹ç§¯åˆ†ï¼ˆé¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘å·²ç»è¶…è¿‡äº†ï¼‰ï¼Œæ‰€ä»¥è¯·æ˜æ™ºåœ°ä½¿ç”¨ã€‚

æˆ‘åˆ›å»ºäº† `DocumentLoader` ç±»ï¼Œå®ƒæ¥å— API å¯†é’¥ä½œä¸ºå­—ç¬¦ä¸²è¾“å…¥ï¼Œå¹¶å…·æœ‰ä¸€ä¸ª `get_docs` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å— URL ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºåŒ…å«å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨ã€‚


```python
from typing import List
from langchain_community.document_loaders import FireCrawlLoader
from document import Document

class DocumentLoader:
    def __init__(self, api_key: str):
        self.api_key = api_key

    def get_docs(self, url: str) -> List[Document]:
        """
        Retrieves documents from the specified URL using the FireCrawlLoader.

        Args:
            url (str): The URL to crawl for documents.

        Returns:
            List[Document]: A list of Document objects containing the retrieved content.
        """
        loader = FireCrawlLoader(
            api_key=self.api_key, url=url, mode="crawl"
        )

        raw_docs = loader.load()
        docs = [Document(page_content=doc.page_content, metadata=doc.metadata) for doc in raw_docs]

        return docs
```
å°±æˆ‘è€Œè¨€ï¼Œæˆ‘å·²ç»çˆ¬å–äº†æ–‡æ¡£ï¼Œå¹¶å°†æ–‡æ¡£ä¿å­˜åœ¨æœ¬åœ°ï¼Œä»¥ä¾¿ä¸é‡å¤è¯¥è¿‡ç¨‹å¹¶æµªè´¹æˆ‘çš„ç§¯åˆ†ã€‚ç¬¬ä¸€æ¬¡æ‚¨å¯ä»¥ä½¿ç”¨ `get_docs` å‡½æ•°ï¼›å¦åˆ™æ‚¨å¯ä»¥åŠ è½½æ–‡æ¡£ã€‚


```python
import pickle

# Load the crawled saved docs from the local file
with open("crawled_docs/saved_docs.pkl", "rb") as f:
    saved_docs = pickle.load(f)
```

## ç¬¬ 3 æ­¥ï¼šåˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†æ–‡æ¡£ï¼Œæˆ‘ä»¬æƒ³å°†å®ƒä»¬åˆ†æˆæ›´å°çš„éƒ¨åˆ†ï¼Œå¹¶å°†åµŒå…¥å­˜å‚¨åœ¨å¼€æºå‘é‡å­˜å‚¨ä¸­ä»¥ä¾¿æ£€ç´¢ã€‚æˆ‘ä»¬å°†ä¾èµ– OpenAI åµŒå…¥æ¨¡å‹å’Œ FAISS å‘é‡å­˜å‚¨ã€‚å¯é€‰åœ°ï¼Œæ‚¨è¿˜å¯ä»¥æä¾›ä¸€ä¸ªè·¯å¾„ä»¥ä¾¿åœ¨æœ¬åœ°ä¿å­˜å‘é‡å­˜å‚¨ã€‚

```python
from typing import List, Optional
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_vector_store(docs, store_path: Optional[str] = None) -> FAISS:
    """
    Creates a FAISS vector store from a list of documents.

    Args:
        docs (List[Document]): A list of Document objects containing the content to be stored.
        store_path (Optional[str]): The path to store the vector store locally. If None, the vector store will not be stored.

    Returns:
        FAISS: The FAISS vector store containing the documents.
    """
    # Creating text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )

    texts = text_splitter.split_documents(docs)

    # Embedding object
    embedding_model = OpenAIEmbeddings()

    # Create the FAISS vector store
    store = FAISS.from_documents(texts, embedding_model)

    # Save the vector store locally if a path is provided
    if store_path:
        store.save_local(store_path)

    return store


# create vector store
store = create_vector_store(saved_docs)

# creating retriever
retriever = store.as_retriever()
```

## ç¬¬4æ­¥ï¼šåˆ›å»ºç”¨äºç”Ÿæˆå“åº”çš„æ£€ç´¢é“¾

ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ›å»º `create_generate_chain` å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†åˆ›å»ºä¸€ä¸ªç”Ÿæˆå“åº”çš„é“¾ã€‚ä¸ºäº†åˆ›å»ºè¿™ä¸ªé“¾ï¼Œæˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ `generate_template`ï¼Œåœ¨å…¶ä¸­æä¾›å…³äºè¯¥è¿‡ç¨‹çš„è¯¦ç»†è¯´æ˜ã€‚æ¨¡æ¿æœ‰ä¸¤ä¸ªå ä½ç¬¦ï¼š`{context}` ç”¨äºå­˜å‚¨ç›¸å…³ä¿¡æ¯ï¼Œ`{input}` ç”¨äºé—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LangChain çš„ PromptTemplate æ¨¡å—ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå˜é‡ï¼š`template = generate_template` å’Œ `input_variables = ["context", "input"]`ã€‚

æœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ `generate_prompt`ã€`llm` æ¨¡å‹å’Œ `StrOutputParser()` åˆ›å»º `generate_chain`ã€‚

```python
# generate_chain.py
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

def create_generate_chain(llm):
    """
    åˆ›å»ºä¸€ä¸ªç”¨äºå›ç­”ä»£ç ç›¸å…³é—®é¢˜çš„ç”Ÿæˆé“¾ã€‚

    å‚æ•°:
        llm (LLM): ç”¨äºç”Ÿæˆå“åº”çš„è¯­è¨€æ¨¡å‹ã€‚

    è¿”å›:
        ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸Šä¸‹æ–‡å’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²å“åº”ã€‚
    """
    generate_template = """
    ä½ æ˜¯ä¸€ä¸ªåä¸º Speckly çš„æœ‰ç”¨ä»£ç åŠ©æ‰‹ã€‚ç”¨æˆ·å‘ä½ æä¾›ä¸€ä¸ªä»£ç ç›¸å…³çš„é—®é¢˜ï¼Œå…¶å†…å®¹ç”±ä»¥ä¸‹ä¸Šä¸‹æ–‡éƒ¨åˆ†è¡¨ç¤ºï¼ˆä»¥<context></context>åˆ†éš”ï¼‰ã€‚
    ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚
    è¿™äº›æ–‡ä»¶æ¶‰åŠ Speckle å¼€å‘è€…æ–‡æ¡£ã€‚ä½ å¯ä»¥å‡è®¾ç”¨æˆ·æ˜¯åœŸæœ¨å·¥ç¨‹å¸ˆã€å»ºç­‘å¸ˆæˆ–è½¯ä»¶å¼€å‘äººå‘˜ã€‚
    å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚ä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
    å¦‚æœé—®é¢˜ä¸ä¸Šä¸‹æ–‡æ— å…³ï¼Œè¯·ç¤¼è²Œåœ°å›åº”ä½ åªå›ç­”ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜ã€‚
    å°½å¯èƒ½æä¾›è¯¦ç»†çš„ç­”æ¡ˆï¼Œå¹¶ç”Ÿæˆ Python ä»£ç ï¼ˆé»˜è®¤ï¼‰é™¤éç”¨æˆ·åœ¨é—®é¢˜ä¸­ç‰¹åˆ«æåˆ°å…¶ä»–è¯­è¨€ã€‚

    <context>
    {context}
    </context>

    <question>
    {input}
    </question>
    """

    generate_prompt = PromptTemplate(template=generate_template, input_variables=["context", "input"])

    # åˆ›å»ºç”Ÿæˆé“¾
    generate_chain = generate_prompt | llm | StrOutputParser()

    return generate_chain


# åˆ›å»ºç”Ÿæˆé“¾
generate_chain = create_generate_chain(llm)
```
ç¨ä½œåç¦»ã€‚è¯·æ³¨æ„ï¼Œ`StrOutputParser()` ç”¨äºä» LLM è·å–å­—ç¬¦ä¸²è¾“å‡ºï¼Œå¦åˆ™è¾“å‡ºå¯èƒ½ä¼šå¾ˆå¤æ‚ï¼Œä¾‹å¦‚ JSON æˆ–ç»“æ„åŒ–æ¶ˆæ¯å¯¹è±¡ï¼Œå¯èƒ½æ— æ³•ç›´æ¥ç”¨äºè¿›ä¸€æ­¥å¤„ç†æˆ–æ˜¾ç¤ºç»™ç”¨æˆ·ã€‚ä¾‹å¦‚ï¼Œæœªä½¿ç”¨ `StrOutputParser()` çš„è¾“å‡ºå¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
{
    "content": "è¿™æ˜¯æ¥è‡ª LLM çš„å“åº”ã€‚",
    "metadata": {
        "confidence": 0.8,
        "response_time": 0.5
    }
}
```
è€Œä½¿ç”¨ `StrOutputParser()` åï¼Œè¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
è¿™æ˜¯æ¥è‡ª LLM çš„å“åº”ã€‚
```

## ç¬¬5æ­¥ï¼šåˆ›å»ºè¯„åˆ†å™¨

åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸åŒçš„è¯„åˆ†å™¨ï¼Œä»¥è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„ç›¸å…³æ€§ã€è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆã€æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ˜¯è™šæ„çš„ï¼Œä»¥åŠåœ¨æœªè·å¾—ç›¸å…³æ–‡æ¡£æ—¶çš„æŸ¥è¯¢é‡å†™å™¨ã€‚æˆ‘ä»¬å°†é€æ­¥è¿›è¡Œæ¯ä¸€ä¸ªéƒ¨åˆ†ã€‚

**æ£€ç´¢è¯„åˆ†å™¨**

æˆ‘ä»¬å°†é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ£€ç´¢è¯„åˆ†å™¨ï¼Œä»¥è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª `create_retrieval_grader` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸€ä¸ªå¸¦æœ‰æ–°æŒ‡ä»¤çš„æç¤ºæ¨¡æ¿ `grade_prompt`ã€‚

å®ƒè¡¨ç¤ºè¯„åˆ†å™¨åº”åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…³é”®è¯ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„å…³é”®è¯ï¼Œåˆ™è¯¥æ–‡æ¡£è¢«è§†ä¸ºç›¸å…³ã€‚ç„¶åï¼Œå®ƒåº”æä¾›ä¸€ä¸ªäºŒå…ƒè¯„åˆ†ï¼Œå³â€œæ˜¯â€æˆ–â€œå¦â€ï¼Œä»¥æŒ‡ç¤ºæ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ï¼Œå¹¶ä»¥ JSON æ ¼å¼æä¾›ç»“æœï¼Œåªæœ‰ä¸€ä¸ªé”®â€œscoreâ€ã€‚

```python
def create_retrieval_grader(model):
    """
    åˆ›å»ºä¸€ä¸ªæ£€ç´¢è¯„åˆ†å™¨ï¼Œä»¥è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚

    è¿”å›ï¼š
        ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œæ¥å—æ–‡æ¡£å’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªäºŒå…ƒè¯„åˆ†ï¼ŒæŒ‡ç¤ºæ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚
    """
    grade_prompt = PromptTemplate(
        template="""
        <|begin_of_text|><|start_header_id|>system<|end_header_id|>
        ä½ æ˜¯ä¸€ä¸ªè¯„åˆ†å™¨ï¼Œè¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§ã€‚å¦‚æœæ–‡æ¡£åŒ…å«ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…³é”®è¯ï¼Œåˆ™å°†å…¶è¯„åˆ†ä¸ºç›¸å…³ã€‚å®ƒä¸éœ€è¦æ˜¯ä¸¥æ ¼çš„æµ‹è¯•ã€‚ç›®æ ‡æ˜¯è¿‡æ»¤æ‰é”™è¯¯çš„æ£€ç´¢ç»“æœã€‚
        ç»™å‡ºä¸€ä¸ªäºŒå…ƒè¯„åˆ† 'yes' æˆ– 'no'ï¼Œä»¥æŒ‡ç¤ºæ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚
        å°†äºŒå…ƒè¯„åˆ†ä»¥ JSON æ ¼å¼æä¾›ï¼Œåªæœ‰ä¸€ä¸ªé”® 'score'ï¼Œæ²¡æœ‰å‰è¨€æˆ–è§£é‡Šã€‚
        <|eot_id|>
        <|start_header_id|>user<|end_header_id|>

        è¿™æ˜¯æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š \n\n {document} \n\n
        è¿™æ˜¯ç”¨æˆ·é—®é¢˜ï¼š {input} \n
        <|eot_id|>
        <|start_header_id|>assistant<|end_header_id|>
        """,
        input_variables=["document", "input"],
    )

    # åˆ›å»ºæ£€ç´¢å™¨é“¾
    retriever_grader = grade_prompt | model | JsonOutputParser()

    return retriever_grader
```
ä¾‹å¦‚ï¼š

```python
model = ... # åœ¨æ­¤æä¾›ä½ çš„ llm
grader = create_retrieval_grader(model)

document = "æ³•å›½æ˜¯ä¸€ä¸ªä½äºæ¬§æ´²çš„å›½å®¶ã€‚å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ã€‚"
question = "æ³•å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
score = grader(document, question)
print(score)  # è¾“å‡º: {"score": "yes"}
```
**è™šæ„è¯„åˆ†å™¨**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªè™šæ„è¯„åˆ†å™¨ï¼Œä»¥è¯„ä¼°ä» LLM è·å¾—çš„ç­”æ¡ˆæ˜¯å¦åŸºäºæˆ–å¾—åˆ°ä¸€ç»„äº‹å®çš„æ”¯æŒã€‚ç„¶åï¼Œå®ƒæä¾›ä¸€ä¸ªäºŒå…ƒè¯„åˆ†ï¼ˆâ€œæ˜¯â€æˆ–â€œå¦â€ï¼‰ï¼ŒæŒ‡ç¤ºç­”æ¡ˆæ˜¯å¦æœ‰ä¾æ®ã€‚æç¤ºæ¨¡æ¿å°†åŒ…æ‹¬äº‹å®çš„å ä½ç¬¦ï¼ˆ`{documents}`ï¼‰å’Œç­”æ¡ˆçš„å ä½ç¬¦ï¼ˆ`{generation}`ï¼‰ï¼Œåœ¨ä½¿ç”¨æç¤ºæ—¶å°†å¡«å……è¿™äº›å ä½ç¬¦ã€‚

```python
def create_hallucination_grader(self):
    """
    åˆ›å»ºä¸€ä¸ªè™šæ„è¯„åˆ†å™¨ï¼Œä»¥è¯„ä¼°ç­”æ¡ˆæ˜¯å¦åŸºäº/å¾—åˆ°ä¸€ç»„äº‹å®çš„æ”¯æŒã€‚

    è¿”å›ï¼š
        ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªç”Ÿæˆçš„ç­”æ¡ˆå’Œä¸€ç»„æ–‡æ¡£ï¼ˆäº‹å®ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªäºŒå…ƒè¯„åˆ†ï¼ŒæŒ‡ç¤ºç­”æ¡ˆæ˜¯å¦åŸºäº/å¾—åˆ°äº‹å®çš„æ”¯æŒã€‚
    """
    hallucination_prompt = PromptTemplate(
        template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
        ä½ æ˜¯ä¸€ä¸ªè¯„åˆ†å™¨ï¼Œè¯„ä¼°ç­”æ¡ˆæ˜¯å¦åŸºäº/å¾—åˆ°ä¸€ç»„äº‹å®çš„æ”¯æŒã€‚ç»™å‡ºä¸€ä¸ªäºŒå…ƒè¯„åˆ† 'yes' æˆ– 'no'ï¼Œä»¥æŒ‡ç¤ºç­”æ¡ˆæ˜¯å¦åŸºäº/å¾—åˆ°ä¸€ç»„äº‹å®çš„æ”¯æŒã€‚å°†äºŒå…ƒè¯„åˆ†ä»¥ JSON æ ¼å¼æä¾›ï¼Œåªæœ‰ä¸€ä¸ªé”® 'score'ï¼Œæ²¡æœ‰å‰è¨€æˆ–è§£é‡Šã€‚
        <|eot_id|>
        <|start_header_id|>user<|end_header_id|>
        è¿™é‡Œæ˜¯äº‹å®ï¼š
        \n ------- \n
        {documents}
        \n ------- \n
        è¿™æ˜¯ç­”æ¡ˆï¼š {generation}
        <|eot_id|>
        <|start_header_id|>assistant<|end_header_id|>""",
        input_variables=["generation", "documents"],
    )

    hallucination_grader = hallucination_prompt | self.model | JsonOutputParser()

    return hallucination_grader
```
ä¾‹å¦‚ï¼š

```python
from langchain_openai import ChatOpenAI

## LLM æ¨¡å‹
model = ChatOpenAI(model="gpt-4o", temperature=0)
## è¯„åˆ†å™¨
grader = create_hallucination_grader(model)
answer = "æ³•å›½çš„é¦–éƒ½ä¸ºå·´é»ã€‚"
facts = ["æ³•å›½æ˜¯ä¸€ä¸ªä½äºæ¬§æ´²çš„å›½å®¶ã€‚", "å·´é»æ˜¯æ³•å›½çš„é¦–éƒ½ã€‚"]
score = grader(answer, facts)
print(score)  # è¾“å‡º: {"score": "yes"}
```
**ä»£ç è¯„ä¼°å™¨**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª `create_code_evaluator` å‡½æ•°ï¼Œåˆ›å»ºä¸€ä¸ªä»£ç è¯„ä¼°å™¨ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„ä»£ç æ˜¯å¦æ­£ç¡®ä¸”ä¸ç»™å®šé—®é¢˜ç›¸å…³ã€‚å®ƒä½¿ç”¨ä¸€ä¸ª `PromptTemplate` æ¥æŒ‡ç¤ºè¯„ä¼°å™¨æä¾›ä¸€ä¸ª JSON å“åº”ï¼ŒåŒ…å«ä¸€ä¸ªäºŒå…ƒè¯„åˆ†å’Œåé¦ˆã€‚è¯„ä¼°å™¨æ¥å—ä¸€ä¸ªç”Ÿæˆçš„ä»£ç ã€ä¸€ä¸ªé—®é¢˜å’Œä¸€ç»„æ–‡æ¡£ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªè¯„åˆ†ï¼ŒæŒ‡ç¤ºä»£ç æ˜¯å¦æ­£ç¡®ä¸”ç›¸å…³ï¼Œä»¥åŠå¯¹è¯„ä¼°çš„ç®€è¦è¯´æ˜ã€‚

```python
def create_code_evaluator(self):
    """
    åˆ›å»ºä¸€ä¸ªä»£ç è¯„ä¼°å™¨ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„ä»£ç æ˜¯å¦æ­£ç¡®ä¸”ä¸ç»™å®šé—®é¢˜ç›¸å…³ã€‚

    è¿”å›ï¼š
        ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªç”Ÿæˆçš„ä»£ç ã€ä¸€ä¸ªé—®é¢˜å’Œä¸€ç»„æ–‡æ¡£ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªäºŒå…ƒè¯„åˆ†å’Œåé¦ˆã€‚
    """
    eval_template = PromptTemplate(
        template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> ä½ æ˜¯ä¸€ä¸ªä»£ç è¯„ä¼°å™¨ï¼Œè¯„ä¼°ç”Ÿæˆçš„ä»£ç æ˜¯å¦æ­£ç¡®ä¸”ä¸ç»™å®šé—®é¢˜ç›¸å…³ã€‚
        æä¾›ä¸€ä¸ª JSON å“åº”ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š

        'score': ä¸€ä¸ªäºŒå…ƒè¯„åˆ† 'yes' æˆ– 'no'ï¼ŒæŒ‡ç¤ºä»£ç æ˜¯å¦æ­£ç¡®ä¸”ç›¸å…³ã€‚
        'feedback': å¯¹ä½ çš„è¯„ä¼°çš„ç®€è¦è¯´æ˜ï¼ŒåŒ…æ‹¬ä»»ä½•é—®é¢˜æˆ–æ”¹è¿›å»ºè®®ã€‚

        <|eot_id|><|start_header_id|>user<|end_header_id|>
        è¿™æ˜¯ç”Ÿæˆçš„ä»£ç ï¼š
        \n ------- \n
        {generation}
        \n ------- \n
        è¿™æ˜¯é—®é¢˜ï¼š {input}
        \n ------- \n
        è¿™æ˜¯ç›¸å…³æ–‡æ¡£ï¼š {documents}
        <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
        input_variables=["generation", "input", "documents"],
    )

    code_evaluator = eval_template | self.model | JsonOutputParser()

    return code_evaluator
```
ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ç”¨æ³•ï¼š

```python
model = ...  # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹

code_evaluator = create_code_evaluator(model)

code = "def greet(name): return f'Hello, {name}!'"
question = "å†™ä¸€ä¸ªå‡½æ•°æ¥æ ¹æ®åå­—é—®å€™æŸäººã€‚"
documents = ["ä¸€ä¸ªå‡½æ•°åº”è¯¥æ¥å—ä¸€ä¸ªåå­—ä½œä¸ºè¾“å…¥å¹¶è¿”å›ä¸€ä¸ªé—®å€™æ¶ˆæ¯ã€‚"]

result = code_evaluator(code, question, documents)
print(result)  # è¾“å‡º: {"score": "yes", "feedback": "ä»£ç æ­£ç¡®ä¸”ä¸é—®é¢˜ç›¸å…³ã€‚"}
```
**é—®é¢˜é‡å†™å™¨**

æœ€åï¼Œæˆ‘ä»¬å°†åˆ›å»º `create_question_rewriter` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ„å»ºä¸€ä¸ªé‡å†™é“¾ï¼Œä»¥æ”¹è¿›ç»™å®šé—®é¢˜çš„æ¸…æ™°åº¦å’Œç›¸å…³æ€§ã€‚æ­¤å‡½æ•°è¿”å›ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†é‡å†™çš„é—®é¢˜ä½œä¸ºå­—ç¬¦ä¸²è¾“å‡ºã€‚

```python
def create_question_rewriter(model):
    """
    åˆ›å»ºä¸€ä¸ªé—®é¢˜é‡å†™é“¾ï¼Œä»¥é‡å†™ç»™å®šé—®é¢˜ä»¥æé«˜å…¶æ¸…æ™°åº¦å’Œç›¸å…³æ€§ã€‚

    è¿”å›ï¼š
        ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›é‡å†™çš„é—®é¢˜ä½œä¸ºå­—ç¬¦ä¸²ã€‚
    """
    re_write_prompt = hub.pull("efriis/self-rag-question-rewriter")
    question_rewriter = re_write_prompt | self.model | StrOutputParser()

    return question_rewriter
        
```

```python
rewriter = create_question_rewriter()
original_question = "å¦‚ä½•ä½¿ç”¨ speckle çš„ python sdkï¼Ÿ"
rewritten_question = rewriter(original_question)
print(rewritten_question)  # è¾“å‡º: "å¦‚ä½•å®‰è£… speckle çš„ python sdkï¼Ÿ"
```
ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†è¿™äº›ç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è¿™äº›å‡½æ•°çš„ç±» `GraderUtils`ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æˆ‘ä»¬çš„ LLM æ¨¡å‹åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹ï¼Œå› ä¸ºè¿™æ˜¯å”¯ä¸€å¿…è¦çš„è¾“å…¥ã€‚

```python
from langchain_openai import ChatOpenAI

class GraderUtils:
    def __init__(self, model):
        self.model = model

    def create_retrieval_grader(self):
          ...
        
    def create_hallucination_grader(self):
          ...
        
    def create_code_evaluator(self):
          ... 

    def create_question_rewriter(self):
          ...

## LLM æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# åˆ›å»º GraderUtils ç±»çš„å®ä¾‹
grader = GraderUtils(llm)

# è·å–æ£€ç´¢è¯„åˆ†å™¨
retrieval_grader = grader.create_retrieval_grader()

# è·å–è™šæ„è¯„åˆ†å™¨
hallucination_grader = grader.create_hallucination_grader()

# è·å–ä»£ç è¯„ä¼°å™¨
code_evaluator = grader.create_code_evaluator()

# è·å–é—®é¢˜é‡å†™å™¨
question_rewriter = grader.create_question_rewriter()
```

> æ¬²äº†è§£æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹è¿™äº›æ¥è‡ª langchain-ai ä»“åº“çš„ [RAG ç¬”è®°æœ¬](https://github.com/langchain-ai/langgraph/tree/main/examples/rag)ã€‚è¿˜æœ‰å¦ä¸€ç¯‡å¾ˆæ£’çš„ [æ–‡ç« ](https://readmedium.com/building-a-rag-agent-with-langgraph-llama3-70b-and-scaling-with-amazon-bedrock-2be03fb4088b)ï¼Œç”± [Philipp Kaindl](https://medium.com/@philippkai) æ’°å†™ï¼Œè§£é‡Šäº†é«˜çº§ RAG æŠ€æœ¯ä»¥åŠä¸ AWS bedrock çš„éƒ¨ç½²ã€‚

## ç¬¬6æ­¥ï¼šåˆ›å»ºå›¾å½¢

ç°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰æ‰€æœ‰ç»„ä»¶ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ LangGraph åˆ›å»ºæˆ‘ä»¬çš„å›¾å½¢ã€‚åœ¨æˆ‘ä¹‹å‰çš„ [åšå®¢æ–‡ç« ](https://readmedium.com/designing-rag-agent-workflow-with-langgraph-861b002d9380) ä¸­ï¼Œæˆ‘è¯¦ç»†ä»‹ç»äº†å›¾å½¢å·¥ä½œæµçš„æ ¸å¿ƒæ¦‚å¿µã€‚åœ¨æ­¤ï¼Œæˆ‘å‡è®¾æ‚¨å…·å¤‡å¿…è¦çš„å·¥ä½œçŸ¥è¯†ã€‚

**å®šä¹‰å›¾å½¢çš„çŠ¶æ€**

æœ€åˆï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª `GraphState` ç±»ï¼Œè¯¥ç±»å®šä¹‰äº†å›¾å½¢çš„çŠ¶æ€ï¼Œç”±ä¸‰ä¸ªå…³é”®å±æ€§ç»„æˆï¼š`input`ã€`generation` å’Œ `documents`ã€‚`input` å±æ€§ä¿å­˜ä½œä¸ºå­—ç¬¦ä¸²å¤„ç†çš„è¾“å…¥æˆ–é—®é¢˜ï¼Œè€Œ `generation` å±æ€§å­˜å‚¨åŸºäºè¾“å…¥çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾“å‡ºï¼ŒåŒæ ·ä¸ºå­—ç¬¦ä¸²ã€‚`documents` å±æ€§è¡¨ç¤ºç›¸å…³æ–‡æ¡£çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

```python
## Start the Graph
from typing_extensions import TypedDict
from typing import List

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents
    """

    input: str
    generation: str
    documents: str #List[str]
```
è¯¥çŠ¶æ€åœ¨æ•´ä¸ªå›¾å½¢ä¸­å…¨å±€å¯è®¿é—®ï¼Œè¿™äº›å±æ€§æ˜¯å”¯ä¸€å¯ä»¥è¢«èŠ‚ç‚¹å†…çš„å‡½æ•°ä¿®æ”¹çš„å˜é‡ã€‚è¿™å°†å¼•å¯¼æˆ‘ä»¬å®šä¹‰èŠ‚ç‚¹ã€‚

**èŠ‚ç‚¹**

èŠ‚ç‚¹å¯ä»¥æ˜¯ Python å‡½æ•°ï¼Œè¿™äº›å‡½æ•°å°†è·å–å›¾å½¢çš„çŠ¶æ€ï¼Œæ‰§è¡Œä¸€äº›æ“ä½œï¼Œå¹¶ä¿®æ”¹ä»»ä½•çŠ¶æ€å˜é‡ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåä¸º `GraphNodes` çš„ç±»ã€‚åœ¨å½“å‰ç›®å½•ä¸­ï¼Œ`utils` æ–‡ä»¶å¤¹åŒ…å«æ‰€æœ‰æ¨¡å—ï¼Œå› æ­¤æˆ‘ä»¬å°†ä» `utils.generate_chain` å¯¼å…¥ `create_generate_chain` å‡½æ•°ä½œä¸ºæ¨¡å—ã€‚

```python
from document import Document
from utils.generate_chain import create_generate_chain

class GraphNodes:
    def __init__(self, llm, retriever, retrieval_grader, hallucination_grader, code_evaluator, question_rewriter):
        self.llm = llm
        self.retriever = retriever
        self.retrieval_grader = retrieval_grader
        self.hallucination_grader = hallucination_grader
        self.code_evaluator = code_evaluator
        self.question_rewriter = question_rewriter
        self.generate_chain = create_generate_chain(llm)

    def retrieve(self, state):
        """
        Retrieve documents

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): New key added to state, documents, that contains retrieved documents
        """
        print("---RETRIEVE---")
        question = state["input"]

        # Retrieval
        documents = self.retriever.invoke(question)
        return {"documents": documents, "input": question}

    def generate(self, state):
        """
        Generate answer

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): New key added to state, generation, that contains LLM generation
        """
        print("---GENERATE---")
        question = state["input"]
        documents = state["documents"]

        # RAG generation
        generation = self.generate_chain.invoke({"context": documents, "input": question})
        return {"documents": documents, "input": question, "generation": generation}

    def grade_documents(self, state):
        """
        Determines whether the retrieved documents are relevant to the question.

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): Updates documents key with only filtered relevant documents
        """
        print("---CHECK DOCUMENT RELEVANCE TO QUESTION---")
        question = state["input"]
        documents = state["documents"]

        # score each doc
        filtered_docs = []

        for d in documents:
            score = self.retrieval_grader.invoke({"input": question, "document": d.page_content})
            grade = score["score"]
            if grade == "yes":
                print("---GRADE: DOCUMENT RELEVANT---")
                filtered_docs.append(d)
            else:
                print("---GRADE: DOCUMENT IR-RELEVANT---")
                continue

        return {"documents": filtered_docs, "input": question}

    def transform_query(self, state):
        """
        Transform the query to produce a better question.

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): Updates question key with a re-phrased question
        """
        print("---TRANSFORM QUERY---")
        question = state["input"]
        documents = state["documents"]

        # Re-write question
        better_question = self.question_rewriter.invoke({"input": question})
        return {"documents": documents, "input": better_question}
```
è¯¥ç±»å®šä¹‰äº†å›¾å½¢çš„èŠ‚ç‚¹å‡½æ•°ï¼Œè´Ÿè´£å›¾å½¢å·¥ä½œæµä¸­çš„å„ç§ä»»åŠ¡ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸ªå‡½æ•°çš„æè¿°ï¼š

1. `retrieve`ï¼šæ ¹æ®è¾“å…¥é—®é¢˜æ£€ç´¢æ–‡æ¡£ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å›¾å½¢çŠ¶æ€ä¸­ã€‚
2. `generate`ï¼šä½¿ç”¨è¾“å…¥é—®é¢˜å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼Œå¹¶å°†ç”Ÿæˆç»“æœæ·»åŠ åˆ°å›¾å½¢çŠ¶æ€ä¸­ã€‚
3. `grade_documents`ï¼šæ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸è¾“å…¥é—®é¢˜çš„ç›¸å…³æ€§è¿›è¡Œè¿‡æ»¤ï¼Œæ›´æ–°å›¾å½¢çŠ¶æ€ï¼Œä»…ä¿ç•™ç›¸å…³æ–‡æ¡£ã€‚
4. `transform_query`ï¼šé‡æ–°è¡¨è¿°è¾“å…¥é—®é¢˜ï¼Œä»¥æé«˜å…¶æ¸…æ™°åº¦å’Œç›¸å…³æ€§ï¼Œæ›´æ–°å›¾å½¢çŠ¶æ€ä¸­çš„è½¬æ¢é—®é¢˜ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰ `EdgeGraph` ç±»ï¼Œè¯¥ç±»å®šä¹‰äº†å›¾å½¢çš„è¾¹å‡½æ•°ã€‚

**è¾¹**

è¾¹å‡½æ•°å¼•å¯¼å›¾å½¢å¤„ç†ç®¡é“ï¼Œæ ¹æ®å½“å‰çŠ¶æ€å’Œå„ç§èŠ‚ç‚¹å‡½æ•°çš„ç»“æœåšå‡ºå†³ç­–ã€‚

```python
class EdgeGraph:
    def __init__(self, hallucination_grader, code_evaluator):
        self.hallucination_grader = hallucination_grader
        self.code_evaluator = code_evaluator

    def decide_to_generate(self, state):
        """
        Determines whether to generate an answer, or re-generate a question.

        Args:
            state (dict): The current graph state

        Returns:
            str: Binary decision for next node to call
        """
        print("---ASSESS GRADED DOCUMENTS---")
        question = state["input"]
        filtered_documents = state["documents"]

        if not filtered_documents:
            # All documents have been filtered check_relevance
            # We will re-generate a new query
            print("---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---")
            return "transform_query"  # "retrieve_from_community_page", "transform_query"
        else:
            # We have relevant documents, so generate answer
            print("---DECISION: GENERATE---")
            return "generate"

    def grade_generation_v_documents_and_question(self, state):
        """
        Determines whether the generation is grounded in the document and answers question.

        Args:
            state (dict): The current graph state

        Returns:
            str: Decision for next node to call
        """
        print("---CHECK HALLUCINATIONS---")
        question = state["input"]
        documents = state["documents"]
        generation = state["generation"]

        score = self.hallucination_grader.invoke({"documents": documents, "generation": generation})
        grade = score["score"]

        # Check hallucination
        if grade == "yes":
            print("---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---")
            # Check question-answering
            print("---GRADE GENERATION vs QUESTION---")
            score = self.code_evaluator.invoke({"input": question, "generation": generation, "documents": documents})
            grade = score["score"]
            if grade == "yes":
                print("---DECISION: GENERATION ADDRESSES QUESTION---")
                return "useful"
            else:
                print("---DECISION: GENERATION DOES NOT ADDRESS QUESTION---")
                return "not useful"
        else:
            print("---DECISION: GENERATIONS ARE HALLUCINATED, RE-TRY---")
            return "not supported"
```
ä»¥ä¸‹æ˜¯æ¯ä¸ªå‡½æ•°çš„æè¿°ï¼š

1. `decide_to_generate`ï¼šæ ¹æ®è¿‡æ»¤æ–‡æ¡£ä¸è¾“å…¥é—®é¢˜çš„ç›¸å…³æ€§ï¼Œå†³å®šæ˜¯ç”Ÿæˆç­”æ¡ˆè¿˜æ˜¯é‡æ–°ç”Ÿæˆé—®é¢˜ã€‚å¦‚æœæ‰€æœ‰æ–‡æ¡£éƒ½ä¸ç›¸å…³ï¼Œåˆ™å†³å®šè½¬æ¢æŸ¥è¯¢ï¼›å¦åˆ™ï¼Œå†³å®šç”Ÿæˆç­”æ¡ˆã€‚
2. `grade_generation_v_documents_and_question`ï¼šæ ¹æ®ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦åŸºäºæ–‡æ¡£ä»¥åŠæ˜¯å¦èƒ½å¤Ÿè§£å†³é—®é¢˜æ¥è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆã€‚å¦‚æœç”Ÿæˆæ˜¯åŸºäºæ–‡æ¡£å¹¶è§£å†³äº†é—®é¢˜ï¼Œåˆ™è¢«è§†ä¸ºæœ‰ç”¨ï¼›å¦åˆ™ï¼Œè§†ä¸ºä¸æ”¯æŒæˆ–æ— ç”¨ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†å›¾å½¢çŠ¶æ€ã€èŠ‚ç‚¹å’Œè¾¹å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æœ€ç»ˆå¼€å§‹æ„å»ºæˆ‘ä»¬çš„å›¾å½¢ã€‚

**æ„å»ºå›¾å½¢**

```python
# Initiating the Graph
workflow = StateGraph(GraphState)
# Create an instance of the GraphNodes class
graph_nodes = GraphNodes(llm, retriever, retrieval_grader, hallucination_grader, code_evaluator, question_rewriter)
# Create an instance of the EdgeGraph class
edge_graph = EdgeGraph(hallucination_grader, code_evaluator)
# Define the nodes
workflow.add_node("retrieve", graph_nodes.retrieve) # retrieve documents
workflow.add_node("grade_documents", graph_nodes.grade_documents)  # grade documents
workflow.add_node("generate", graph_nodes.generate) # generate answers
workflow.add_node("transform_query", graph_nodes.transform_query)  # transform_query
# Build graph
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    edge_graph.decide_to_generate,
    {
        "transform_query": "transform_query", # "transform_query": "transform_query",
        "generate": "generate",
    },
)
workflow.add_edge("transform_query", "retrieve")
workflow.add_conditional_edges(
    "generate",
    edge_graph.grade_generation_v_documents_and_question,
    {
        "not supported": "generate",
        "useful": END,
        "not useful": "transform_query", # "transform_query"
    },
)
# Compile
chain = workflow.compile()
```
é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»å·²ç»å®šä¹‰çš„ `StateGraph` ç±»åˆå§‹åŒ–å›¾å½¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»º `graph_nodes` å’Œ `edge_graph` å®ä¾‹ï¼Œåˆ†åˆ«æ¥è‡ª `GraphNodes` å’Œ `EdgeGraph` ç±»ã€‚

ç„¶åï¼Œæˆ‘ä»¬å°†æ·»åŠ å·²ç»å®šä¹‰äº†å‡½æ•°çš„èŠ‚ç‚¹ï¼š

1. **Retrieve**: æ ¹æ®è¾“å…¥é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚
2. **Grade Documents**: æ ¹æ®æ–‡æ¡£ä¸é—®é¢˜çš„ç›¸å…³æ€§è¿‡æ»¤æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚
3. **Generate**: æ ¹æ®è¿‡æ»¤åçš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆã€‚
4. **Transform Query**: è½¬æ¢è¾“å…¥é—®é¢˜ä»¥æé«˜å…¶æ¸…æ™°åº¦å’Œç›¸å…³æ€§ã€‚

å›¾çš„èµ·ç‚¹åœ¨ `retrieve` èŠ‚ç‚¹ã€‚`retrieve` å’Œ `grade_documents` èŠ‚ç‚¹ä¹‹é—´æœ‰ä¸€æ¡æ™®é€šè¾¹ã€‚åœ¨ `grade_documents` èŠ‚ç‚¹ä¹‹åï¼Œå·¥ä½œæµç¨‹åˆ°è¾¾ä¸€ä¸ªæ¡ä»¶è¾¹ã€‚è°ƒç”¨ `edge_graph.decide_to_generate` å‡½æ•°æ¥ç¡®å®šå·¥ä½œæµç¨‹çš„ä¸‹ä¸€æ­¥ã€‚è¯¥å‡½æ•°è¯„ä¼°å·²è¯„åˆ†çš„æ–‡æ¡£ï¼Œå¹¶å†³å®šæ˜¯è½¬æ¢æŸ¥è¯¢è¿˜æ˜¯ç”Ÿæˆç­”æ¡ˆã€‚å¦‚æœå‡½æ•°è¿”å› `"transform_query"`ï¼Œå·¥ä½œæµç¨‹å°†ç§»åŠ¨åˆ° `transform_query` èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹è½¬æ¢è¾“å…¥é—®é¢˜ä»¥æé«˜å…¶æ¸…æ™°åº¦å’Œç›¸å…³æ€§ã€‚å¦‚æœå‡½æ•°è¿”å› `"generate"`ï¼Œå·¥ä½œæµç¨‹å°†ç§»åŠ¨åˆ° `generate` èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹æ ¹æ®è¿‡æ»¤åçš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆã€‚

`transform_query` å’Œ `retrieve` ä¹‹é—´ä¹Ÿæœ‰ä¸€æ¡æ™®é€šè¾¹ã€‚è¿™æ˜¯å› ä¸ºåœ¨æŸ¥è¯¢è¢«è½¬æ¢åï¼Œå·¥ä½œæµç¨‹ä¼šè¿”å›åˆ° `retrieve` èŠ‚ç‚¹ï¼Œä»¥æ ¹æ®è½¬æ¢åçš„æŸ¥è¯¢æ£€ç´¢æ–°æ–‡æ¡£ã€‚

ç”Ÿæˆç­”æ¡ˆåï¼Œå·¥ä½œæµç¨‹åˆ°è¾¾ä¸€ä¸ªæ¡ä»¶è¾¹ã€‚è°ƒç”¨ `edge_graph.grade_generation_v_documents_and_question` å‡½æ•°æ¥è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆï¼ŒåŸºäºå…¶åœ¨æ–‡æ¡£ä¸­çš„åŸºç¡€å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚å¦‚æœå‡½æ•°è¿”å› `"not supported"`ï¼Œå·¥ä½œæµç¨‹å°†è¿”å›åˆ° `generate` èŠ‚ç‚¹ä»¥é‡æ–°ç”Ÿæˆç­”æ¡ˆã€‚æ­¤æ­¥éª¤æ˜¯å¿…è¦çš„ï¼Œä»¥ç¡®ä¿å·¥ä½œæµç¨‹ç”Ÿæˆçš„ç­”æ¡ˆå¾—åˆ°æ–‡æ¡£çš„æ”¯æŒã€‚å¦‚æœå‡½æ•°è¿”å› `"useful"`ï¼Œå·¥ä½œæµç¨‹å°†ç»“æŸï¼Œè¡¨ç¤ºç”Ÿæˆäº†æœ‰ç”¨çš„ç­”æ¡ˆã€‚å¦‚æœå‡½æ•°è¿”å› `"not useful"`ï¼Œå·¥ä½œæµç¨‹å°†ç§»åŠ¨åˆ° `transform_query` èŠ‚ç‚¹ä»¥å†æ¬¡è½¬æ¢æŸ¥è¯¢ã€‚

æœ€åï¼Œæˆ‘ä»¬å°†ç¼–è¯‘å›¾ä»¥å°†å…¶è½¬æ¢ä¸ºå¯æ‰§è¡Œé“¾ã€‚ä»¥ä¸‹æ˜¯å·¥ä½œæµç¨‹çš„æ ·å­ï¼š

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ucZ2E78HWua8WfkynRAqHQ.png)

## ç¬¬7æ­¥ï¼šä½¿ç”¨ FastAPI å¯åŠ¨æœåŠ¡å™¨

ç°åœ¨ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä½¿ç”¨ FastAPI å¯åŠ¨æœåŠ¡å™¨æ‰€éœ€çš„æœ€åæ­¥éª¤ã€‚æˆ‘ä»¬å°†é€æ­¥åˆ†æä»£ç å¹¶è¯¦ç»†è§£é‡Šæ¯ä¸ªæ­¥éª¤ã€‚

ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ª FastAPI åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¯¼å…¥ FastAPI å¹¶åˆ›å»º `FastAPI` ç±»çš„å®ä¾‹æ¥å®ç°ã€‚æˆ‘ä»¬ä¼ å…¥ä¸€äº›å…ƒæ•°æ®ï¼Œä¾‹å¦‚åº”ç”¨çš„æ ‡é¢˜ã€ç‰ˆæœ¬å’Œæè¿°ã€‚

```python
app = FastAPI(
    title="Speckle Server",
    version="1.0",
    description="An API server to answer questions regarding the Speckle Developer Docs"
)
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ ¹ URL (`"/"`) çš„è·¯ç”±ï¼Œè¯¥è·¯ç”±é‡å®šå‘åˆ°æ–‡æ¡£ URL (`"/docs"`)ã€‚è¿™æ˜¯ FastAPI åº”ç”¨ä¸­çš„ä¸€ç§å¸¸è§æ¨¡å¼ï¼Œå› ä¸ºå®ƒå…è®¸ç”¨æˆ·è½»æ¾è®¿é—®æ–‡æ¡£ã€‚

```python
@app.get("/")
async def redirect_root_to_docs():
    return RedirectResponse("/docs")
```
æˆ‘ä»¬ä½¿ç”¨ Pydantic çš„ `BaseModel` å®šä¹‰ä¸¤ä¸ªæ¨¡å‹ï¼š`Input` å’Œ `Output`ã€‚è¿™äº›æ¨¡å‹å°†ç”¨äºå®šä¹‰æˆ‘ä»¬ API çš„è¾“å…¥å’Œè¾“å‡ºæ•°æ®çš„ç»“æ„ã€‚

```python
class Input(BaseModel):
    input: str

class Output(BaseModel):
    output: dict
```
æˆ‘ä»¬ä½¿ç”¨ `add_routes` å‡½æ•°å‘åº”ç”¨æ·»åŠ è·¯ç”±ã€‚è¯¥å‡½æ•°æ¥å—ä¸‰ä¸ªå‚æ•°ï¼šåº”ç”¨å®ä¾‹ã€é“¾å®ä¾‹å’Œè·¯ç”±çš„è·¯å¾„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸º `/speckle_chat` ç«¯ç‚¹æ·»åŠ äº†ä¸€ä¸ªè·¯ç”±ã€‚

```python
add_routes(
    app,
    chain.with_types(input_type=Input, output_type=Output),
    path="/speckle_chat",
)
```
æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ Uvicorn è¿è¡ŒæœåŠ¡å™¨ã€‚æˆ‘ä»¬å¯¼å…¥ Uvicorn å¹¶è°ƒç”¨ `run` å‡½æ•°ï¼Œä¼ å…¥åº”ç”¨å®ä¾‹ã€ä¸»æœºå’Œç«¯å£ã€‚

```python
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="localhost", port=8000)
```
å°±è¿™æ ·ã€‚é€šè¿‡è¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª FastAPI åº”ç”¨å¹¶å¯åŠ¨äº†ä¸€ä¸ªå¯ä»¥åœ¨ [`http://localhost:8000`](http://localhost:8000) è®¿é—®çš„æœåŠ¡å™¨ã€‚

## ç¬¬8æ­¥ï¼šåˆ›å»ºä¸€ä¸ªå¸¦æœ‰ Streamlit/Gradio UI çš„å®¢æˆ·ç«¯

æˆ‘ä»¬ç°åœ¨å°†åˆ›å»ºä¸€ä¸ª `client.py` æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶å°†ä½¿ç”¨ Python çš„ Streamlit åº“ä¸æœåŠ¡å™¨è¿›è¡Œäº¤äº’ã€‚

```python
import streamlit as st
from langserve import RemoteRunnable
from pprint import pprint

st.title('Welcome to Speckle Server')
input_text = st.text_input('ask speckle related question here')

if input_text:
    with st.spinner("Processing..."):
        try:
            app = RemoteRunnable("http://localhost:8000/speckle_chat/")
            for output in app.stream({"input": input_text}):
                for key, value in output.items():
                    # Node
                    pprint(f"Node '{key}':")
                    # Optional: print full state at each node
                    # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
                pprint("\n---\n")
            output = value['generation']  
            st.write(output)
        
        except Exception as e:
            st.error(f"Error: {e}")
```
è®©æˆ‘ä»¬å¼€å§‹è®¾ç½® **Streamlit** åº”ç”¨ç¨‹åºï¼Œæ·»åŠ ä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªæ–‡æœ¬è¾“å…¥å­—æ®µï¼Œä¾›ç”¨æˆ·è¾“å…¥ä»–ä»¬çš„é—®é¢˜ã€‚å½“ç”¨æˆ·è¾“å…¥ä»»ä½•æ–‡æœ¬æ—¶ï¼Œåº”ç”¨ç¨‹åºä¼šæ˜¾ç¤ºä¸€ä¸ªåŠ è½½æŒ‡ç¤ºå™¨ï¼Œä»¥è¡¨æ˜è¾“å…¥æ­£åœ¨å¤„ç†ã€‚ç„¶åï¼Œåº”ç”¨ç¨‹åºä½¿ç”¨ `langserve` ä¸­çš„ `RemoteRunnable` æ¨¡å—è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨æœåŠ¡å™¨ URLã€‚å®ƒé€šè¿‡ `stream` å‘½ä»¤ä» LLM æ¨¡å‹æµå¼ä¼ è¾“å“åº”ï¼ŒåŒæ—¶æ‰“å°å›¾å½¢å·¥ä½œæµä¸­è¢«è§¦å‘çš„èŠ‚ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä»å€¼å­—å…¸ä¸­æ£€ç´¢å­˜å‚¨åœ¨ `'generation'` é”®ä¸­çš„æœ€ç»ˆè¾“å‡ºã€‚å¦‚æœåœ¨å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œå°†æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯ã€‚å®ƒçš„æ ·å­æ˜¯è¿™æ ·çš„ï¼ ğŸ˜

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YYKC2u0qB3MLbtCmIinwgQ.png)

## å¯é€‰ï¼šä½¿ç”¨ Gradio åˆ›å»ºç”¨æˆ·ç•Œé¢

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Gradioï¼›è¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“ï¼Œç”¨äºä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹ã€API å’Œä»»æ„ Python å‡½æ•°åˆ›å»ºäº¤äº’å¼åŸºäº Web çš„ç”¨æˆ·ç•Œé¢ã€‚å®ƒçš„ä¸»è¦ç›®çš„æ˜¯é€šè¿‡æä¾›æ˜“äºä½¿ç”¨çš„ç•Œé¢æ¥å¼¥åˆæœºå™¨å­¦ä¹ æ¨¡å‹ä¸æœ€ç»ˆç”¨æˆ·ä¹‹é—´çš„å·®è·ï¼Œä»è€Œä¾¿äºéƒ¨ç½²å’Œä¸è¿™äº›æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚

è®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿ä» LLM æ¨¡å‹è·å–æœ€ç»ˆå“åº”ã€‚

```python
def get_response(input_text):
    app = RemoteRunnable("http://localhost:8000/speckle_chat/")
    for output in app.stream({"input": input_text}):
        for key, value in output.items():
            # Node
            pprint(f"Node '{key}':")
            # Optional: print full state at each node
            # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
        pprint("\n---\n")
    output = value['generation']
    return output      
```
ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®€å•çš„ Gradio ç”¨æˆ·ç•Œé¢ï¼Œåœ¨ Gradio çš„ `Interface` å‡½æ•°ä¸­å°† `get_response` å‡½æ•°åˆ†é…ç»™ `fn` å˜é‡ã€‚

```python
import gradio as gr
from langserve import RemoteRunnable
from pprint import pprint

# Create the UI In Gradio
iface = gr.Interface(fn=get_response, 
          inputs=gr.Textbox(
          value="è¾“å…¥æ‚¨çš„é—®é¢˜"), 
          outputs="textbox",  
          title="å…³äº Speckle å¼€å‘æ–‡æ¡£çš„é—®ç­”",
          description="è¯¢é—®æœ‰å…³ Speckle å¼€å‘æ–‡æ¡£çš„é—®é¢˜ï¼Œå¹¶ä»ä»£ç åŠ©æ‰‹é‚£é‡Œè·å¾—ç­”æ¡ˆã€‚è¯¥åŠ©æ‰‹æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£å¹¶å›ç­”æ‚¨çš„ä»£ç ç›¸å…³é—®é¢˜ã€‚",
          examples=[["å¦‚ä½•å®‰è£… Speckle çš„ Python SDKï¼Ÿ"], 
                  ["å¦‚ä½•ä» Speckle æäº¤å’Œæ£€ç´¢å¯¹è±¡ï¼Ÿ"],
                  ],
          theme=gr.themes.Soft(),
          allow_flagging="never",)

iface.launch(share=True) # put share equal to True for public URL
```
è¿™å°±æ˜¯å®ƒçš„æ ·å­ï¼ ğŸ‘‡ğŸ¼

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6cgm8RgCrtsEnxdd6yNojg.png)

æ‚¨åªéœ€åœ¨ `launch` å‡½æ•°ä¸­åŒ…å« `share=True`ï¼Œå³å¯åœ¨æœ¬åœ° URL ä¹‹ä¸Šè·å–å…¬å…± URLã€‚

# ç»“è®º

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸€ä¸ªæœåŠ¡å™¨-å®¢æˆ·ç«¯æ¶æ„çš„å›¾å½¢å·¥ä½œæµçš„å¼€å‘ï¼Œè¯¥æ¶æ„ç»“åˆäº†å…ˆè¿›çš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ¦‚å¿µã€‚æœåŠ¡å™¨ç»„ä»¶æ¶µç›–äº†ä¸€ä¸ªå…¨é¢çš„ç®¡é“ï¼ŒåŒ…æ‹¬å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œè¯„åˆ†ã€å¯¹å“åº”è¿›è¡Œè¯„åˆ†ã€æ£€æŸ¥å¹»è§‰ä»¥åŠæŸ¥è¯¢é‡å†™ã€‚

ä¸ºäº†ä¸è¿™ä¸ªæœ¬åœ°æœåŠ¡å™¨è¿›è¡Œäº¤äº’ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªå®¢æˆ·ç«¯åº”ç”¨ç¨‹åºï¼Œä¸€ä¸ªä½¿ç”¨Streamlitï¼Œå¦ä¸€ä¸ªä½¿ç”¨Gradioã€‚ä¸¤ä¸ªç”¨æˆ·ç•Œé¢éƒ½æä¾›äº†å‹å¥½çš„ç•Œé¢ï¼Œä¾›ç”¨æˆ·è¾“å…¥æŸ¥è¯¢å¹¶å®æ—¶æ¥æ”¶æœåŠ¡å™¨çš„å“åº”ã€‚è¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„é¡¹ç›®ï¼Œå°†å…è®¸å¼€å‘äººå‘˜æ„å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºå¹¶åœ¨æœ¬åœ°è¿›è¡Œæµ‹è¯•ï¼Œç„¶åå†éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ã€‚

åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨Dockerå¯¹è¿™ä¸ªåº”ç”¨ç¨‹åºè¿›è¡Œå®¹å™¨åŒ–ï¼Œå¹¶ä½¿ç”¨Google Cloud Platformè¿›è¡Œéƒ¨ç½²ã€‚

è¿™é‡Œæ˜¯Specklyæœºå™¨äººçš„[GitHubä»“åº“](https://github.com/bhargobdeka/RAG-chatbot-Speckly)ã€‚

éšç€æˆ‘ç»§ç»­å­¦ä¹ ï¼Œæˆ‘å°†æ·»åŠ æ›´å¤šå†…å®¹ã€‚å¦‚æœä½ å–œæ¬¢æˆ‘çš„è´¡çŒ®ï¼Œå¯ä»¥ç”¨â­ï¸æ¥æ”¯æŒè¿™ä¸ªä»“åº“ :-)

ä½ ä¹Ÿå¯ä»¥åœ¨[LinkedIn](https://www.linkedin.com/in/bhargobdeka/)ä¸Šä¸æˆ‘è”ç³»ï¼

Ã€ bientÃ´t!
