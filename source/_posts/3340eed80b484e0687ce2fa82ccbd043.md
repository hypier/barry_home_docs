
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Y9_qIOxQgnbkSNrgUkBfEA.jpeg
date: '2024-05-11 16:58:50'
tags:
  - 人工智能
  - 网页抓取
  - 数据工程
title: 高级RAG 11查询分类与优化

---


# 人工智能正迎来网页抓取的新时代

## 如何利用生成式AI以空前的规模和无与伦比的准确性从互联网上抓取数据

# 关于我

*我是Hugo Lu，职业生涯始于伦敦的并购领域，后加入JUUL并涉足数据工程。短暂回归金融行业后，我领导了伦敦金融科技公司[Codat](http://www.codat.io/?utm_campaign=hugolu)的数据部门。目前，我是[Orchestra](https://getorchestra.io?utm_campaign=qyqkhcyorm)的CEO，这是一款数据发布管道工具，助力数据团队可靠高效地将数据投入生产🚀*

也请关注我们的[Substack](https://dataopsleadership.substack.com/)和[内部博客](https://getorchestra.io/blog?utm_campaign=qyqkhcyorm) ⭐️

想了解Orchestra如何通过提供无与伦比的成本节约和可见性来改变游戏规则？立即尝试我们的[免费层级](https://app.getorchestra.io/signup)。

# 引言

互联网上大约有 [500亿个页面](https://siteefy.com/how-many-websites-are-there/#:~:text=根据研究项目的数据，截至2023年1月，全球网站数量已超过18亿，涵盖了与网站相关的绝大多数事物。)。这相当于50,000,000,000份内容。尽管这一数字令人望而生畏，但对于训练有素的数据工程师而言，这却是一个信息宝库，可用于各种应用场景。

过去，这一过程得益于 [网页抓取](https://en.wikipedia.org/wiki/Web_scraping) 工具，如 [Beautiful Soup](https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)#:~:text=Beautiful Soup 是一个Python库，用于从网页中提取数据，对于网页抓取非常有用。)（一个HTML解析器）和 [Selenium](https://www.selenium.dev/)，一个浏览器自动化工具。然而，许多人并未意识到，按照互联网标准，这些工具已经非常老旧。两者均于2004年发布，今年将迎来它们的20岁生日。

自2004年以来，已有多个HTML解析器试图复制Beautiful Soup的成功。其中 [Beautiful Soup的替代品](https://stackshare.io/beautifulsoup/alternatives) 包括Scrapy、PyQuery和Mechanical Soup。

尽管存在众多替代品，但以系统化、高效且可重复的方式收集数据确实是一个非常棘手的问题。如今我们所称的“人工智能”（AI）不仅有望加速开发进程，还能促进网页抓取在数据领域的应用。

本文将深入探讨这一问题，并介绍如何利用传统方法解决它，以及与之相关的痛点。我们将看到AI驱动的网页抓取工具如何缓解这些痛点，以及数据工程师如何利用这些工具来发挥自己的优势。

# HTML解析的问题

每个网页都由HTML代码——[超文本标记语言](https://www.w3schools.com/html/html_intro.asp)所定义。对于任何页面，通常可以右键点击屏幕并选择“检查”来查看其内部结构：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eWuq9zcVbkXP_jmEhM2MFw.png)

HTML代码由HTML元素组成。W3对此解释为：

> HTML元素由开始标签、一些内容和结束标签定义：内容在此…

HTML代码必然具有[*结构*](https://www.w3.org/TR/html4/struct/global.html)：

> HTML 4文档由三部分组成：

> 包含[HTML版本信息](https://www.w3.org/TR/html4/struct/global.html#version-info)的一行，

> 声明性头部区域（由[HEAD](https://www.w3.org/TR/html4/struct/global.html#edef-HEAD)元素分隔），

> 一个主体，包含文档的实际内容。主体可以由[BODY](https://www.w3.org/TR/html4/struct/global.html#edef-BODY)元素或[FRAMESET](https://www.w3.org/TR/html4/present/frames.html#edef-FRAMESET)元素实现。

在主体中，作为对提取数据感兴趣的人，我们希望访问这些元素。此外，我们希望保留[*层次*](https://htschool.hindustantimes.com/editorsdesk/knowledge-vine/what-are-html-tags-and-html-hierarchy-in-coding)结构。例如：

```python
<head> e-commerce < / head>  
<title> Welcome! < / title> 
<body>  
  <p1> My first paragraph < / p1> 
    <div> Some stuff </div>
  <p2> My second paragraph < / p2> 
    <div> Some more stuff </div>
  <p3> My third paragraph < / p3> 
    <div> Even more stuff </div>
< / body> 
< / html> 
```
从中，我们可能需要：

```python
{
"elements":[
  {
  "type" : "p1",
  "content" "My first paragraph",
  "children" : [
    {
      "type" : "div",
      "content" "some stuff",
      "children" :[]
    }
  ... 
  ]
}
```
这些数据对于结构化数据分析来说基本上是无用的。因此，通过这个JSON，我们可能会进一步将JSON扁平化为表格形式。

这就引出了问题的第一部分，即**解析是复杂的。HTML代码**具有深度嵌套的结构，并且在具有多层嵌套方面[变得越来越复杂](https://www.quora.com/How-complicated-does-HTML-get)。

还有一些额外的考虑因素使得这一过程变得不那么简单。

## 时间或持续时间

网络爬虫通常需要在长时间内对多个网页进行抓取。然而，计算本身并不困难，但它需要一种不同于许多数据工程工作负载（短时高计算）的基础设施（长时间运行，低计算）。这意味着在类似于[Kubernetes上的Airflow](https://www.getorchestra.io/guides/airflow-operator-series-apache-airflow-providers-cncf-kubernetes-explained)的基础设施上简单地运行Python网络爬虫作业是不合理的昂贵。

## 使用浏览器自动化工具

有时，页面加载时并非所有HTML代码都会渲染。您需要点击某个元素才能显示内容。这意味着需要使用Selenium这样的浏览器自动化工具来“获取”数据。更多关于BS和Selenium的区别，请阅读[这里](https://www.reddit.com/r/Python/comments/ndozrt/why_would_you_want_to_use_beautifulsoup_instead/)。

## “变更模式”

网页会发生变化，这意味着HTML文档的结构也会随之改变。使用Beautiful Soup等工具时，随着时间的推移，当[HTML结构发生变化](https://www.pluralsight.com/resources/blog/guides/web-scraping-with-beautiful-soup)时，修改代码可能会变得困难且耗时，这使得利用网络爬虫收集数据的过程变得不稳定且不可靠。

在数据领域，我们可以利用[数据合约](https://readmedium.com/should-data-teams-care-about-data-contracts-efbbe78e0d76)或传统的人际关系来确保这种情况不会发生。当你不知道电话另一端是谁（或者该打给谁）时，这就困难得多了。

## 合法性

将互联网作为数据源——好主意，对吧？想象一下，你可以自由合法地访问到那些令人难以置信的数据片段。再想想。

许多网站，如Linkedin，在其使用条款中明确禁止使用网络爬虫。在[犯罪行为与仅被封禁](https://www.reddit.com/r/webscraping/comments/lrzh8i/can_you_get_banned_for_scraping_job_data_on/)之间似乎有一条细微的界限。根据你的使用场景，你需要极其谨慎，并评估你正在爬取的网站是否可以[合法地](https://oxylabs.io/blog/is-web-scraping-legal)进行。

*这不是法律建议，也不应以此作为法律依据。*

## 网络爬虫的代理服务

代理服务是一种允许你通过其主机路由请求的服务。这意味着你所使用的IP地址，以及可能的浏览器或其他组件都会有所不同。

这样做有[多种优势](https://www.zyte.com/learn/use-proxies-for-web-scraping/)。它允许你进行多个并发请求（扩展性、速度），你可以绕过全局的IP封禁（有时网站会直接封禁AWS的IP），并且你可以从特定的地理位置设置代理，从而获取针对该特定地区的内容。

这通常是大多数人难以自行管理的事情。

# 如何利用AI提升网页抓取效率

通过一个简单示例和Open AI，我们将展示如何抓取数据以及AI如何简化这一过程。

## 抓取我自己的网站

为了便于演示，我将展示如何抓取Orchestra网站。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*u5LMqhPhOnpMcE_VtqAimA.png)

我们将抓取我的集成页面。虽然不确定你为何想要这样做，但从竞争情报的角度来看，你或许希望关注我的集成及其随时间的发展。

使用Python，Beautiful Soup脚本大致如下：

```python
import json
import pandas as pd
from bs4 import BeautifulSoup

# 加载HTML内容
with open('path_to_your_html_file.html', 'r') as file:
    html_content = file.read()

# 使用Beautiful Soup解析HTML
soup = BeautifulSoup(html_content, 'html.parser')

# 查找所有集成项 - 根据你的HTML结构调整类或标签
integration_cards = soup.find_all('div', class_='your_card_class_here')

data = []
for card in integration_cards:
    integration_name = card.find('div', class_='integration_name_class').text.strip()
    status = card.find('div', class_='status_class').text.strip()
    description = card.find('p', class_='description_class').text.strip()
    
    data.append({
        'Integration Name': integration_name,
        'Status': status,
        'Description': description
    })

# 将数据转换为DataFrame并保存到Excel
df = pd.DataFrame(data)
df.to_excel('integrations.xlsx', index=False)

# 将数据保存到JSON文件
with open('integrations.json', 'w') as json_file:
    json.dump(data, json_file)
```
我们可以看到，上一节提到的所有问题在这里都适用。如果我将集成类的名称从“integration_name_class”改为其他名称，这个脚本就会失效。这不好。

此外，我仍然需要花时间编写代码来扁平化我的JSON。这并不理想，因为我可能更希望有一个单独的流程来处理这个，而不是实时处理（如果我在大规模操作的话）。

然而，通过访问可以轻松下载的*原始*HTML，这可以传递给Open AI，并提示请求结构化数据。它会如期返回：

```python
import boto3
import requests
from bs4 import BeautifulSoup
import openai
import pandas as pd

# AWS S3配置
s3 = boto3.client('s3')
bucket_name = 'your-bucket-name'

# 将网页保存到S3的函数
def save_webpage_to_s3(url, file_name):
    response = requests.get(url)
    s3.put_object(Bucket=bucket_name, Key=file_name, Body=response.content)
    return f"s3://{bucket_name}/{file_name}"

# 从S3加载网页并提取数据的函数
def extract_data_from_html_s3(file_name):
    obj = s3.get_object(Bucket=bucket_name, Key=file_name)
    html_content = obj['Body'].read().decode('utf-8')
    
    # 将HTML内容发送到OpenAI进行处理
    response = openai.Completion.create(
        engine="text-davinci-002",
        prompt="从这段HTML中提取带有状态和描述的集成列表: " + html_content,
        max_tokens=1024
    )
    
    # 将响应转换为DataFrame
    data = response.choices[0].text
    df = pd.read_json(data)
    
    # 将DataFrame保存回S3为CSV
    csv_buffer = StringIO()
    df.to_csv(csv_buffer)
    s3.put_object(Bucket=bucket_name, Key='integrations.csv', Body=csv_buffer.getvalue())

# 主执行流程
url_to_save = "https://www.getorchestra.io/integrations"
html_file_name = "webpage.html"
save_webpage_to_s3(url_to_save, html_file_name)
extract_data_from_html_s3(html_file_name)
```
代码稍多一些，但只要你对Open AI API返回的数据结构满意，需要做的工作就少得多。

利用LLM（大型语言模型）使整个解析HTML的过程变得冗余/简化，而且由于LLM足够智能，能够推断出随时间变化的架构更改的影响。

此外，这个过程很容易扩展。它基本上只是保存轻量级文件并进行API调用（可以同步/异步进行），这是一个[经典的ELT流程](https://stackoverflow.com/questions/35717284/getting-data-from-api-synchronously)。

# AI驱动的网页抓取工具

众所周知，[LLMs会产生幻觉](https://www.ibm.com/topics/ai-hallucinations#:~:text=%7C,are%20nonsensical%20or%20altogether%20inaccurate.)，无法保证从LLM获得的响应始终是正确格式。

因此，涌现了大量AI驱动的网页抓取API，您可以利用上述模式，确保不一致的数据成为他人的问题，而非您的困扰。

## Nimble

[Nimble](https://nimbleway.com/nimble-api/web/) 是一个支持 AI 的网络抓取 API，数据工程师可以利用它从互联网上的数据源中可靠地提取数据，并将其存入 S3 或 GCS 存储桶。

它处理了“解封”（即我们之前提到的代理服务器问题）等较为棘手的方面，并为某些网站类型封装了专属模型，这些模型实质上提供了自动化解析（或称为“Nimble 技能”）。

其中包括搜索引擎结果页面（“SERP”）API、专注于电商平台的 API（用于访问价格数据）、地图 API（尽管 [GMaps 有自己的 API](https://developers.google.com/maps)）以及一个多用途的网络 API。

对于市场研究公司或零售商而言，这里存在着一些极其强大的应用场景，特别是在收集情报方面，而 SERP 功能对于实时监控某些关键词的表现极为有用。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*drZEIDliqRlnyCEcz6y28g.png)

## Diffbot

[Diffbot](https://www.diffbot.com/company/) 是一家相对较老的公司，由 [Mike Tung](https://www.linkedin.com/in/miketung/) 大约在12年前创立。其核心价值主张与我们在这篇文章中讨论的非常相似——专注于沿着两个关键维度抓取网页：准确性和规模。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vkCtx-qAuMObF-A-X6EFng.png)

他们似乎还有一些相当有趣的 [自然语言](https://www.diffbot.com/products/natural-language/) 产品，可用于数据丰富（我在[这里](https://towardsdatascience.com/how-i-use-gen-ai-as-a-data-engineer-6a686a921c7b)简要提到过）。在我看来，价格也相对合理，包括免费层级（见[此处](https://www.diffbot.com/pricing/)）。

## 其他

随着人工智能的发展，涌现出许多AI驱动的网络抓取工具，如ScrapeStorm、Octoparse等，更多工具可参见[此处](https://ubiquedigitalsolutions.com/blog/top-7-web-scraping-tools-using-ai/)。

这些工具的共同特点似乎包括易用的“点选式”用户界面、基础设施处理能力、顺畅的反封锁/代理网络支持，以及在需要时提供低延迟抓取的承诺。

尽管许多解决方案价格较为亲民，但规模、速度、代理网络的必要性以及网站类型等因素，都使得不同工具之间存在差异，从而影响选择。在挑选工具前，请仔细考虑您的具体需求。

# 结论

本文概述了数据工程师使用网络爬虫作为实时收集数据手段的基本问题。我们了解到，像Beautiful Soup和Selenium这样的技术使得这一过程成为可能，但在投资网络爬虫项目之前，仍有许多考量因素。

人工智能极大地提高了网络爬虫的便捷性、准确性和速度，市场上涌现出的以API为先的新解决方案，能够轻松嵌入模块化数据架构中，实现真正有洞察力和实用性的用例，这确实令人振奋。

如果您对数据工程师和数据团队如何利用生成式AI感兴趣，欢迎在[Medium上关注我](https://medium.com/@hugolu87)或通过[LinkedIn](https://www.linkedin.com/in/hugo-lu-confirmed/)与我联系。Hugo 🚀

# 了解更多关于Orchestra

[Orchestra](https://getorchestra.io/) 是一个旨在以尽可能人性化的方式最大化数据价值的平台。它还是一个[功能丰富的编排](https://www.getorchestra.io/product)工具，能够解决多种[用例和解决方案](https://www.getorchestra.io/solutions)。我们的文档在此，但不妨也查看我们的[集成](https://www.getorchestra.io/integrations)——我们负责管理这些，以便您能*立即*开始使用您的管道。我们还设有[博客](https://www.getorchestra.io/blog)，由Orchestra团队及特邀作者撰写，以及一些[白皮书](https://www.getorchestra.io/whitepapers)供深入阅读。
