
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5p2LrTQ4Yho7IA1T.png
date: '2024-07-08 11:55:50'
tags:
  - 开源平台
  - 大型语言模型
  - AI 应用开发
title: 开始使用 Dify无代码 AI 应用开发

---


## 快速入门与实用实施的全面指南



# 作者

* [杨子健](https://www.linkedin.com/in/zijian-yang/) (**ORCID:** [0009–0006–8301–7634](https://orcid.org/0009-0006-8301-7634))

# 介绍

Dify 是一个开源的大型语言模型 (LLM) 应用开发平台。它结合了后端即服务 (Backend-as-a-Service) 和 LLMOps 的概念，使开发者能够快速构建生产级的生成式 AI 应用。即使是非技术人员也可以参与 AI 应用的定义和数据操作。

通过整合构建 LLM 应用所需的关键技术栈，包括对数百种模型的支持、直观的提示编排界面、高质量的 RAG 引擎以及灵活的代理框架，同时提供一套易于使用的接口和 API，Dify 为开发者节省了大量重新发明轮子的时间，使他们能够专注于创新和业务需求。

Dify 这个名字来源于 Define + Modify，指的是定义和持续改进您的 AI 应用。

# 为什么使用 Dify？

你可以将像 LangChain 这样的库视为装有锤子、钉子等工具的工具箱。相比之下，Dify 提供了一个更适合生产的完整解决方案——可以将 Dify 想象成一个具有精细工程设计和软件测试的脚手架系统。

重要的是，Dify 是 **开源** 的，由一个专业的全职团队和社区共同创建。你可以基于任何模型自我部署类似于 Assistants API 和 GPTs 的能力，保持对数据的完全控制，并具有灵活的安全性，所有这些都在一个易于使用的界面上。

# Dify可以做什么？

* **工作流**：Dify提供一个可视化画布，用于构建和测试强大的AI工作流。此功能使用户能够充分利用Dify的全部功能，包括模型集成和提示设计。
* **全面的模型支持**：该平台支持与数百个专有和开源LLM的无缝集成，包括流行选项如GPT、Mistral、Llama3以及任何与OpenAI API兼容的模型。这种广泛的模型支持确保了开发人员的灵活性和选择。
* **提示IDE**：Dify包含一个直观的提示IDE，允许用户设计提示、比较模型性能，并通过文本转语音等附加功能增强应用程序。
* **RAG管道**：Dify的RAG（检索增强生成）功能涵盖从文档摄取到检索的所有内容。它包括开箱即用的支持，用于从各种文档格式（如PDF和PPT）中提取文本。
* **代理能力**：用户可以使用LLM功能调用或ReAct定义代理，并集成预构建或自定义工具。Dify为AI代理提供超过50个内置工具，包括Google搜索、DALL·E、Stable Diffusion和WolframAlpha。
* **LLMOps**：该平台包括可观察性功能，用于监控和分析应用程序日志和性能。这允许根据现实世界的数据和注释持续改进提示、数据集和模型。
* **后端即服务**：Dify为其所有功能提供相应的API，使其能够轻松集成到现有的业务逻辑中。
* **云**：Dify提供零设置的云服务，包括自托管版本的所有功能。Sandbox计划提供200次免费的GPT-4调用以供实验。
* **自托管**：Dify的社区版可以在任何环境中快速设置，并提供详细文档以便进行更深层次的自定义。
* **企业解决方案**：Dify提供以企业为中心的功能，如SSO和访问控制。它还在AWS Marketplace上提供Dify Premium选项，包括应用程序的自定义品牌和徽标。

# 您可以从本文中获得的收益

通过阅读本文，您将全面了解如何利用 Dify，一个开源 LLM 应用开发平台，创建强大的 AI 应用，而无需编写代码。

您将学习如何使用可视化界面构建和部署工作流，集成各种模型，并实现 RAG 管道和 AI 代理等高级功能。

此外，本文还提供了有关设置云端和自托管解决方案的实用指导，使您具备高效部署和管理 AI 应用的技能。

无论您是 AI 新手还是经验丰富的开发者，您都将找到有价值的见解和实用步骤，以提升您在 Dify 上的 AI 开发能力。

# 选择使用方法

## 云服务：在线体验

Dify 提供了一项云服务，供所有人使用，因此您可以在不自行部署的情况下使用 Dify 的全部功能。

从免费计划开始，其中包括 200 次 OpenAI 调用的免费试用。要使用云版本的免费计划，您需要一个 GitHub 或 Google 账户以及一个 OpenAI API 密钥。以下是您可以开始的步骤：

1. 注册 [Dify Cloud](https://cloud.dify.ai/) 并创建一个新的工作区或加入一个现有的工作区。
2. 配置您的模型提供者或使用托管模型提供者。
3. 您现在可以创建一个应用程序！

只有两个登录选项：GitHub 和 Google。您可以选择其中一个进行登录。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*3aljFmBlA5zmUm9-.png)

登录后，您将看到 Studio 界面。您可以跳过下一部分，直接进入“模型”章节。

# 部署社区版：本地运行

如果您想在本地运行 Dify，可以选择部署 Dify 社区版，这是开源版本。您可以通过 Docker Compose 或本地源代码进行部署。本文将演示在 Windows 上使用 Docker Compose 本地部署 Dify 的更方便方法。

首先，安装并运行 Docker Desktop，并启用 WSL 2。您可以从以下链接下载；本文不涵盖详细的安装过程：

<https://www.docker.com/products/docker-desktop/>

在您希望存储 Dify 的目录中打开命令提示符，并输入：

```python
git clone https://github.com/langgenius/dify.git
```
您将看到以下输出：

```python
D:\Joe\Project\Dify>git clone https://github.com/langgenius/dify.git
Cloning into 'dify'...
remote: Enumerating objects: 66122, done.
remote: Counting objects: 100% (10553/10553), done.
remote: Compressing objects: 100% (1540/1540), done.
remote: Total 66122 (delta 9611), reused 9196 (delta 9008), pack-reused 55569
Receiving objects: 100% (66122/66122), 38.65 MiB | 11.63 MiB/s, done.
Resolving deltas: 100% (47189/47189), done.
Updating files: 100% (5109/5109), done.
```
如果您尚未安装 Git，可以从 Dify 的 GitHub 仓库下载整个项目并解压缩，然后继续执行以下步骤。但这种方法在未来更新时不太方便。

导航到 Dify 源代码的 `docker` 目录并执行一键启动命令：

```python
# Navigate to the docker directory
cd dify/docker
# Copy and rename the configuration file
cp .env.example .env
# If you are using Windows cmd, use the copy command instead of cp
copy .env.example .env
# Start docker compose
docker compose up -d
```
部署输出：

```python
D:\Joe\Project\Dify\dify\docker>docker compose up -d
[+] Running 75/9
 ✔ weaviate Pulled                                                                                                27.0s
 ✔ web Pulled                                                                                                     61.1s
 ✔ ssrf_proxy Pulled                                                                                              26.9s
 ✔ api Pulled                                                                                                     51.3s
 ✔ redis Pulled                                                                                                   27.3s
 ✔ sandbox Pulled                                                                                                 40.4s
 ✔ db Pulled                                                                                                      30.1s
 ✔ nginx Pulled                                                                                                   27.1s
 ✔ worker Pulled                                                                                                  51.3s
[+] Running 11/11
 ✔ Network docker_default             Created                                                                      0.0s
 ✔ Network docker_ssrf_proxy_network  Created                                                                      0.1s
 ✔ Container docker-sandbox-1         Started                                                                      2.1s
 ✔ Container docker-weaviate-1        Started                                                                      2.7s
 ✔ Container docker-db-1              Started                                                                      2.7s
 ✔ Container docker-ssrf_proxy-1      Started                                                                      2.7s
 ✔ Container docker-redis-1           Started                                                                      2.7s
 ✔ Container docker-web-1             Started                                                                      1.6s
 ✔ Container docker-api-1             Started                                                                      2.9s
 ✔ Container docker-worker-1          Started                                                                      2.9s
 ✔ Container docker-nginx-1           Started                                                                      3.3s
```
最后，检查所有容器是否正常运行：

```python
docker compose ps
```
运行输出：

```python
D:\Joe\Project\Dify\dify\docker>docker compose ps
NAME                  IMAGE                              COMMAND                  SERVICE      CREATED         STATUS                   PORTS
docker-api-1          langgenius/dify-api:0.6.15         "/bin/bash /entrypoi…"   api          3 minutes ago   Up 3 minutes             5001/tcp
docker-db-1           postgres:15-alpine                 "docker-entrypoint.s…"   db           3 minutes ago   Up 3 minutes (healthy)   5432/tcp
docker-nginx-1        nginx:latest                       "sh -c 'cp /docker-e…"   nginx        3 minutes ago   Up 3 minutes             0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp
docker-redis-1        redis:6-alpine                     "docker-entrypoint.s…"   redis        3 minutes ago   Up 3 minutes (healthy)   6379/tcp
docker-sandbox-1      langgenius/dify-sandbox:0.2.1      "/main"                  sandbox      3 minutes ago   Up 3 minutes
docker-ssrf_proxy-1   ubuntu/squid:latest                "sh -c 'cp /docker-e…"   ssrf_proxy   3 minutes ago   Up 3 minutes             3128/tcp
docker-weaviate-1     semitechnologies/weaviate:1.19.0   "/bin/weaviate --hos…"   weaviate     3 minutes ago   Up 3 minutes
docker-web-1          langgenius/dify-web:0.6.15         "/bin/sh ./entrypoin…"   web          3 minutes ago   Up 3 minutes             3000/tcp
docker-worker-1       langgenius/dify-api:0.6.15         "/bin/bash /entrypoi…"   worker       3 minutes ago   Up 3 minutes             5001/tcp
```
输出应包括 3 个业务服务：api、worker 和 web，以及 6 个基础组件：weaviate、db、redis、nginx、ssrf_proxy 和 sandbox。

然后，打开您的浏览器并访问 `http://localhost` 以访问 Dify。输入必要的信息以完成用户注册。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*DbkqN7eH0GO7kiFp.png)

填写注册信息后，点击“设置”以进入登录页面。输入您刚注册的帐户信息并登录：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ESLspD3sWAb_uW2C.png)

您应该能够成功登录。此时，您本地部署的 Dify 已准备好使用。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*UTVY6nibqfg3tIEJ.png)

当您需要在未来更新本地版本的 Dify 时，请导航到 Dify 源代码中的 `docker` 目录，并按顺序执行以下命令：

```python
cd dify/docker
docker compose down
git pull origin main
docker compose pull
docker compose up -d
```
**接下来，请不要忘记同步您的环境变量配置。此步骤非常重要：**

* 如果 `.env.example` 文件有更新，请务必同步并相应地修改您的本地 `.env` 文件。
* 检查 `.env` 文件中的所有配置项，以确保它们与您实际运行的环境相匹配。您可能需要将 `.env.example` 文件中的新变量添加到 `.env` 文件中，并更新任何更改的值。

# 模型

**模型类型**

Dify 将模型分为 4 种类型，分别用于不同的用途：

1. **系统推理模型：** 用于聊天、名称生成和建议后续问题等任务的应用程序。
* 提供者包括 OpenAI、Azure OpenAI Service、Anthropic、Hugging Face Hub、Replicate、Xinference、OpenLLM、iFLYTEK SPARK、WENXINYIYAN、TONGYI、Minimax、ZHIPU(ChatGLM) Ollama、LocalAI。
1. **嵌入模型：** 用于将分段文档嵌入知识中并处理用户查询的应用程序。
* 提供者包括 OpenAI、ZHIPU (ChatGLM)、Jina AI(Jina Embeddings 2)。

**3. 重新排序模型：** 提升 LLM 的搜索能力。

* 提供者：Cohere。

**4. 语音转文本模型：** 将口语转换为文本，用于对话应用程序。

* 提供者：OpenAI。

# 选择和配置模型

登录并首次进入 Dify Studio 页面后，您需要在 Dify 的“设置”→“模型提供者”部分添加和配置所需的模型。

点击右上角的头像按钮，然后选择“设置”。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*zy4SJ8T1ugy6o4Mk.png)

在左侧菜单中点击“模型提供者”，然后您将在右上角看到“系统模型设置”。在这里，您可以设置系统的默认模型。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*kG_nBVpFCXhkxhby.png)

# 预定义模型集成（以 OpenAI 为例）

Dify 支持主要模型提供商，如 OpenAI 的 GPT 系列和 Anthropic 的 Claude 系列。每个模型的功能和参数各不相同，因此请根据您的应用需求选择合适的模型提供商。在使用 Dify 之前，您可以从模型提供商的官方网站获取 API 密钥。

在这里，我们将以 OpenAI 的 API 密钥为例。使用 API 密钥可以让我们选择更多的模型。本文不详细介绍如何获取 API 密钥。如果您需要集成开源模型，Dify 也支持这一点。例如，您可以通过 Hugging Face 或 Ollama 集成开源模型。当然，您也可以暂时跳过任何设置，使用默认的最新 gpt-4o-mini 模型作为系统推理模型，但在这种情况下，您只能获得 200 个单词的令牌用于试用。

要配置 OpenAI API 密钥，请点击 OpenAI 部分右侧的“设置”按钮，输入 API 密钥，然后点击保存。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*uF1Ax6E9abbhmP4N.png)

一旦 API 密钥配置正确，您将看到“设置”按钮上方的指示器变为绿色。此外，通过点击 OpenAI 标志下方的“27 个模型”按钮，所有可用模型将被显示并可供访问。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*VT90oXf3jRXmAomV.png)

此时，您可以在“系统模型设置”中选择其他所需模型，例如更强大的 gpt-4o。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9hvnftiPTdtThCVW.png)

在这里，我们注意到 Rerank 模型为空，没有可选择的选项。这是因为目前只有 Cohere 和 JinaAI 模型支持重新排序。如果需要，您也可以通过配置各自的 API 密钥来启用它们。

本文不会深入探讨 Rerank 的实践，但我们可以简单了解 Rerank 是什么以及它的作用。


> *混合检索可以结合不同检索技术的优势，以实现更好的召回结果。然而，不同检索模式下的查询结果需要合并和标准化（将数据转换为统一的标准范围或分布，以便于比较、分析和处理）后，才能一起提供给大型模型。在此，我们需要引入一个评分系统：Rerank 模型。*


> *Rerank 模型计算候选文档列表与用户查询之间的语义匹配，基于语义匹配重新排序结果，以改善语义排名结果。其原理是计算用户查询与每个给定候选文档之间的相关性得分，然后返回按相关性从高到低排序的文档列表。常见的 Rerank 模型包括 Cohere rerank、bge-reranker 等。*

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*J2JM7eAm1dbbNXG0.png)


> *在大多数情况下，通常会在重新排序之前进行初始检索，因为计算查询与数百万个文档之间的相关性得分效率非常低。因此，重新排序通常放在搜索过程的最后阶段，使其非常适合合并和排序来自不同检索系统的结果。*


> *其最大优势在于，它不仅提供了一种简单且低复杂度的方法来改善搜索结果，使用户能够将语义相关性纳入现有搜索系统，而且不需要对基础设施进行重大修改。*

# 自定义模型集成 (以 Ollama + Llama 3.1 为例)

Dify 可以与本地模型或托管模型集成，例如 Ollama、LocalAI、Hugging Face、Replicate、Xinference 和 OpenLLM。

在这里，我们将通过将本地部署的 Dify Docker 实例与通过 Ollama 运行的本地 Llama 3.1 实例连接，演示如何在 Dify 中使用开源本地模型。连接到其他平台的方法可以在 [官方文档](https://docs.dify.ai/guides/model-configuration/customizable-model) 中找到。

需要注意的是，如果您想使用 Dify 的官方 Web 版本连接到本地 Ollama 实例，您需要使用 Ngrok 或 Frpc 等工具将本地 Ollama 端口暴露到公共互联网。或者，如果您的网络有公共 IP，您可以使用路由器端口映射将 Ollama 的服务放置在公共互联网。由于网络配置的复杂性和多样性，本文将不深入探讨与网络相关的问题。

在上一篇文章 [如何在 Windows 上运行 Ollama](https://readmedium.com/how-to-run-ollama-on-windows-8a1622525ada) 中，您可以学习如何使用 Ollama 本地运行模型。

一旦 Ollama 和模型准备就绪，请退出 Ollama，然后将 `OLLAMA_HOST` 环境变量添加到您的系统中（您可以通过搜索功能找到此项），并将其值设置为 `0.0.0.0`。这是为了将 Ollama 的服务暴露给 Dify，以便后续调用。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*VKJ8bCr3ofjcj7q0.png)

在保存环境变量后，首先运行 Llama 3.1 实例：

```python
ollama run llama3.1
```
成功启动后，Ollama 在本地端口 11434 上启动 API 服务，可以通过 [`http://localhost:11`434.](http://localhost:11434.) 访问。

现在，让我们检查一下您的计算机的内部 IP 地址。打开命令提示符并输入 `ipconfig`。您应该会看到类似以下的输出：

```python
C:\Users\Edd1e>ipconfig
```

```python
Windows IP Configuration
```

```python
Ethernet adapter Ethernet 5:
```

```python
   Connection-specific DNS Suffix  . : uds.anu.edu.au
   Link-local IPv6 Address . . . . . : fe80::5b74:3153:67b9:412c%27
   IPv4 Address. . . . . . . . . . . : 130.56.120.123
   Subnet Mask . . . . . . . . . . . : 255.255.252.0
   Default Gateway . . . . . . . . . : 130.56.120.1
```

```python
Wireless LAN adapter Wi-Fi:
```

```python
   Connection-specific DNS Suffix  . : anu.edu.au
   Link-local IPv6 Address . . . . . : fe80::5bd:7139:58d7:69eb%15
   IPv4 Address. . . . . . . . . . . : 10.20.96.139
   Subnet Mask . . . . . . . . . . . : 255.255.224.0
   Default Gateway . . . . . . . . . : 10.20.96.1
```
识别 `IPv4 Address` 并复制。如果连接了多个网络接口，您可以记录不同的 `IPv4 Addresses` 并尝试它们。

接下来，运行本地部署的 Dify Docker。

登录 Dify 后，导航到 `Settings > Model Providers > Ollama`，并输入如下图所示的参数：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*nP91y8VVg-f8Pu2V.png)

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*va59zF4YYqjreEa1.png)

图中所示参数的具体含义和填写方法如下：

* 模型名称: `llama3.1`
* 基础 URL: `http://<your-ollama-endpoint-domain>:11434`，其中 URL 由 `http://` 后接之前获得的本地 IP 地址和端口号组成。
* 注意，这里使用 `127.0.0.1` 是不行的；您需要使用本地 IP 地址。
* 如果 Dify 是通过 Docker 部署的，考虑使用本地网络 IP 地址，例如 `http://192.168.1.100:11434` 或 Docker 主机机器 IP 地址，例如 [`http://172.17.0.1:11`434.](http://172.17.0.1:11434.)。
* 如果 Ollama 在 Docker Desktop 中运行，`http://host.docker.internal:11434` 可能是正确的地址，因为 Docker 中的 localhost 与主机的 localhost 不同，因为容器存在于一个单独的网络命名空间中。
* 如果 Dify 是使用本地源代码部署的方法，您可以尝试使用 [`http://localhost:11`434.](http://localhost:11434.)。
* 模型类型: `Chat`
* 模型上下文长度: `4096`
* 模型的最大上下文长度。如果不确定，请使用默认值 4096。
* 最大令牌限制: `4096`
* 模型返回的最大令牌数。如果对模型没有具体要求，这可以与模型上下文长度保持一致。
* 支持视觉: `No`
* 如果模型支持图像理解（多模态），例如 `llava`，请勾选此选项。

在确认没有错误后，点击“保存”以在应用程序中使用该模型。

嵌入模型的集成方法与 LLM 相似，只需将模型类型更改为文本嵌入。

保存后，您将能够看到添加的模型：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*_tdLUtuO-C_ylYjQ.png)

此时，Dify 已准备好与 Ollama 一起工作。建议继续阅读后面的实践部分，以验证本节所做工作的有效性。此外，您可以创建自己的 RAG（检索增强生成）知识库聊天机器人。

如果您使用 Docker 部署 Dify 和 Ollama，您可能会遇到以下错误：

```python
httpconnectionpool(host=127.0.0.1, port=11434): max retries exceeded with url:/cpi/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8562812c20>: fail to establish a new connection:[Errno 111] Connection refused'))
```

```python
httpconnectionpool(host=localhost, port=11434): max retries exceeded with url:/cpi/chat (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8562812c20>: fail to establish a new connection:[Errno 111] Connection refused'))
```
此错误发生是因为 Ollama 服务无法从 Docker 容器访问。`localhost` 通常指的是容器本身，而不是主机机器或其他容器。要解决此问题，您需要将 Ollama 服务暴露到网络上。

以下是在 macOS 和 Linux 上为 Ollama 设置环境变量的方法。Windows 的相关设置在前面已经提到。

如果 Ollama 作为 macOS 应用程序运行，则应使用 `launchctl` 设置环境变量：

1. 对于每个环境变量，调用 `launchctl setenv`。

```python
launchctl setenv OLLAMA_HOST "0.0.0.0"
```
2. 重新启动 Ollama 应用程序。

3. 如果上述步骤无效，您可以使用以下方法：

问题出在 Docker 本身，以及访问 Docker 主机。您应该连接到 `host.docker.internal`。因此，将服务中的 `localhost` 替换为 `host.docker.internal` 将有效地解决问题。

```python
http://host.docker.internal:11434
```
如果 Ollama 作为 systemd 服务运行，则应使用 `systemctl` 设置环境变量：

1. 通过调用 `systemctl edit ollama.service` 编辑 systemd 服务。这将打开一个编辑器。
2. 对于每个环境变量，在 `[Service]` 部分下添加一行 `Environment`：

```python
[Service] 
Environment="OLLAMA_HOST=0.0.0.0"
```
3. 保存并退出。

4. 重新加载 `systemd` 并重新启动 Ollama：

```python
systemctl daemon-reload  
systemctl restart ollama
```

# 应用编排

在 Dify 中，“应用”是指基于大型语言模型如 GPT 构建的实际场景应用。通过创建应用，您可以将智能 AI 技术应用于特定需求。它涵盖了开发 AI 应用的工程范式和具体交付成果。

简而言之，应用为开发者提供了：

* 一个用户友好的 API，可以被后端或前端应用直接调用，通过 Token 进行身份验证
* 一个现成的美观的托管 WebApp，您可以使用 WebApp 模板进一步开发
* 一个易于使用的界面，包括提示工程、上下文管理、日志分析和注释

您可以选择其中任何一个或全部来支持您的 AI 应用开发。

# 创建应用程序

您可以通过 Dify 的工作室以三种方式创建应用程序：

* 基于应用程序模板创建（推荐给初学者）
* 创建空白应用程序
* 通过 DSL 文件创建应用程序（本地/在线）

## 从模板创建应用程序

第一次使用 Dify 时，您可能对创建应用程序不太熟悉。为了帮助新用户快速了解可以在 Dify 上构建的应用程序类型，Dify 团队的提示工程师已经为多种场景创建了高质量的应用程序模板。

您可以从导航菜单中选择“Studio”，然后在应用程序列表中选择“从模板创建”。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*SVBOgMKFDK_-MCSO.png)

选择任何模板并将其添加到您的工作区。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Cl1JJtKmJEfQ94xO.png)

以三步翻译工作流为例，您可以在创建后开始编辑工作流。从 LLM 2 的说明中可以清楚地看出，这是一个专业的中文翻译机器人。它通过检测专业术语、逐字翻译、审核和二次自由翻译等步骤，提高专业文章翻译的准确性。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*N58L6mjHx3vWc12V.png)

点击“发布”和“运行应用”可以让您测试包含许多技术术语的论文摘要作为翻译材料。右侧窗口显示工作流和翻译结果，效果非常出色。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0A5moFofmNxfKmnt.png)

**创建新应用程序**

如果您需要在 Dify 上创建一个空白应用程序，可以从导航中选择“Studio”，然后在应用程序列表中选择“从空白创建”。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-x3e4XpeqXZVuAsy.png)

第一次创建应用程序时，您可能需要先了解 Dify 上四种不同类型应用程序的基本概念：

**应用程序类型**

Dify 提供四种类型的应用程序：

* **聊天助手**：基于 LLM 构建的对话助手
* **文本生成**：用于文本生成任务的助手，如写故事、文本分类、翻译等
* **智能代理**：能够进行任务分解、推理和工具调用的对话智能助手
* **工作流**：基于流程编排定义更灵活的 LLM 工作流

文本生成和聊天助手之间的区别如下表所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hKFa69ULZuALoN8K3zcMsQ.png)

创建应用程序时，您需要为其命名，选择合适的图标，并使用清晰简洁的文本描述该应用程序的目的，以便于团队后续使用。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ygWwijMXC3ehrtk7.png)

# 对话助手

对话应用程序采用一问一答的模式与用户进行持续对话。

## 适用场景

对话应用可以用于客户服务、在线教育、医疗保健、金融服务等领域。这些应用可以帮助组织提高工作效率，降低劳动力成本，并提供更好的用户体验。

## 如何编写

对话应用程序支持提示、变量、上下文、开场白和下一个问题的建议。

在这里，我们以面试官应用程序为例，介绍如何编写对话应用程序。

**步骤 1 创建应用程序**

点击主页上的“创建应用程序”按钮以创建应用程序。填写应用程序名称，并选择 **“**聊天机器人**”** 作为应用程序类型。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ncUuMBljhC32JDUW.png)

**步骤 2：编写应用程序**

应用程序成功创建后，将自动重定向到应用程序概览页面。点击侧边菜单：“编排”以编写应用程序。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*MMajhVC4CiGh62rW.png)

**2.1 填写提示**

提示短语用于指导AI提供专业的响应，使回复更加准确。您可以利用内置的提示生成器来制作合适的提示。提示支持插入表单变量，例如 `{{input}}`。提示变量中的值将被用户填写的值替换。

示例：

1. 输入面试场景命令。
2. 提示将在右侧内容框中自动生成。
3. 您可以在提示中插入自定义变量，以根据特定需求或细节进行调整。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CyYxWnnl9vudv0XM.png)

为了更好的体验，我们将添加一个开场对话：“你好，{{name}}。我是你的面试官，Bob。你准备好了吗？”

要添加开场对话，请点击底部的“添加功能”按钮，并启用“对话开场者”功能：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*d4f78D4ru13EJ-DS.png)

在编辑开场白时，您还可以添加几个介绍性问题：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CmObbuArJJpLr8AZ.png)

**2.2 添加上下文**

如果一个应用程序希望基于私密的上下文对话生成内容，可以使用知识功能。点击上下文中的“添加”按钮以添加知识库。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*UnaTbG4JXG2iqv_9.png)

**2.3 调试**

在右侧输入用户输入并检查响应内容。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2Qc73ZgnuFqupEyr.png)

如果结果不令人满意，您可以调整提示和模型参数。点击右上角的模型名称以设置模型的参数：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*w20vzvPARw8QH5Q4.png)

**使用多个模型进行调试：**

如果使用单个模型进行调试感觉效率低下，您可以利用 **“作为多个模型调试”** 功能批量测试模型的响应效果。

支持同时添加最多 4 个 LLM。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ah5eZAdRNDHJ6hF.png)

**2.4 发布应用**

调试完应用程序后，点击右上角的 **“发布”** 按钮以创建独立的AI应用程序。除了通过公共URL体验应用程序外，您还可以基于API进行二次开发，将其嵌入到网站中等等。

# 代理

代理助手可以利用大型语言模型（LLMs）的推理能力。它独立设定目标，简化复杂任务，操作工具，并优化流程以自主完成任务。

为了便于快速学习和使用，代理助手的应用模板可以在“探索”部分找到。您可以将这些模板集成到您的工作区。新的 Dify “工作室”还允许创建自定义代理助手，以满足个人需求。该助手可以协助分析财务报告、撰写报告、设计徽标和组织旅行计划。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*NMpZye94b0GrBAt4.png)

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*NYAAHgK5m_0hOtx2.png)

**为代理助手编排提示：** 在“指令”部分，您可以定义代理助手的任务目标、工作流程、资源和限制。这通过为代理的行动提供清晰的指导，确保最佳结果。

**为代理助手添加工具：** 在“上下文”部分，您可以集成知识库工具，以帮助信息检索，增强代理的背景知识。“工具”部分允许添加扩展语言学习模型（LLMs）能力的工具，例如互联网搜索、科学计算或图像创作。Dify 支持内置和自定义工具，包括与 OpenAPI/Swagger 和 OpenAI 插件标准兼容的自定义 API 工具。

工具使得在 Dify 上创建更强大的 AI 应用成为可能。它们允许代理助手通过推理、步骤分解和工具调用来执行复杂任务。此外，这些工具还促进了应用程序与外部系统或服务的集成，使得执行代码和访问独特信息源等活动成为可能。

**代理设置：** Dify 为代理助手提供两种推理模式：功能调用和 ReAct。支持功能调用的模型，如 GPT-3.5 和 GPT-4，提供更好且更稳定的性能。对于不支持功能调用的模型，使用 ReAct 推理框架以实现类似结果。代理设置还允许修改代理的迭代限制。

**配置对话开启者：** 您可以为代理助手设置对话开启者和初始问题。此功能在用户第一次互动开始时显示，突出代理可以执行的任务类型，并提供示例问题。

**调试和预览：** 在将代理助手发布为应用之前，您可以对其进行调试和预览。此步骤允许您测试代理在完成任务方面的有效性，并进行必要的调整。

# 应用工具包

在 **Studio — 应用编排** 中，点击 **添加功能** 打开应用工具箱。

应用工具箱为 Dify 的应用提供了各种附加功能：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*VvsSSnJozgrgTMXy.png)

**对话开场**

在对话应用中，AI 会主动说出第一句话或提出问题。您可以编辑开场内容，包括初始问题。使用对话开场可以引导用户提问，解释应用背景，并降低发起对话的门槛。在与 AI 应用的互动中，我们通常对内容安全、用户体验和法律法规有严格要求。在这种情况下，我们需要“敏感内容审核”功能，以为最终用户创造更好的互动环境。

**下一步问题建议**

设置下一步问题建议允许 AI 根据之前的对话生成 3 个后续问题，引导下一轮互动。

**引用和归属**

当启用此功能时，大型语言模型在回答问题时将引用知识库中的内容。您可以在回答下方查看具体的引用细节，包括原始文本片段、片段编号和匹配分数。

**内容审核**

在与 AI 应用的互动中，我们通常对内容安全、用户体验和法律法规有严格要求。在这种情况下，我们需要“敏感内容审核”功能，以为最终用户创造更好的互动环境。

**注释回复**

注释回复功能允许通过手动编辑和注释来自定义高质量的问答响应。

# 工作流程

工作流程通过将复杂任务分解为更小的步骤（节点），减少系统复杂性，降低对提示工程和模型推理能力的依赖，并增强LLM应用程序在复杂任务中的表现。这也提高了系统的可解释性、稳定性和容错能力。

Dify工作流程分为两种类型：

* **聊天流程（Chatflow）**：为对话场景设计，包括客户服务、语义搜索和其他需要多步骤逻辑构建响应的对话应用程序。
* **工作流程（Workflow）**：面向自动化和批处理场景，适用于高质量翻译、数据分析、内容生成、电子邮件自动化等。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Rprjn2k8ZCDJVHxh.png)

为了应对自然语言输入中用户意图识别的复杂性，聊天流程提供了问题理解节点。与工作流程相比，它增加了对聊天机器人功能的支持，如对话历史（内存）、注释回复和回答节点。

为了处理自动化和批处理场景中的复杂业务逻辑，工作流程提供了多种逻辑节点，如代码节点、IF/ELSE节点、模板转换、迭代节点等。此外，它还提供了定时和事件触发操作的能力，便于构建自动化流程。

节点是工作流程中的基本组件，通过将具有各种功能的节点连接起来，实现一系列操作。

**核心节点：**

1. **开始（Start）：** 设置开始工作流程过程的初始参数。
2. **结束（End）：** 定义结束工作流程过程的最终输出。
3. **回答（Answer）：** 指定聊天流程过程中的响应内容。
4. **大型语言模型（LLM）：** 利用大型语言模型回答问题或处理自然语言。
5. **知识检索（Knowledge Retrieval）：** 从知识库中获取相关文本内容，以便在下游LLM节点中提供上下文。
6. **问题分类器（Question Classifier）：** 使用定义的分类描述，使LLM能够对用户输入进行分类。
7. **IF/ELSE：** 根据条件语句将工作流程分成两个分支。
8. **代码执行（Code Execution）：** 使用Python/NodeJS代码执行自定义逻辑，如数据转换。
9. **模板转换（Template Transformation）：** 使用Jinja2（一种Python模板语言）进行灵活的数据转换和文本处理。
10. **变量聚合器（Variable Aggregator）：** 将多个分支中的变量合并为一个，以便在下游节点中统一配置。
11. **参数提取器（Parameter Extractor）：** 使用LLM推断并从自然语言中提取结构化参数，以便后续工具使用或HTTP请求。
12. **迭代（Iteration）：** 对列表对象重复步骤，直到处理完所有结果。
13. **HTTP请求（HTTP Request）：** 使用HTTP协议发送服务器请求，以进行外部数据检索、Webhook、图像生成等。
14. **工具（Tools）：** 允许在工作流程中集成内置的Dify工具、自定义工具、子工作流程等。

构建工作流程的过程灵活而复杂。本文提供了详细的介绍，允许您自由探索。

# 实现 RAG：使用 Ollama 和 Dify 创建简单的本地知识库聊天应用

大型语言模型的训练数据一般基于公开可用的数据，每次训练会话都需要大量的计算能力。这意味着模型的知识通常不包括私有领域知识，并且在公共知识领域存在一定的延迟。为了解决这个问题，目前常见的解决方案是使用 RAG（检索增强生成）技术，它利用用户的问题来匹配最相关的外部数据，在检索到相关内容后，重新组织并将响应插入模型提示的上下文中。

Dify 的知识库功能可视化了 RAG 流程中的每一步，提供了一个简单易用的用户界面，帮助应用构建者管理个人或团队知识库，并迅速将其集成到 AI 应用中。您只需准备文本内容，例如：

* 长文本内容（TXT、Markdown、DOCX、HTML、JSONL，甚至 PDF 文件）
* 结构化数据（CSV、Excel 等）

在 Dify 中，知识是文档的集合。知识库可以作为检索上下文集成到应用中。文档可以由开发者或运营团队的成员上传，或从其他数据源同步（通常对应于数据源中的一个单元文件）。

> ***什么是 RAG？***

> *RAG 架构以向量检索为核心，已成为使大型模型能够访问最新外部知识的主流技术框架，同时解决生成内容中的幻觉问题。这项技术已在多种应用场景中得以实现。*

> *开发者可以利用这项技术以低成本构建 AI 驱动的客户服务、企业知识库、AI 搜索引擎等。通过使用自然语言输入与各种知识组织形式进行交互，他们可以创建智能系统。让我们以一个典型的 RAG 应用为例：*

> *在下面的图示中，当用户询问“美国总统是谁？”时，系统并不会直接将问题传递给大型模型以获取答案。相反，它首先在知识库中执行向量搜索（如图中所示的维基百科），通过语义相似性匹配找到相关内容（例如，“乔·拜登是美国第46任现任总统……”）。然后，系统将用户的问题与检索到的相关知识一起提供给大型模型，使其能够获得足够的信息来可靠地回答问题。*

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Q5qFR_Lv9Zvy4nhy.png)

> ***为什么这很必要？***

> *我们可以将大型模型视为一位熟悉人类知识各个领域的超级专家。然而，它有其局限性。例如，它不知道关于您的个人信息，因为这些信息是私密的，互联网上没有公开，因此没有学习的机会。*

> *当您想要聘请这位超级专家作为您的个人财务顾问时，您需要允许他们查看您的投资记录、家庭开支和其他数据，然后再回答您的问题。这样，专家才能根据您的个人情况提供专业建议。*

> ***这正是 RAG 系统所做的：它帮助大型模型临时获取它所不具备的外部知识，使其能够在回答问题之前找到答案。***

> *从上面的例子中，我们可以很容易地看到 RAG 系统中最关键的部分是外部知识的检索。专家能否提供专业的财务建议，取决于他们能否准确找到必要的信息。如果他们找到的是您的减肥计划而不是投资记录，即使是最有知识的专家也无能为力。*

让我们动手创建一个简单的具有知识库的聊天机器人。以下步骤基于之前 Dify 和 Ollama 的本地部署和集成。

首先，点击 `Knowledge > Create Knowledge`。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*P6EB1SACVxgO6McJ.png)

选择 `Import from file` 上传文档。在这里，我将使用 Nvidia 最新的论文 *ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities* 作为示例。上传后，点击 `Next`。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BL7VZD72ORLYYqkM.png)

论文摘要

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4d7n4GojIKJNEfb5.png)

点击 `Save & Process` 完成知识库的创建。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fJobwBFQ2nVmnKVx.png)

看到“Knowledge Created”提示表示知识库创建完成。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4wnUibEAyMqCZQXF.png)

知识库创建完成后，前往 `Studio > Create from Blank`。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FkoRvoiM4Hvjrwmr.png)

选择聊天机器人类型进行测试，您可以随意命名。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ARRHixAlbbV43RV4.png)

点击 `Context` 旁边的 `Add` 按钮从知识库中添加文件。您还会注意到右上角显示本地 Ollama 模型正在运行，表明 Dify 和 Ollama 已成功集成。点击它可以选择其他模型或调整模型参数。在左侧的 `Instructions` 输入框中，您可以为聊天机器人编写提示。设置完成后，点击 `Publish` 保存。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lmZ6RRZpzDH-uaIp.png)

点击 `Publish` 后，您可以直接点击 `Run App` 运行应用。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Pnqkg_BWGanTcjhf.png)

通过询问与论文中 ChatQA 2 相关的问题，我们可以看到机器人利用知识库中的文档并提供了准确的答案，展示了 RAG 的有效性。至此，一个简单的 RAG 聊天机器人成功创建。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H-Q9A3b8wdJjnksK.png)

# 结论

本文全面介绍了 Dify，一个开源平台，简化了大语言模型（LLM）应用的开发。Dify 的关键特性，如提示编排、RAG 流水线和广泛的模型支持，使其对开发者和非技术用户都很友好。该平台将后端即服务与 LLMOps 结合，使用户能够快速创建和部署 AI 应用，而无需 extensive coding。通过详细说明云端和本地部署选项，本文为读者提供了实用知识，以利用 Dify 构建和管理可扩展的 AI 解决方案。Dify 的开源特性和社区驱动的方法提供了灵活性和持续改进，使其成为任何希望在 AI 领域进行创新的人的宝贵工具。本指南使读者能够自信地开始或增强他们的 AI 开发之旅，使用 Dify。

# 参考文献

* “欢迎使用 Dify | Dify。” *Dify.ai*, 2024年4月9日, docs.dify.ai/. 访问时间 2024年7月26日。
* “ConnectionError: HTTPConnectionPool(Host=“Localhost”, Port=11434): Max Retries Exceeded with Url: /Api/Generate/ (Caused by NewConnectionError(’: Failed to Establish a New Connection: [Errno 111] Connection Refused’)) · Issue #3200 · Ollama/Ollama。” *GitHub*, github.com/ollama/ollama/issues/3200. 访问时间 2024年7月26日。
* “Docker Desktop Distro 安装在升级到 4.32.0 后失败。” *Docker Community Forums*, 2024年7月10日, forums.docker.com/t/docker-desktop-distro-installation-failed-after-upgradation-to-4–32–0/142393. 访问时间 2024年7月26日。
* “Langgenius/Dify。” *GitHub*, 2024年5月23日, github.com/langgenius/dify.
* “Issues · Ollama/Ollama。” *GitHub*, github.com/ollama/ollama/issues/. 访问时间 2024年7月26日。
