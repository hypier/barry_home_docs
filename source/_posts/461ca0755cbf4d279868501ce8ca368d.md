
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6GIC4KplDWtnL2lEwGR_UA.png
date: '2024-02-05 22:16:01'
tags:
  - RAG评估
  - LlamaIndex
  - 生成模型
title: 高级RAG 03使用RAGAs  LlamaIndex进行RAG评估

---


## 包含原理、图表和代码

如果您为实际业务系统开发了检索增强生成（RAG）应用程序，您很可能会关注其效果。换句话说，您希望评估RAG的表现如何。

此外，如果您发现现有的RAG效果不够理想，**您可能需要验证高级RAG改进方法的有效性**。换句话说，您需要进行评估，看看这些改进方法是否有效。

**在本文中，我们首先介绍由[RAGAs（检索增强生成评估）](https://arxiv.org/pdf/2309.15217.pdf)提出的RAG评估指标**，这是一个用于评估RAG管道的框架。**然后，我们解释如何使用RAGAs + LlamaIndex实现整个评估过程。**

# RAG评估指标

**简而言之，RAG过程涉及三个主要部分**：输入查询、检索到的上下文以及LLM生成的响应。这三个元素构成了RAG过程中最重要的三元组，并且相互依赖。

因此，可以通过衡量这些三元组之间的相关性来评估RAG的有效性，如图1所示。



[论文](https://arxiv.org/pdf/2309.15217.pdf)中提到了总共3个指标：忠实度（Faithfulness）、答案相关性（Answer Relevance）和上下文相关性（Context Relevance），这些指标不需要访问人工标注的数据集或参考答案。

此外，[RAGAs网站](https://docs.ragas.io/en/latest/concepts/metrics/index.html)还介绍了另外两个指标：上下文精确度（Context Precision）和上下文召回率（Context Recall）。

## 忠实度/基于事实性

忠实度指的是确保答案基于给定的上下文。这对于避免幻觉和确保检索到的上下文能作为生成答案的依据至关重要。

如果得分较低，表明LLM的回应不符合检索到的知识，提供虚幻答案的可能性增加。例如：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WaC1zztISLJ-WN7VCwIsGw.png)

为了评估忠实度，我们首先使用LLM提取一组陈述，`**S(a(q))**`。方法是使用以下提示：

```python
给定一个问题和答案，从答案的每个句子中创建一个或多个陈述。
question: [question]
answer: [answer]
```
生成`**S(a(q))**`后，LLM判断每个陈述si是否能从`**c(q)**`推断出来。这一验证步骤通过以下提示进行：

```python
考虑给定的上下文和以下陈述，然后判断它们是否得到上下文中信息的支撑。在得出结论（是/否）前，为每个陈述提供简短解释。按照指定格式，在最后依次为每个陈述提供最终结论。请勿偏离指定格式。

statement: [statement 1]
...
statement: [statement n]
```
最终的忠实度得分`**F**`计算为`**F = |V| / |S|**`，其中`**|V|**`表示LLM支持的陈述数量，`**|S|**`表示陈述总数。

## 答案相关性

此指标衡量生成答案与查询之间的相关性。得分越高，表示相关性越好。例如：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Xi0gJgpHR0Q_bYIRkhGd6A.png)

为了估计答案的相关性，我们提示LLM根据给定的答案`**a(q)**`生成n个潜在问题`**qi**`，如下所示：

```python
Generate a question for the given answer.

answer: [answer]
```
然后，我们利用文本嵌入模型获取所有问题的嵌入表示。

对于每个`**qi**`，我们计算其与原始问题`**q**`的相似度`**sim(q, qi)**`。这对应于嵌入之间的余弦相似度。问题`**q**`的答案相关性得分`**AR**`计算如下：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pE3MZF3cvPQX_qPq_dkpKA.png)

## 上下文相关性

这是一个衡量检索质量的指标，主要评估检索到的上下文在多大程度上支持查询。低分表明检索到了大量不相关的内容，这可能会影响LLM生成的最终答案。例如：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jfbqIbMEAcojDoD_MRi22Q.png)

为了估计上下文的相关性，从上下文（`**c(q)**`）中使用LLM提取一组关键句子（`**Sext**`）。这些句子对于回答问题至关重要。提示如下：

```python
请从提供的上下文中提取可能有助于回答以下问题的相关句子。
如果未找到相关句子，或者您认为无法根据给定的上下文回答问题，
请返回短语“Insufficient Information”。
在提取候选句子时，不允许对给定上下文中的句子进行任何更改。
```

然后，在RAGAs中，使用以下公式在句子级别计算相关性：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-yWsfTEAw1KOKT0qoCGUjg.png)

## 上下文一致性评估

该指标衡量检索到的上下文与标注答案之间的一致性程度。它通过使用真实标签和检索到的上下文来计算，数值越高表示性能越好。例如：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fMAgzplM1FuA33KAXvZXyQ.png)

在实施时，需要提供真实标签数据。

计算公式如下：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uIyxtKvWPxIwwn302XfS2g.png)

## 上下文精度

该指标相对复杂，用于衡量所有包含真实事实的相关上下文是否都被排在最前面。分数越高，表示精度越高。

该指标的计算公式如下：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uQGuHPp5hbrLZv0WacyZKg.png)

上下文精度的优势在于其能够感知排序效果。然而，其缺点是如果相关召回非常少，但它们都排名很高，分数也会很高。因此，需要结合其他几个指标来综合考虑整体效果。

# 使用 RAGAs + LlamaIndex 进行 RAG 评估

主要流程如图 6 所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JjIjndOyuMuxnDnkOV-Vxw.png)

## 环境配置

安装 ragas：`pip install ragas`。然后，检查当前版本。

```python
(py) Florian:~ Florian$ pip list | grep ragas
ragas                        0.0.22
```
值得一提的是，如果你使用 `pip install git+https://github.com/explodinggradients/ragas.git` 安装最新版本（v0.1.0rc1），则不支持 LlamaIndex。

接下来，导入相关库，设置环境和全局变量

```python
import os
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_KEY"
dir_path = "YOUR_DIR_PATH"

from llama_index import VectorStoreIndex, SimpleDirectoryReader

from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_recall,
    context_precision
)

from ragas.llama_index import evaluate
```
目录中只有一个PDF文件，使用的是论文“[TinyLlama: An Open-Source Small Language Model](https://arxiv.org/pdf/2401.02385.pdf)”。

```python
(py) Florian:~ Florian$ ls /Users/Florian/Downloads/pdf_test/
tinyllama.pdf
```

## 使用 LlamaIndex 构建一个简单的 RAG 查询引擎

```python
documents = SimpleDirectoryReader(dir_path).load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
```
默认情况下，LlamaIndex 使用 OpenAI 模型，LLM 和嵌入模型可以通过 ServiceContext 轻松配置。

## 构建评估数据集

由于某些指标需要手动标注的数据集，我自行编写了一些问题及其对应的答案。

```python
eval_questions = [
    "能否简要描述TinyLlama模型的特点？",
    "我想了解TinyLlama在速度优化方面做了哪些改进？",
    "为什么TinyLlama使用Grouped-query Attention？",
    "TinyLlama模型是开源的吗？",
    "请介绍一下starcoderdata数据集",
]
eval_answers = [
    "TinyLlama是一个紧凑的1.1B语言模型，在大约1万亿个token上预训练了约3个周期。基于Llama 2的架构和分词器，TinyLlama利用了开源社区贡献的各种先进技术（例如FlashAttention），实现了更好的计算效率。尽管体积相对较小，TinyLlama在一系列下游任务中表现出色，显著优于现有同规模的开放源代码语言模型。",
    "在训练过程中，我们的代码库集成了FSDP，以高效利用多GPU和多节点设置。另一个关键改进是集成了Flash Attention，一种优化的注意力机制。我们将xFormers（Lefaudeux等人，2022年）库中的融合SwiGLU模块替换为原始的SwiGLU模块，进一步提高了代码库的效率。通过这些特性，我们可以减少内存占用，使1.1B模型能够适应40GB的GPU内存。",  
    "为了减少内存带宽开销并加速推理，我们在模型中使用了grouped-query attention。我们的查询注意力有32个头，并使用4组键值头。通过这种技术，模型可以在不牺牲太多性能的情况下，跨多个头共享键和值表示。",
    "是的，TinyLlama是开源的。",
    "该数据集是为了训练StarCoder（Li等人，2023年），一个强大的开源大型代码语言模型而收集的。它包含约2500亿个token，涵盖86种编程语言。除了代码外，还包括GitHub问题和涉及自然语言的文本-代码对。",
]
eval_answers = [[a] for a in eval_answers]
```

## 指标选择与RAGAs评估

```python
metrics = [
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_precision,
    context_recall,
]

result = evaluate(query_engine, metrics, eval_questions, eval_answers)
result.to_pandas().to_csv('YOUR_CSV_PATH', sep=',')
```
需要注意的是，在RAGAs中，默认使用OpenAI模型。

在RAGAs中，如果你想使用其他LLM（如Gemini）结合**LlamaIndex**进行评估，我在RAGAs版本0.0.22中尚未找到有效方法，即便调试了RAGAs的源代码。

## 最终代码

```python
import os
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_KEY"
dir_path = "YOUR_DIR_PATH"

from llama_index import VectorStoreIndex, SimpleDirectoryReader

from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_recall,
    context_precision
)

from ragas.llama_index import evaluate

documents = SimpleDirectoryReader(dir_path).load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

eval_questions = [
    "Can you provide a concise description of the TinyLlama model?",
    "I would like to know the speed optimizations that TinyLlama has made.",
    "Why TinyLlama uses Grouped-query Attention?",
    "Is the TinyLlama model open source?",
    "Tell me about starcoderdata dataset",
]
eval_answers = [
    "TinyLlama is a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes.",
    "During training, our codebase has integrated FSDP to leverage multi-GPU and multi-node setups efficiently. Another critical improvement is the integration of Flash Attention, an optimized attention mechanism. We have replaced the fused SwiGLU module from the xFormers (Lefaudeux et al., 2022) repository with the original SwiGLU module, further enhancing the efficiency of our codebase. With these features, we can reduce the memory footprint, enabling the 1.1B model to fit within 40GB of GPU RAM.",  
    "To reduce memory bandwidth overhead and speed up inference, we use grouped-query attention in our model. We have 32 heads for query attention and use 4 groups of key-value heads. With this technique, the model can share key and value representations across multiple heads without sacrificing much performance",
    "Yes, TinyLlama is open-source",
    "This dataset was collected to train StarCoder (Li et al., 2023), a powerful opensource large code language model. It comprises approximately 250 billion tokens across 86 programming languages. In addition to code, it also includes GitHub issues and text-code pairs that involve natural languages.",
]
eval_answers = [[a] for a in eval_answers]

metrics = [
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_precision,
    context_recall,
]

result = evaluate(query_engine, metrics, eval_questions, eval_answers)
result.to_pandas().to_csv('YOUR_CSV_PATH', sep=',')
```
注意，在终端运行程序时，pandas dataframe 可能无法完全显示。要查看它，可以将其导出为 CSV 文件，如图 6 所示。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-7Bh7n84dUxWNjE2VmbLXw.png)

从图 6 可以看出，第四个问题“`Tell me about starcoderdata dataset`”全是 0，这是因为 LLM 无法提供答案。第二个和第三个问题上下文精度为 0，表明检索到的相关上下文没有排在最前面。第二个问题的上下文召回率为 0，表明检索到的上下文与标注答案不匹配。

现在，让我们看看第 0 到 3 个问题。这些问题的答案相关性得分很高，表明答案与问题之间有很强的关联性。此外，忠实度得分并不低，这表明答案主要来自上下文的推导或总结，可以得出结论，这些答案并非 LLM 幻觉产生。

此外，我们发现尽管上下文相关性得分较低，但 gpt-3.5-turbo-16k（RAGAs 的默认模型）仍能从中推导出答案。

根据结果，显然这个基本的 RAG 系统仍有很大的改进空间。

# 结论

总体而言，RAGAs 提供了全面的指标来评估 RAG，并提供了便捷的调用方式。目前，RAG 评估框架尚不完善，RAGAs 提供了一个有效的工具。

在调试 RAGAs 的内部源代码后，可以明显看出 RAGAs 仍处于早期开发阶段。我们对其未来的更新和改进持乐观态度。

最后，如果您对本文有任何疑问，请在评论区指出。

# 用简单英语 🚀

*感谢您成为[**用简单英语**](https://plainenglish.io)社区的一员！在您离开之前：*

* 请务必**点赞**并**关注**作者️👏**️️**
* 关注我们：[**X**](https://twitter.com/inPlainEngHQ) **| [LinkedIn](https://www.linkedin.com/company/inplainenglish/) | [YouTube](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) | [Discord](https://discord.gg/in-plain-english-709094664682340443) | [Newsletter](https://newsletter.plainenglish.io/)**
* 访问我们的其他平台：[**Stackademic**](https://stackademic.com/) **| [CoFeed](https://cofeed.app/) | [Venture](https://venturemagazine.net/)**
* 更多内容请访问[**PlainEnglish.io**](https://plainenglish.io)
