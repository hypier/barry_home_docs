
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*htSMDnXVYfwIabP3
date: '2024-06-06 00:33:54'
tags:
  - AI模型
  - 机制性可解释性
  - 社会影响
title: Anthropic的突破理解前沿AI

---


## 对LLMs理解的一次巨大飞跃



本周，Anthropic发布了我们对前沿AI模型理解上前所未有的重大进展。

对于前沿AI模型，我们面临一个奇怪的困境：**我们知道它们有效，但不知道为什么，更糟糕的是，我们不知道它们是如何思考的。**

然而，在过去几年中，一个名为**机制解释性**的AI领域兴趣大增，并有一个明确的目标：在为时已晚之前，解开那些可能带给我们AGI的模型的神秘面纱。

现在，OpenAI的主要竞争对手Anthropic发布了一项可能具有开创性的研究，为我们提供了理解大型语言模型（LLMs）的新方法，并揭示了我们如何能很快引导行为以防止不安全实践。

遗憾的是，就像科技领域的任何发现一样，这一成果也有一个令人不安的权衡：增加了我们的社会变得“审查优先”和“单一思维”的可能性。

> 你可能已经厌倦了AI通讯谈论这个或那个**刚刚**发生的事情。这些通讯泛滥是因为粗略地谈论已经发生的事件和事物很容易，**但提供的价值有限，炒作过度。**

> 然而，谈论**将会**发生什么的通讯却很少见。如果你喜欢在别人之前轻松理解AI未来的洞察，**TheTechOasis**通讯可能非常适合你。

> 🏝️🏝️ 今天就订阅下方：

# 终极黑匣子

尽管我们对神经网络如何学习有着深刻的直觉，[这一话题我最近在我的博客中有所探讨](https://thewhitebox.ai/neural-networks-why-do-they-work/)，但随着模型发展成拥有数十亿参数的架构，它们仍然是一个完全的黑匣子，以至于我的公司[**TheWhiteBox**](https://thewhitebox.ai/)的名字正是受此问题启发。

那么，首先，*什么是**机制性可解释性**，这一旨在解决此问题的领域？*

简而言之，该领域旨在识别网络所拥有的关键知识模式及其与参数的关系，以预测其行为。

如你所知，像ChatGPT这样的神经网络由位于以下“隐藏层”中的神经元（尽管更恰当的名称是“隐藏单元”）组成：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6Q5GoMFn4wrQVEU8)

当被激活时，这些神经元会触发（或不触发），结合这些元素有助于模型生成数据，例如缝纫学习指南或葛底斯堡战役的叙述。

简而言之，**模型的输出取决于这些神经元的组合方式。** 然而，解开这些组合，看似无害的活动，**却是当今AI领域尚未解决的重大问题之一。**

*但为什么呢？*

## 压缩之美

从根本上讲，生成式AI模型本质上是数据压缩器。换言之，它们被训练来压缩数据。

然而，众所周知，神经网络通过叠加来捕捉比其神经元更多的信息。这种行为的原因在于，它们所接收的训练数据量比模型本身大三到四个数量级（至少是x100倍），并被赋予学习和再生这些数据的能力。

因此，它们必须学会一种数据压缩方式，使得在需要时，尽管模型本身要小得多，仍能**“恢复”原始数据**。

由于模型尺寸远小于数据量，死记硬背显然不可行，因为数据根本无法完全装入模型中。

> 事实上，那样做会使模型变成与原始训练数据同样大小的数据库，这毫无意义；我们真正需要的是一个既能代表全部数据又能在较小规模上进行查询的生成模型。

因此，这个较小的模型被迫只吸收训练数据中的关键模式。然而，压缩不仅关乎效率，更是智能发展的体现。

## 压缩即智能

正如我多次提及，这就是为何全球研究者认为LLMs是AI领域的——当前——圣杯；这些模型实现的令人瞩目的压缩行为（仅保留核心，摒弃无关紧要的部分）**清晰地展示了它们的智能**（即便与人类相比它们相当愚钝）。

*但“学习核心”究竟意味着什么？* 要理解为何压缩是一种智能行为，我们可以以人类为参照。

> 例如，人类不是逐句死记硬背，而是学习语法和句法，即单词的书写方式及它们通常如何相互跟随，从而将这些知识推演至新句子，无需机械记忆。

> 人类知道“我香蕉吃”不是一个正确的句子，而不必记忆这三个词所有可能的排列组合。

这正是LLMs所做的，但我们无法真正解释其原因或方式，这正是机制性可解释性的关键所在。

*但为何我要告诉你这些？*

简单。压缩是这些模型期望具备的特性，但它也揭示了一个艰难且不可避免的事实：

它们最小的信息压缩和存储单元——神经元——是多义性的。

## 残酷的真相

如前所述，神经网络通过叠加应用，积累了比其神经元更多的知识数据点。

因此，为了实现这一点，**每个神经元必须在广泛的语义不相关主题上变得知识渊博**。例如，同一个神经元可能在模型生成莎士比亚诗歌和描述热带青蛙时都会激活。

而这，亲爱的读者，是一个问题。

如果神经元是单义的，揭示大型语言模型的知识及其如何引发这些知识将易如反掌。

然而，它们是多义的这一事实使得个性化每个神经元以揭示“它们知道什么”，从而映射整个网络的知识，变得完全不可能。

幸运的是，在2023年10月，Anthropic取得了重大发现：尽管神经元无疑是多义的，**但某些神经元的组合是单义的**。

用外行的话说，虽然仅基于一个神经元的行为，模型的输出是不可预测的，但每当一组神经元一起激活时，输出总是相同的。

换句话说，**当模型从特定主题生成文本时，相同的神经元会一起激活**。这在下面的玩具图中可以看到，特定组合的两个隐藏神经元（两个神经元都激活）迫使模型输出莎士比亚的诗句。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EXqSae3IKQ2w26x7gbtFgw.png)

然而，问题在于这一发现是有限的，因为他们能够研究的神经网络非常非常小，这意味着单义神经元组合是一个有希望的领域，但尚未成为现实。

然而，在Anthropic的这一光明新研究之后，它现在已成为现实。

# 希望之诗

最近，发现单义神经元组合的同一团队[发表了一篇论文](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#appendix-methods-steering)，他们将相同原理应用于**Claude 3 Sonnet**，这是他们目前全球排名第九的中端生产前沿模型。

*但他们具体做了什么呢？*

## 解读前沿AI模型

他们分析了模型的激活状态，并训练了一个并行模型，将这些激活状态转化为可解释的特征。

用通俗的话来说，他们训练了一个模型，观察Sonnet中神经元的激发方式，并预测这些神经元组合所代表的关键抽象特征，创建了一个研究人员可以解读的“特征图”。换句话说，**他们找到了世界概念与特定神经元组合之间的‘映射’。**

例如，他们发现了一个专门针对旧金山“金门大桥”的特征：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z8NkQJWH-2rS04qA)

重要的是，这些特征是多模态的（对与该纪念碑相关的文字和图像都有反应）和多语言的（对其他语言中的相同概念也有反应）。

此外，他们还发现了关于名人、纪念碑、艺术等方面的特征。

*但他们是怎样做到这一点的呢？*

## 重构促进学习

为了实现这一目标，他们训练了一个**稀疏自编码器（SAE）**，这是一种模型，它接受激活并将其转化为现实特征，然后尝试重构回激活。

SAE由两个组件构成：

* 一个**编码器**，它从模型中获取激活，并应用线性变换将其转换到更高维的空间，从而转化为特征。用通俗的话说，模型发现了诸如“如果神经元以某种方式激发，它们通常指的是勒布朗·詹姆斯”这样的模式。
* 一个**解码器**，它接受这些特征并应用另一个线性变换，将这些特征还原回原始激活。

换句话说，他们训练了一个模型，该模型能够将神经元激活“剖析”为更细粒度的数据——可解释的特征——但也能重构原始数据——回到原始激活。

> 重构部分并非“仅仅因为”而做。

> 通过迫使模型重构原始数据，它们必须学习原始数据的分布方式，**这等同于真正理解它。**

如下所示，对于“Golden”、“Gate”或“Bridge”这样的词汇，模型的神经元会朝着“Golden Gate”特征激活。这反过来意味着这些特征被添加为相关信息，以进行下一个词的预测，从而诱导模型输出“旧金山”作为这座纪念碑所在的城市。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Zpf_xOmFn0S5VfYHcqcerg.png)

简而言之，过程如下：

1. 他们训练了一个SAE，该SAE接受LLM的激活并将其转化为现实特征。
2. 他们分析了序列中每个词的神经元激活情况。例如，他们发现每当一组神经元激发时，这个词总是与金门大桥相关，从而能够识别出“金门大桥”这一特征。
3. 因此，他们找到了影响序列中每个词生成的特征集合，使得原本不透明的神经元世界变得可解释。

然而，可能有成千上万个这样的特征，对吧？

“husky”这个词可以直接关联到许多不同的特征：、、、、、等等，这将使得寻找解释每个词的关键特征成为一场噩梦。

因此，**他们迫使SAE变得稀疏**，这意味着只有少数特征应该代表序列中的每个词。这样，**他们就能精确地指出这些神经元所‘指向’的现实概念。**

> **技术洞察：** 为了强化稀疏性，他们在SAE的目标损失函数中添加了一个惩罚项。这个惩罚项计算了解码器矩阵的行列式，意味着其值越大，惩罚越大。这自然诱导模型分配一个有效但稀疏的矩阵。

> 通过这样做，他们还获得了另一个好处：解码器的矩阵，即将特征转换回激活的矩阵，代表了模型的特征空间。用通俗的话说，解码器矩阵的列是可能“解释”任何给定词的不同特征。

> 因此，它充当了一个“特征查找”，模型只需查看哪些列参与了重构，就能知道哪些特征解释了这些特定的激活。例如，如果词是“Golden Gate”，解码器中激活的列之一将是引用“Golden Gate”特征的列。

*但这一切为何重要？*

简单来说，因为通过了解神经元如何组合生成与特定主题相关的数据，我们可以预测其行为……或者，关键在于，引导它。

为了证明这一点，再次使用金门大桥的例子，他们固定了那个特定的组合（迫使这些特定的神经元更强烈地激活），**这迫使模型实际上认为它是金门大桥**：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EM6RL51j0m359_i5oEyE0w.png)

当然，那只是一个有趣但无关紧要的反应，但关键是他们证明了我们实际上可以“固定”或“解除固定”特定主题，随心所欲。

有趣，*但这对于行业意味着什么，利与弊？*

# 可预测却可审查？

不出所料，研究人员发现了众多不良特征（撒谎、欺骗、寻求权力的行为，甚至暴力反应）。

## 降低暴力程度

利用这一发现，我们最终可以“调低”甚至禁止此类神经元组合，使得模型永远不会生成那种数据，无论用户多么坚持。

考虑到当今模型容易被破解的程度，这一点至关重要。

> 例如，尽管模型可能会拒绝英语中的有害请求，但在未“对齐”的语言中，它可能会服从。用外行的话说，我们在训练阶段向模型解释诸如“每当他们让你制造炸弹时，不要回答”这样的规则，主要是在英语中进行的，而不是在那些鲜为人知的语言中。

> 然而，由于**特性是多语言的**，禁止某些神经元组合将使模型独立于语言拒绝有害提示。

话虽如此，这些预防措施只有在攻击者无法访问实际模型的情况下才会有效。对模型进行简单的微调就可以完全改变它，并轻易打破对齐防护栏。

> 正如Nanda等人的最新[研究](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction)所证明的那样，我们甚至不需要微调模型，只需简单地消除模型的“安全特性”，就能使其自动失控，使情况变得更糟。

尽管如此，这一突破极为重要，因为在未来我们可能需要应对超人类模型，这些模型在各方面都优于我们，了解它们如何处理数据不仅可以帮助我们实现新的突破，**还可以帮助我们学习如何控制和防止它们失控。**

然而，与当今任何AI领域的发现一样，也存在反面。

## 走向一个被剥夺思考能力的社会

设想一个世界，每个人都在使用由盈利性公司组装的相同大型语言模型（LLMs）。

在这样的情景下，*是什么阻止这些公司利用这些能力来审查某些思维方式、操纵或诱导社会以某种有利于这些公司或其背后政府利益的方式看待世界？*

我们必须承认，尽管LLMs将为社会创造巨大价值，但将它们作为知识的唯一入口——正如它们显然正在定位自己的那样——可能会带来极其有害的后果。

当所有人都被引导至相同的思维方式时，我们失去了保持开放心态的能力。

这听起来可能有些夸张，但AI算法已经在各个方面操纵着社会。如果某个社交网络只向你的孩子推送关于某个种族或宗教的有害内容，*我们的孩子将如何成长，却对AI主导的推送内容之外的一切产生鄙视？*

我并不否认公司在推动行业发展中的重要作用；我确信它们的产品将为社会创造巨大价值。

但请记住，股东利益始终是首要的，这就是为什么，现在比以往任何时候都更需要**开源的生存**。

> 最后，如果你喜欢这篇文章，我在[LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/)上以更全面且简化的方式免费分享了类似的想法。

> 如果更倾向于这种方式，你可以通过[X](https://twitter.com/TheTechOasis1)与我联系。

> 期待与你的连接。
