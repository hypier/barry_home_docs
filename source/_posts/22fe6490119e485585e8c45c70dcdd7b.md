
---
categories: äººå·¥æ™ºèƒ½
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tZKb3rGxaatEN9ovIqSzlg.png
date: '2024-08-18 22:02:58'
tags:
  - AIé©±åŠ¨æ£€ç´¢
  - åˆ†å—å¤„ç†
  - æ£€ç´¢å¢å¼ºç”Ÿæˆ
title: åˆ†å—è‰ºæœ¯æå‡ RAG æ¶æ„ä¸­ AI æ€§èƒ½

---


## æœ‰æ•ˆçš„AIé©±åŠ¨æ£€ç´¢çš„å…³é”®

[å…è´¹é“¾æ¥](https://towardsdatascience.com/the-art-of-chunking-boosting-ai-performance-in-rag-architectures-acdbdb8bdc2b?sk=ddf15d6997d51dd53bcc09b9e586db6d): è¯·åƒè¿™æ ·å¸®åŠ©æˆ‘ [LinkedInå¸–å­](https://www.linkedin.com/posts/hanheloiryan_the-art-of-chunking-boosting-ai-performance-activity-7230999707009908736-z2Dr?utm_source=share&utm_medium=member_desktop)ã€‚

**èªæ˜çš„äººéƒ½æ˜¯æ‡’æƒ°çš„ã€‚** ä»–ä»¬å¯»æ‰¾æœ€æœ‰æ•ˆçš„æ–¹æ³•æ¥è§£å†³å¤æ‚çš„é—®é¢˜ï¼Œæœ€å°åŒ–åŠªåŠ›åŒæ—¶æœ€å¤§åŒ–ç»“æœã€‚

åœ¨ç”ŸæˆAIåº”ç”¨ä¸­ï¼Œè¿™ç§æ•ˆç‡æ˜¯é€šè¿‡åˆ†å—å®ç°çš„ã€‚å°±åƒå°†ä¸€æœ¬ä¹¦åˆ†æˆç« èŠ‚ä½¿å…¶æ›´æ˜“äºé˜…è¯»ä¸€æ ·ï¼Œåˆ†å—å°†é‡è¦æ–‡æœ¬åˆ†æˆæ›´å°ã€æ›´æ˜“äºå¤„ç†å’Œç†è§£çš„éƒ¨åˆ†ã€‚

åœ¨æ¢è®¨åˆ†å—çš„æœºåˆ¶ä¹‹å‰ï¼Œäº†è§£è¿™ä¸€æŠ€æœ¯æ‰€è¿ä½œçš„æ›´å¹¿æ³›æ¡†æ¶æ˜¯è‡³å…³é‡è¦çš„ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼ŒRAGï¼‰ã€‚

## ä»€ä¹ˆæ˜¯ RAGï¼Ÿ



æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§å°†æ£€ç´¢æœºåˆ¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLM æ¨¡å‹ï¼‰ç›¸ç»“åˆçš„æ–¹æ³•ã€‚å®ƒåˆ©ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å¢å¼º AI èƒ½åŠ›ï¼Œä»¥ç”Ÿæˆæ›´å‡†ç¡®å’Œå…·æœ‰ä¸Šä¸‹æ–‡ä¸°å¯Œæ€§çš„å“åº”ã€‚

## å¼•å…¥åˆ†å—å¤„ç†

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vRLUR6BbhF72bUpq69N6RQ.png)

åˆ†å—å¤„ç†æ˜¯å°†å¤§æ®µæ–‡æœ¬æ‹†åˆ†æˆæ›´å°ã€æ›´æ˜“ç®¡ç†çš„éƒ¨åˆ†ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

* æ•°æ®å‡†å¤‡ï¼šå¯é çš„æ•°æ®æºè¢«åˆ†å‰²æˆå—æ–‡æ¡£å¹¶å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ã€‚å¦‚æœåœ¨å—ä¸­ç”ŸæˆåµŒå…¥ï¼Œæ•°æ®åº“å¯ä»¥æ˜¯ä¸€ä¸ªå‘é‡å­˜å‚¨ã€‚
* æ£€ç´¢ï¼šå½“ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œç³»ç»Ÿé€šè¿‡å‘é‡æœç´¢ã€å…¨æ–‡æœç´¢æˆ–ä¸¤è€…çš„ç»„åˆåœ¨æ–‡æ¡£å—ä¸­è¿›è¡Œæœç´¢ã€‚è¿™ä¸ªè¿‡ç¨‹è¯†åˆ«å¹¶æ£€ç´¢ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„å—ã€‚

## ä¸ºä»€ä¹ˆ Chunking åœ¨ RAG æ¶æ„ä¸­è‡³å…³é‡è¦

Chunking åœ¨ RAG æ¶æ„ä¸­æ˜¯ç»å¯¹å¿…è¦çš„ï¼Œå› ä¸ºå®ƒæ˜¯å†³å®šæ‚¨çš„ Gen AI åº”ç”¨ç¨‹åºå‡†ç¡®æ€§çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚

1. **å—åº”è¯¥å°ä»¥æé«˜å‡†ç¡®æ€§ï¼š** Chunking ä½¿ç³»ç»Ÿèƒ½å¤Ÿå¯¹è¾ƒå°çš„æ–‡æœ¬ç‰‡æ®µè¿›è¡Œç´¢å¼•å’Œæœç´¢ï¼Œä»è€Œæé«˜æ‰¾åˆ°ç›¸å…³æ–‡æ¡£çš„å‡†ç¡®æ€§ã€‚å½“å‘å‡ºæŸ¥è¯¢æ—¶ï¼Œç³»ç»Ÿå¯ä»¥å¿«é€Ÿå®šä½æœ€ç›¸å…³çš„å—ï¼Œä»è€Œæé«˜æ£€ç´¢è¿‡ç¨‹çš„ç²¾ç¡®åº¦ã€‚
2. **å—åº”è¯¥å¤§ä»¥å¢å¼ºä¸Šä¸‹æ–‡ç”Ÿæˆï¼š** ä¸æ˜¯æ‰€æœ‰å—éƒ½åº”è¯¥å°ã€‚é€šè¿‡å¤„ç†è¾ƒå°çš„å—ï¼Œç”Ÿæˆæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨æ¯ä¸ªç‰‡æ®µæä¾›çš„ä¸Šä¸‹æ–‡ã€‚è¿™ä¼šå¯¼è‡´æ›´è¿è´¯å’Œä¸Šä¸‹æ–‡å‡†ç¡®çš„å“åº”ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥åˆ©ç”¨ç‰¹å®šçš„ã€ç›¸å…³çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åœ¨ä¸€ä¸ªå¤§å‹ã€æœªåˆ†å‰²çš„æ–‡æ¡£ä¸­ç­›é€‰ã€‚
3. **å¯æ‰©å±•æ€§å’Œæ€§èƒ½ï¼š** Chunking å…è®¸å¯¹å¤§æ•°æ®é›†è¿›è¡Œæ›´å¯æ‰©å±•å’Œé«˜æ•ˆçš„å¤„ç†ã€‚å®ƒé€šè¿‡å°†æ•°æ®åˆ†è§£ä¸ºå¯ç®¡ç†çš„éƒ¨åˆ†æ¥å‡å°‘è®¡ç®—è´Ÿæ‹…ï¼Œè¿™äº›éƒ¨åˆ†å¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œä»è€Œæé«˜ RAG ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚ç„¶è€Œï¼Œåº”ç¡®ä¿å¯æ‰©å±•æ€§

Chunking æ˜¯ä¸€ç§æŠ€æœ¯å¿…è¦æ€§å’Œæˆ˜ç•¥æ–¹æ³•ï¼Œç¡®ä¿å¼ºå¤§ã€é«˜æ•ˆå’Œå¯æ‰©å±•çš„ RAG ç³»ç»Ÿã€‚å®ƒå¢å¼ºäº†æ£€ç´¢å‡†ç¡®æ€§ã€å¤„ç†æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ï¼Œåœ¨ RAG åº”ç”¨çš„æˆåŠŸä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚

## æ”¹è¿›åˆ†å—çš„æŠ€æœ¯

å‡ ç§æŠ€æœ¯å¯ä»¥æ”¹è¿›åˆ†å—ï¼Œä»åŸºæœ¬æ–¹æ³•åˆ°é«˜çº§æ–¹æ³•ä¸ç­‰ï¼š

* **å›ºå®šå­—ç¬¦å¤§å°ï¼š** ç®€å•æ˜äº†ï¼Œå°†æ–‡æœ¬åˆ†æˆå›ºå®šå­—ç¬¦æ•°çš„å—ã€‚
* **é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²ï¼š** ä½¿ç”¨ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·ç­‰åˆ†éš”ç¬¦æ¥åˆ›å»ºæ›´å…·ä¸Šä¸‹æ–‡æ„ä¹‰çš„å—ã€‚
* **æ–‡æ¡£ç‰¹å®šåˆ†å‰²ï¼š** æ ¹æ®æ–‡æ¡£ç±»å‹ï¼ˆå¦‚PDFæˆ–Markdownæ–‡ä»¶ï¼‰å®šåˆ¶åˆ†å—æ–¹æ³•ã€‚
* **è¯­ä¹‰åˆ†å‰²ï¼š** ä½¿ç”¨åµŒå…¥æ ¹æ®è¯­ä¹‰å†…å®¹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—ã€‚
* **ä»£ç†åˆ†å‰²ï¼š** åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ ¹æ®å†…å®¹å’Œä¸Šä¸‹æ–‡ç¡®å®šæœ€ä½³åˆ†å—æ–¹å¼ã€‚

é€šè¿‡é‡‡ç”¨è¿™äº›æŠ€æœ¯ï¼ŒRAGç³»ç»Ÿå¯ä»¥å®ç°æ›´é«˜çš„æ€§èƒ½å’Œæ›´å‡†ç¡®çš„ç»“æœï¼Œå·©å›ºå…¶ä½œä¸ºAIä¸­é‡è¦å·¥å…·çš„è§’è‰²ã€‚

# å›ºå®šå­—ç¬¦å¤§å°

å›ºå®šå­—ç¬¦å¤§å°åˆ†å—æ˜¯æ‹†åˆ†æ–‡æœ¬çš„æœ€åŸºæœ¬æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•æ¶‰åŠå°†æ–‡æœ¬åˆ†æˆé¢„å®šæ•°é‡çš„å­—ç¬¦å—ï¼Œè€Œä¸è€ƒè™‘å†…å®¹ã€‚è¿™ç§æ–¹æ³•ç®€å•æ˜äº†ï¼Œä½†ç¼ºä¹å¯¹æ–‡æœ¬ç»“æ„å’Œä¸Šä¸‹æ–‡çš„è€ƒè™‘ï¼Œè¿™å¯èƒ½å¯¼è‡´å—çš„æ„ä¹‰è¾ƒä½ã€‚

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rIKoGYCIUrXcjP1d0yX7yQ.png)

## ä¼˜ç‚¹ï¼š

* **ç®€å•æ€§ï¼š** å®ç°ç®€å•ï¼Œæ‰€éœ€çš„è®¡ç®—èµ„æºæå°‘ã€‚
* **ä¸€è‡´æ€§ï¼š** ç”Ÿæˆç»Ÿä¸€çš„å—ï¼Œç®€åŒ–åç»­å¤„ç†ã€‚

## ç¼ºç‚¹ï¼š

* **ä¸Šä¸‹æ–‡å¿½è§†ï¼š** å¿½ç•¥æ–‡æœ¬çš„ç»“æ„å’Œå«ä¹‰ï¼Œå¯¼è‡´ä¿¡æ¯ç¢ç‰‡åŒ–ã€‚
* **ä½æ•ˆç‡ï¼š** å¯èƒ½ä¼šåˆ‡æ–­é‡è¦çš„ä¸Šä¸‹æ–‡ï¼Œéœ€è¦é¢å¤–å¤„ç†ä»¥é‡æ–°ç»„åˆæœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚

è¿™é‡Œæ˜¯å¦‚ä½•ä½¿ç”¨ä¹‹å‰æä¾›çš„ä»£ç å®ç°å›ºå®šå­—ç¬¦å¤§å°åˆ†å—çš„ç¤ºä¾‹ï¼š


```python
# Sample text to chunk
text = "This is the text I would like to chunk up. It is the example text for this exercise."

# Set the chunk size
chunk_size = 35
# Initialize a list to hold the chunks
chunks = []
# Iterate over the text to create chunks
for i in range(0, len(text), chunk_size):
    chunk = text[i:i + chunk_size]
    chunks.append(chunk)
# Display the chunks
print(chunks)
# Output: ['This is the text I would like to ch', 'unk up. It is the example text for ', 'this exercise']
```
ä½¿ç”¨ LangChain çš„ `CharacterTextSplitter` å®ç°ç›¸åŒçš„ç»“æœï¼š


```python
from langchain.text_splitter import CharacterTextSplitter

# Initialize the text splitter with specified chunk size
text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0, separator='', strip_whitespace=False)
# Create documents using the text splitter
documents = text_splitter.create_documents([text])
# Display the created documents
for doc in documents:
    print(doc.page_content)
# Output: 
# This is the text I would like to ch
# unk up. It is the example text for 
# this exercise
```
å›ºå®šå­—ç¬¦å¤§å°åˆ†å—æ˜¯ä¸€ç§ç®€å•è€ŒåŸºç¡€çš„æŠ€æœ¯ï¼Œé€šå¸¸ä½œä¸ºåœ¨æ›´å¤æ‚æ–¹æ³•ä¹‹å‰çš„åŸºçº¿ã€‚

# é€’å½’å­—ç¬¦æ–‡æœ¬æ‹†åˆ†

é€’å½’å­—ç¬¦æ–‡æœ¬æ‹†åˆ†æ˜¯ä¸€ç§æ›´é«˜çº§çš„æŠ€æœ¯ï¼Œå®ƒè€ƒè™‘äº†æ–‡æœ¬çš„ç»“æ„ã€‚å®ƒä½¿ç”¨ä¸€ç³»åˆ—åˆ†éš”ç¬¦é€’å½’åœ°å°†æ–‡æœ¬åˆ’åˆ†ä¸ºæ›´æœ‰æ„ä¹‰ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„å—ã€‚

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ylptDkGpU5Hd6w04StQUgw.png)

åœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œå—å¤§å°ä¸º30ä¸ªå­—ç¬¦ï¼Œé‡å ä¸º20ä¸ªå­—ç¬¦ï¼ŒRecursiveCharacterTextSplitterå°†å°è¯•åœ¨ä¿æŒé€»è¾‘è¾¹ç•Œçš„åŒæ—¶æ‹†åˆ†æ–‡æœ¬ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿè¡¨æ˜ç”±äºå—å¤§å°è¾ƒå°ï¼Œå®ƒä»å¯èƒ½åœ¨å•è¯æˆ–å¥å­çš„ä¸­é—´è¿›è¡Œæ‹†åˆ†ï¼Œè¿™å¹¶ä¸æ˜¯æœ€ä¼˜çš„ã€‚

## ä¼˜ç‚¹ï¼š

* **æ”¹å–„ä¸Šä¸‹æ–‡ï¼š** è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨æ®µè½æˆ–å¥å­ç­‰åˆ†éš”ç¬¦ä¿ç•™æ–‡æœ¬çš„è‡ªç„¶ç»“æ„ã€‚
* **çµæ´»æ€§ï¼š** å…è®¸ä¸åŒçš„å—å¤§å°å’Œé‡å ï¼Œä¸ºå—å¤„ç†è¿‡ç¨‹æä¾›æ›´å¥½çš„æ§åˆ¶ã€‚

## ç¼ºç‚¹ï¼š

* **å—å¤§å°å¾ˆé‡è¦ï¼š** å®ƒåº”è¯¥æ˜¯å¯ç®¡ç†çš„ï¼Œä½†ä»ç„¶è‡³å°‘åŒ…å«ä¸€ä¸ªçŸ­è¯­æˆ–æ›´å¤šã€‚å¦åˆ™ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ£€ç´¢å—æ—¶è·å¾—ç²¾ç¡®åº¦ã€‚
* **æ€§èƒ½å¼€é”€ï¼š** ç”±äºé€’å½’æ‹†åˆ†å’Œå¤„ç†å¤šä¸ªåˆ†éš”ç¬¦ï¼Œè¦æ±‚æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è€Œä¸”ä¸å›ºå®šå¤§å°çš„å—ç›¸æ¯”ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„å—æ›´å¤šã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ Langchain ä¸­å®ç°é€’å½’å­—ç¬¦æ–‡æœ¬æ‹†åˆ†çš„ç¤ºä¾‹ï¼š

```python
%pip install -qU langchain-text-splitters
```
é¦–å…ˆå®‰è£… long-chain-text-splitters åº“ï¼Œå¦‚æœæ‚¨è¿˜æ²¡æœ‰è¿™æ ·åšçš„è¯ã€‚

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
# Sample text to chunk
text = """
The Olympic Games, originally held in ancient Greece, were revived in 1896 and
have since become the worldâ€™s foremost sports competition, bringing together 
athletes from around the globe.
"""
# Initialize the recursive character text splitter with specified chunk size
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=30,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)

# Create documents using the text splitter
documents = text_splitter.create_documents([text])
# Display the created documents
for doc in documents:
    print(doc.page_content)
# Output:
# â€œThe Olympic Games, originallyâ€
# â€œheld in ancient Greece, wereâ€
# â€œrevived in 1896 and haveâ€
# â€œhave since become the worldâ€™sâ€
# â€œworldâ€™s foremost sportsâ€
# â€œcompetition, bringing togetherâ€
# â€œtogether athletes from aroundâ€
# â€œaround the globe.â€
```
åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œæ–‡æœ¬é¦–å…ˆæŒ‰è¾ƒå¤§çš„ç»“æ„ï¼ˆå¦‚æ®µè½ï¼‰è¿›è¡Œæ‹†åˆ†ï¼Œå¦‚æœå—ä»ç„¶å¤ªå¤§ï¼Œåˆ™ä½¿ç”¨è¾ƒå°çš„ç»“æ„ï¼ˆå¦‚å¥å­ï¼‰è¿›ä¸€æ­¥æ‹†åˆ†ã€‚æ¯ä¸ªå—ä¿æŒæœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…æˆªæ–­é‡è¦ä¿¡æ¯ã€‚

é€’å½’å­—ç¬¦æ–‡æœ¬æ‹†åˆ†åœ¨ç®€å•æ€§å’Œå¤æ‚æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œæä¾›äº†ä¸€ç§å¼ºå¤§çš„å—å¤„ç†æ–¹æ³•ï¼Œå°Šé‡æ–‡æœ¬çš„å›ºæœ‰ç»“æ„ã€‚

# æ–‡æ¡£ç‰¹å®šæ‹†åˆ†

æ–‡æ¡£ç‰¹å®šæ‹†åˆ†æ ¹æ®ä¸åŒçš„æ–‡æ¡£ç±»å‹å®šåˆ¶åˆ†å—è¿‡ç¨‹ï¼Œä¾‹å¦‚ Markdown æ–‡ä»¶ã€Python è„šæœ¬ã€JSON æ–‡æ¡£æˆ– HTMLï¼Œç¡®ä¿æ¯ç§ç±»å‹ä»¥æœ€é€‚åˆå…¶å†…å®¹å’Œç»“æ„çš„æ–¹å¼è¿›è¡Œæ‹†åˆ†ã€‚

ä¾‹å¦‚ï¼ŒMarkdown åœ¨ GitHubã€Medium å’Œ Confluence ç­‰å¹³å°ä¸Šè¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½¿å…¶æˆä¸º RAG ç³»ç»Ÿä¸­æ‘„å–çš„è‡ªç„¶é€‰æ‹©ï¼Œåœ¨è¿™äº›ç³»ç»Ÿä¸­ï¼Œå¹²å‡€ã€ç»“æ„åŒ–çš„æ•°æ®å¯¹äºç”Ÿæˆå‡†ç¡®çš„å“åº”è‡³å…³é‡è¦ã€‚

æ­¤å¤–ï¼Œè¿˜ä¸ºå„ç§ç¼–ç¨‹è¯­è¨€æä¾›äº†ç‰¹å®šè¯­è¨€çš„æ‹†åˆ†å™¨ï¼ŒåŒ…æ‹¬ C++ã€Goã€Javaã€Python ç­‰ï¼Œç¡®ä¿ä»£ç èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œåˆ†æå’Œæ£€ç´¢ã€‚

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l1xkkSOviCa25x5XYLNN3A.png)

## ä¼˜ç‚¹ï¼š

* **ç›¸å…³æ€§ï¼š** ä½¿ç”¨æœ€åˆé€‚çš„æ–¹æ³•å¯¹ä¸åŒæ–‡æ¡£ç±»å‹è¿›è¡Œæ‹†åˆ†ï¼Œä¿ç•™å…¶é€»è¾‘ç»“æ„ã€‚
* **ç²¾ç¡®æ€§ï¼š** æ ¹æ®æ¯ç§æ–‡æ¡£ç±»å‹çš„ç‹¬ç‰¹ç‰¹å¾å®šåˆ¶æ‹†åˆ†è¿‡ç¨‹ã€‚

## ç¼ºç‚¹ï¼š

* **å¤æ‚çš„å®ç°ï¼š** éœ€è¦é’ˆå¯¹ä¸åŒæ–‡æ¡£ç±»å‹é‡‡ç”¨ä¸åŒçš„åˆ†å—ç­–ç•¥å’Œåº“ã€‚
* **ç»´æŠ¤ï¼š** ç”±äºæ–¹æ³•çš„å¤šæ ·æ€§ï¼Œç»´æŠ¤å˜å¾—æ›´åŠ å¤æ‚ã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ªå¦‚ä½•å®ç°é’ˆå¯¹Markdownå’ŒPythonæ–‡ä»¶çš„æ–‡æ¡£ç‰¹å®šæ‹†åˆ†çš„ç¤ºä¾‹ï¼š

## Markdown åˆ†å‰²

```python
from langchain.text_splitter import MarkdownTextSplitter
# ç¤ºä¾‹ Markdown æ–‡æœ¬
markdown_text = """
# åŠ å·çš„ä¹è¶£
## é©¾é©¶
å°è¯•æ²¿ç€ 1 å·å…¬è·¯å¼€è½¦å‰å¾€åœ£åœ°äºšå“¥
### é£Ÿç‰©
ç¡®ä¿åœ¨é‚£é‡Œçš„æ—¶å€™åƒä¸€ä¸ªå·é¥¼
## å¾’æ­¥æ—…è¡Œ
å»ä¼˜èƒœç¾åœ°
"""
# åˆå§‹åŒ– Markdown æ–‡æœ¬åˆ†å‰²å™¨
splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)
# ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨åˆ›å»ºæ–‡æ¡£
documents = splitter.create_documents([markdown_text])
# æ˜¾ç¤ºåˆ›å»ºçš„æ–‡æ¡£
for doc in documents:
    print(doc.page_content)
# è¾“å‡º:
# # åŠ å·çš„ä¹è¶£\n\n## é©¾é©¶
# å°è¯•æ²¿ç€ 1 å·å…¬è·¯å¼€è½¦å‰å¾€åœ£åœ°äºšå“¥
# ### é£Ÿç‰©
# ç¡®ä¿åœ¨é‚£é‡Œçš„æ—¶å€™åƒä¸€ä¸ªå·é¥¼
# ## å¾’æ­¥æ—…è¡Œ\n\nå»ä¼˜èƒœç¾åœ°
```

## Python ä»£ç æ‹†åˆ†


```python
from langchain.text_splitter import PythonCodeTextSplitter
# Sample Python code
python_text = """
class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age
p1 = Person("John", 36)
for i in range(10):
    print(i)
"""
# Initialize the Python code text splitter
python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)
# Create documents using the text splitter
documents = python_splitter.create_documents([python_text])
# Display the created documents
for doc in documents:
    print(doc.page_content)
# Output:
# class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age
# p1 = Person("John", 36)\n\nfor i in range(10):\n    print(i)
```
ç‰¹å®šæ–‡æ¡£çš„æ‹†åˆ†ä¿ç•™äº†æ–‡æ¡£çš„é€»è¾‘ç»“æ„ï¼Œä½¿å¾—å—æ›´åŠ æœ‰æ„ä¹‰ä¸”ä¸Šä¸‹æ–‡å‡†ç¡®ã€‚ä¾‹å¦‚ï¼Œåœ¨Markdownæ–‡ä»¶ä¸­ï¼Œæ ‡é¢˜å’Œéƒ¨åˆ†æ˜¯åˆ†å¼€çš„ï¼Œè€Œåœ¨Pythonä»£ç ä¸­ä½¿ç”¨ç±»å’Œå‡½æ•°ã€‚

è¿™ç§æ–¹æ³•é€šè¿‡ä¿æŒä¸åŒæ–‡æ¡£ç±»å‹çš„å®Œæ•´æ€§ï¼Œå¢å¼ºäº†ç³»ç»Ÿæ£€ç´¢å’Œç”Ÿæˆç›¸å…³å“åº”çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†RAGç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚

# è¯­ä¹‰æ‹†åˆ†

ä¸ä»¥å‰çš„ä»»æ„é•¿åº¦æˆ–è¯­æ³•è§„åˆ™çš„æ‹†åˆ†æ–¹æ³•ä¸åŒï¼Œè¯­ä¹‰æ‹†åˆ†é€šè¿‡ä½¿ç”¨æ–‡æœ¬çš„å«ä¹‰æ¥ç¡®å®šå—è¾¹ç•Œï¼Œå°†åˆ†å—æå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚

è¯¥æ–¹æ³•åˆ©ç”¨åµŒå…¥å°†è¯­ä¹‰ç›¸ä¼¼çš„å†…å®¹åˆ†ç»„ï¼Œç¡®ä¿æ¯ä¸ªå—åŒ…å«ä¸Šä¸‹æ–‡ä¸€è‡´çš„ä¿¡æ¯ã€‚

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5GFiUJQBgbFubamUc5ea8A.png)

ä¸Šé¢çš„å›¾ç¤ºè¯´æ˜äº†è¯­ä¹‰åˆ†å—çš„å·¥ä½œæµç¨‹ï¼Œä»å¥å­æ‹†åˆ†å¼€å§‹ï¼Œç„¶åç”ŸæˆåµŒå…¥ï¼Œæœ€åæ ¹æ®ç›¸ä¼¼æ€§å¯¹å¥å­è¿›è¡Œåˆ†ç»„ã€‚è¯¥è¿‡ç¨‹ç¡®ä¿å—åœ¨è¯­ä¹‰ä¸Šæ˜¯ä¸€è‡´çš„ï¼Œä»è€Œå¢å¼ºä¿¡æ¯æ£€ç´¢çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ã€‚

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥çœ‹ä¸€ä¸‹è¯¥æ–¹æ³•çš„è¾“å‡ºã€‚

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8PbJZeHxOEzpYzFyt_dFEw.png)

è¯¥å›¾æä¾›äº†ä¸€ä¸ªå®é™…ç¤ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å°†å¥å­åˆ†ç»„ä¸ºå—ã€‚ä¸»é¢˜ç›¸å…³çš„å¥å­è¢«åˆ†ç»„ï¼Œè€Œæ„ä¹‰ä¸åŒçš„å¥å­åˆ™ä¿æŒåˆ†å¼€ã€‚è§†è§‰è§£é‡Šæ¸…æ¥šåœ°è¯´æ˜äº†å¦‚ä½•åº”ç”¨è¯­ä¹‰åˆ†å—ä»¥ä¿æŒæ–‡æœ¬ä¸­çš„ä¸Šä¸‹æ–‡å’Œä¸€è‡´æ€§ã€‚

## ä¼˜ç‚¹ï¼š

* **ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼š** ç¡®ä¿å†…å®¹å—åŒ…å«è¯­ä¹‰ç›¸ä¼¼çš„å†…å®¹ï¼Œæé«˜ä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚
* **åŠ¨æ€é€‚åº”æ€§ï¼š** å¯ä»¥æ ¹æ®æ„ä¹‰è€Œéä¸¥æ ¼è§„åˆ™é€‚åº”å„ç§æ–‡æœ¬ç»“æ„å’Œå†…å®¹ç±»å‹ã€‚

## ç¼ºç‚¹ï¼š

* **è®¡ç®—å¼€é”€ï¼š** éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºæ¥ç”Ÿæˆå’Œæ¯”è¾ƒåµŒå…¥ã€‚
* **å¤æ‚æ€§ï¼š** ä¸æ›´ç®€å•çš„æ‹†åˆ†æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°èµ·æ¥æ›´å¤æ‚ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨åµŒå…¥å®ç°è¯­ä¹‰æ‹†åˆ†çš„ç¤ºä¾‹ã€‚æ­¤ä»£ç æ¥è‡ª Greg Kamradt çš„ç¬”è®°æœ¬ï¼š[5\_Levels\_Of\_Text\_Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)ã€‚

```python
from sklearn.metrics.pairwise import cosine_similarity
from langchain.embeddings import OpenAIEmbeddings
import re
# Sample text
text = """
One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.
Teachers and coaches implicitly told us the returns were linear. "You get out," I heard a thousand times, "what you put in." They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.
It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer.
"""
# Splitting the text into sentences
sentences = re.split(r'(?<=[.?!])\s+', text)
sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(sentences)]
# Combine sentences for context
def combine_sentences(sentences, buffer_size=1):
    for i in range(len(sentences)):
        combined_sentence = ''
        for j in range(i - buffer_size, i):
            if j >= 0:
                combined_sentence += sentences[j]['sentence'] + ' '
        combined_sentence += sentences[i]['sentence']
        for j in range(i + 1, i + 1 + buffer_size):
            if j < len(sentences):
                combined_sentence += ' ' + sentences[j]['sentence']
        sentences[i]['combined_sentence'] = combined_sentence
    return sentences
sentences = combine_sentences(sentences)
# Generate embeddings
oai_embeds = OpenAIEmbeddings()
embeddings = oai_embeds.embed_documents([x['combined_sentence'] for x in sentences])
# Add embeddings to sentences
for i, sentence in enumerate(sentences):
    sentence['combined_sentence_embedding'] = embeddings[i]
# Calculate cosine distances
def calculate_cosine_distances(sentences):
    distances = []
    for i in range(len(sentences) - 1):
        embedding_current = sentences[i]['combined_sentence_embedding']
        embedding_next = sentences[i + 1]['combined_sentence_embedding']
        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
        distance = 1 - similarity
        distances.append(distance)
        sentences[i]['distance_to_next'] = distance
    return distances, sentences
distances, sentences = calculate_cosine_distances(sentences)
# Determine breakpoints and create chunks
import numpy as np
breakpoint_distance_threshold = np.percentile(distances, 95)
indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]
# Combine sentences into chunks
chunks = []
start_index = 0
for index in indices_above_thresh:
    end_index = index
    group = sentences[start_index:end_index + 1]
    combined_text = ' '.join([d['sentence'] for d in group])
    chunks.append(combined_text)
    start_index = index + 1
if start_index < len(sentences):
    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
    chunks.append(combined_text)
# Display the created chunks
for i, chunk in enumerate(chunks):
    print(f"Chunk #{i+1}:\n{chunk}\n")
```
è¯­ä¹‰æ‹†åˆ†ä½¿ç”¨åµŒå…¥åˆ›å»ºè¯­ä¹‰ç›¸ä¼¼çš„å—ï¼Œæé«˜äº† RAG ç³»ç»Ÿä¸­çš„æ£€ç´¢å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡ç”Ÿæˆã€‚ä¸“æ³¨äºæ–‡æœ¬çš„å«ä¹‰ç¡®ä¿æ¯ä¸ªå—åŒ…å«è¿è´¯ä¸”ç›¸å…³çš„ä¿¡æ¯ï¼Œä»è€Œå¢å¼º RAG åº”ç”¨çš„æ€§èƒ½å’Œå¯é æ€§ã€‚

# ä»£ç†åˆ†å‰²

ä»£ç†åˆ†å‰²åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ ¹æ®æ–‡æœ¬çš„è¯­ä¹‰ç†è§£åŠ¨æ€åˆ›å»ºå—ã€‚

è¿™ç§å…ˆè¿›çš„æ–¹æ³•é€šè¿‡è¯„ä¼°å†…å®¹å’Œä¸Šä¸‹æ–‡æ¥ç¡®å®šæœ€ä½³å—è¾¹ç•Œï¼Œæ¨¡ä»¿äººç±»çš„åˆ†å—æ–¹å¼ã€‚

ä»£ç†åˆ†å‰²å™¨å¹¶ä¸ä¾èµ–äºé¢„å®šä¹‰çš„è§„åˆ™æˆ–çº¯ç²¹çš„ç»Ÿè®¡æ–¹æ³•ï¼Œè€Œæ˜¯é€šè¿‡åŠ¨æ€è¯„ä¼°å†…å®¹æ¥å¤„ç†æ–‡æœ¬ï¼Œç±»ä¼¼äºä¸€ä¸ªäººé˜…è¯»æ–‡æ¡£å¹¶æ ¹æ®æ€æƒ³çš„æµåŠ¨å’Œå¥å­çš„ä¸Šä¸‹æ–‡å†³å®šåœ¨å“ªé‡Œåˆ†å‰²ã€‚è¿™ç§æ–¹æ³•å¢å¼ºäº†ç»“æœå—çš„è¿è´¯æ€§å’Œç›¸å…³æ€§ã€‚

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t2uIAauvKb8nSQoqeCSp-A.png)

## ä¼˜ç‚¹ï¼š

* **é«˜ç²¾åº¦ï¼š** é€šè¿‡ä½¿ç”¨å¤æ‚çš„è¯­è¨€æ¨¡å‹ï¼Œæä¾›é«˜åº¦ç›¸å…³å’Œä¸Šä¸‹æ–‡å‡†ç¡®çš„ç‰‡æ®µã€‚
* **é€‚åº”æ€§ï¼š** èƒ½å¤Ÿå¤„ç†å¤šç§ç±»å‹çš„æ–‡æœ¬ï¼Œå¹¶åŠ¨æ€è°ƒæ•´åˆ†å—ç­–ç•¥ã€‚

## ç¼ºç‚¹ï¼š

* **èµ„æºå¯†é›†å’Œé¢å¤–çš„ LLM æˆæœ¬ï¼š** è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚
* **å¤æ‚çš„å®æ–½ï¼š** æ¶‰åŠè®¾ç½®å’Œå¾®è°ƒè¯­è¨€æ¨¡å‹ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

## å¦‚ä½•åœ¨ LangGraph ä¸­å®ç°ä»£ç†åˆ†å‰²å™¨

**äº†è§£ LangGraph ä¸­çš„èŠ‚ç‚¹ï¼š** LangGraph ä¸­çš„èŠ‚ç‚¹ä»£è¡¨å·¥ä½œæµç¨‹ä¸­çš„æ“ä½œæˆ–æ­¥éª¤ã€‚æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶è¾“å…¥ï¼Œå¤„ç†å®ƒï¼Œå¹¶ç”Ÿæˆä¼ é€’ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºã€‚

```python
from langgraph.nodes import InputNode, SentenceSplitterNode, LLMDecisionNode, ChunkingNode

# Step 1: Input Node
input_node = InputNode(name="Document Input")

# Step 2: Sentence Splitting Node
splitter_node = SentenceSplitterNode(input=input_node.output, name="Sentence Splitter")

# Step 3: LLM Decision Node
decision_node = LLMDecisionNode(
    input=splitter_node.output, 
    prompt_template="Does the sentence '{next_sentence}' belong to the same chunk as '{current_chunk}'?", 
    name="LLM Decision"
)

# Step 4: Chunking Node
chunking_node = ChunkingNode(input=decision_node.output, name="Semantic Chunking")

# Run the graph
document = "Your document text here..."
result = chunking_node.run(document=document)
print(result)
```

# ç»“è®º

æ€»ä¹‹ï¼Œåˆ†å—æ˜¯ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å…³é”®ç­–ç•¥ï¼Œå¯ä»¥å®ç°æ›´å‡†ç¡®ã€ä¸Šä¸‹æ–‡ç›¸å…³å’Œå¯æ‰©å±•çš„å“åº”ã€‚

é€šè¿‡å°†å¤§å‹æ–‡æœ¬æ‹†åˆ†ä¸ºå¯ç®¡ç†çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬æé«˜äº†æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œå¹¶æ”¹å–„äº†äººå·¥æ™ºèƒ½åº”ç”¨çš„æ•´ä½“æ•ˆç‡ã€‚

é‡‡ç”¨å…ˆè¿›çš„åˆ†å—æŠ€æœ¯å¯¹äººå·¥æ™ºèƒ½é©±åŠ¨è§£å†³æ–¹æ¡ˆçš„æŒç»­æˆåŠŸå’Œè¿›æ­¥è‡³å…³é‡è¦ã€‚

# è¿›ä¸€æ­¥é˜…è¯»ï¼š

1. **è‡´è°¢ï¼š** æœ¬æ–‡å—åˆ°Greg Kamradtçš„æ·±åˆ»ç¬”è®°æœ¬å’ŒYouTubeè§†é¢‘çš„å¯å‘ï¼Œæ¢è®¨äº†äº”ä¸ªåˆ†å—çº§åˆ«ä»¥æå‡AIæ€§èƒ½ã€‚

2. ç”±æ°å‡ºçš„Apoorva Joshiæ’°å†™çš„ä¸€ç¯‡ç²¾å½©æ–‡ç« â€”â€”è¿™å¯¹äºä»»ä½•æœ‰å…´è¶£ä¼˜åŒ–RAGç³»ç»Ÿä¸­AIæ€§èƒ½çš„äººæ¥è¯´éƒ½æ˜¯å¿…è¯»ä¹‹ä½œã€‚

# åœ¨ä½ ç¦»å¼€ä¹‹å‰ï¼ ğŸ¦¸ğŸ»â€â™€ï¸

1. å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ä»·å€¼å¹¶å¸Œæœ›è¡¨è¾¾ä½ çš„æ”¯æŒï¼Œ**è¯·ç»™è¿™æ¡LinkedInåŠ¨æ€ç‚¹ä¸ªèµã€‚ä½ ä¹Ÿå¯ä»¥åœ¨[LinkedInåŠ¨æ€](https://www.linkedin.com/posts/hanheloiryan_the-future-of-generative-ai-is-agentic-what-activity-7191296371537108992-p1rg?utm_source=share&utm_medium=member_desktop)ä¸­æ‰¾åˆ°å…è´¹çš„æœ‹å‹é“¾æ¥ã€‚** ä½ çš„å‚ä¸å°†æœ‰åŠ©äºæ‰©å¤§è¿™ç¯‡æ–‡ç« çš„ä¼ æ’­èŒƒå›´ï¼Œä½ çš„æ”¯æŒæ˜¯æˆ‘å¾ˆå¤§çš„åŠ¨åŠ›ã€‚ âœğŸ»ğŸ¦¾â¤ï¸
2. ç»™æˆ‘çš„æ–‡ç« ç‚¹èµ50æ¬¡ï¼Œè¿™çœŸçš„ä¼šå¯¹æˆ‘å¸®åŠ©å¾ˆå¤§ï¼Œå¹¶å°†è¿™ç¯‡æ–‡ç« æ¨é€ç»™æ›´å¤šäººã€‚ğŸ‘
3. åœ¨[Medium](https://medium.com/@han.heloir) ã€[LinkedIn](https://www.linkedin.com/in/hanheloiryan/)ä¸Šå…³æ³¨æˆ‘ï¼Œå¹¶[è®¢é˜…](https://medium.com/@han.heloir/about)ä»¥è·å–æˆ‘æœ€æ–°çš„æ–‡ç« ğŸ«¶

# å¦‚æœæ‚¨å¯¹è¯¥ä¸»é¢˜æ„Ÿå…´è¶£ï¼Œæ‚¨è¿˜å¯ä»¥é˜…è¯»ä»¥ä¸‹æ›´å¤šæ–‡ç« ï¼š
