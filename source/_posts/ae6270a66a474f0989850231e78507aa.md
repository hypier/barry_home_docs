
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*X41aBnBo4CMO6aAGqK7Obg.png
date: '2024-01-25 18:16:57'
tags:
  - RAG技术
  - 大型语言模型
  - 信息检索
title: 高级RAG 01朴素RAG的问题

---


[检索增强生成（RAG）](https://readmedium.com/a-brief-introduction-to-retrieval-augmented-generation-rag-b7eb70982891)是通过整合外部知识源的额外信息来改进大型语言模型（LLMs）的过程。**这使得LLMs能够生成更精确和上下文感知的响应，同时减少幻觉现象**。

自2023年以来，RAG已成为基于LLM的系统中最受欢迎的架构。许多产品在功能上高度依赖RAG。**因此，优化RAG的性能，使检索过程更快、结果更准确，已成为一个关键问题**。

**本系列文章将重点介绍高级RAG技术，以提升RAG生成的质量。**

# 朴素RAG回顾

朴素RAG的典型工作流程如图1所示。



如图1所示，RAG主要包含以下步骤：

1. **索引构建**：索引过程是离线执行的关键初始步骤。它首先对原始数据进行清洗和提取，将PDF、HTML和Word等多种文件格式转换为标准化的纯文本。为了适应语言模型的上下文限制，这些文本被分割成更小、更易管理的数据块，这一过程称为分块。随后，这些数据块通过嵌入模型转换为向量表示。最后，创建一个索引，将这些文本块及其向量嵌入存储为键值对，从而实现高效且可扩展的搜索能力。
2. **检索**：用户查询用于从外部知识源检索相关上下文。为此，用户查询通过编码模型处理，生成语义相关的嵌入。然后，在向量数据库上进行相似度搜索，检索出最接近的前k个数据对象。
3. **生成**：用户查询和检索到的附加上下文填充到提示模板中。最终，从检索步骤中得到的增强提示输入到大型语言模型（LLM）中。

# 朴素RAG的问题

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MofEwsFkb-4iYKpiX9LCHg.png)

如图2所示，朴素RAG在上述三个步骤中均存在问题（红色虚线框），且有充足的优化空间。

## 索引

* 信息提取不完整，未能有效处理PDF等非结构化文件中的图像和表格内的有用信息。
* 分块过程采用“一刀切”策略，而非根据不同文件类型的特性选择最佳策略，导致每个分块包含的语义信息不完整。此外，未能考虑文本中已有的标题等重要细节。
* 索引结构优化不足，导致检索功能效率低下。
* 嵌入模型的语义表示能力较弱。

## 检索

* 召回上下文的相关性不足，准确率低。
* 低召回率导致无法检索到所有相关段落，从而阻碍了大型语言模型生成全面答案的能力。
* 查询可能不准确或嵌入模型的语义表示能力较弱，导致无法检索到有价值的信息。
* 检索算法受限，因为它没有结合不同类型的检索方法或算法，例如结合关键词、语义和向量检索。
* 当多个检索到的上下文包含相似信息时，会出现信息冗余，导致生成的答案中内容重复。

## 生成

* 有效整合检索到的上下文与当前生成任务可能存在困难，导致输出不一致。
* 过度依赖生成过程中的增强信息存在高风险。这可能导致输出仅重复检索内容，而未提供有价值的信息。
* LLM 可能生成错误、无关、有害或偏见的回应。

**需要注意的是，这些问题的原因可能是多方面的。例如，** 如果最终提供给用户的回应包含无关内容，这可能并非仅由 LLM 问题引起。根本原因可能是从 PDF 中提取文档不准确，或是嵌入模型无法准确捕捉语义等。

# 结论

本文介绍了Naive RAG存在的问题。

**本系列的下一部分将提供缓解这些问题并增强RAG效果的措施或解决方案。**

最后，若本文存在任何错误或遗漏，敬请指正。

# 用简单英语 🚀

*感谢您成为[**用简单英语**](https://plainenglish.io)社区的一员！在您离开之前：*

* 请务必**点赞**并**关注**作者️👏**️️**
* 关注我们：[**X**](https://twitter.com/inPlainEngHQ) **| [LinkedIn](https://www.linkedin.com/company/inplainenglish/) | [YouTube](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) | [Discord](https://discord.gg/in-plain-english-709094664682340443) | [Newsletter](https://newsletter.plainenglish.io/)**
* 访问我们的其他平台：[**Stackademic**](https://stackademic.com/) **| [CoFeed](https://cofeed.app/) | [Venture](https://venturemagazine.net/)**
* 更多内容请访问[**PlainEnglish.io**](https://plainenglish.io)
