
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kLPKdFZyLhrVYBzUtXlw8g.gif
date: '2024-07-27 23:53:27'
tags:
  - 大型语言模型
  - Streamlit应用
  - Ollama库
title: 使用Ollama构建Llama 3.1 8b Streamlit聊天应用程序逐步指南

---


# 介绍

大型语言模型（LLMs）彻底改变了人工智能领域，提供了令人印象深刻的语言理解和生成能力。



本文将指导您构建一个使用本地LLM的Streamlit聊天应用程序，特别是来自Meta的Llama 3.1 8b模型，通过Ollama库进行集成。

# 前提条件

在我们深入代码之前，请确保您已安装以下内容：

* Python
* Streamlit
* Ollama

# 设置 Ollama 和下载 Llama 3.1 8b

首先，您需要安装 Ollama 并下载 Llama 3.1 8b 模型。打开命令行界面并执行以下命令：


```python
# Install Ollama
pip install ollama

# Download Llama 3.1 8b model
ollama run llama3.1:8b
```

# 创建 Modelfile

要创建一个与您的 Streamlit 应用无缝集成的自定义模型，请按照以下步骤操作：

1. 在您的项目目录中，创建一个名为 `Modelfile` 的文件，不带任何扩展名。
2. 在文本编辑器中打开 `Modelfile`，并添加以下内容：

```python
model: llama3.1:8b
```
此文件指示 Ollama 使用 Llama 3.1 8b 模型。

# 代码

## 导入库和设置日志记录


```python
import streamlit as st
from llama_index.core.llms import ChatMessage
import logging
import time
from llama_index.llms.ollama import Ollama

logging.basicConfig(level=logging.INFO)
```
* `streamlit as st`：这导入了 Streamlit，一个用于创建交互式网页应用的库。
* `ChatMessage` 和 `Ollama`：这些是从 `llama_index` 库中导入的，用于处理聊天消息和与 Llama 模型进行交互。
* `logging`：用于记录信息、警告和错误，有助于调试和跟踪应用程序的行为。
* `time`：这个库用于测量生成响应所需的时间。

## 初始化聊天记录


```python
if 'messages' not in st.session_state:
    st.session_state.messages = []
```
* `st.session_state`: 这是一个 Streamlit 特性，允许您在应用的不同运行之间存储变量。在这里，它用于存储聊天记录。
* `if` 语句检查 'messages' 是否已经在 `session_state` 中。如果没有，它将其初始化为空列表。

## 流式聊天响应的函数


```python
def stream_chat(model, messages):
    try:
        llm = Ollama(model=model, request_timeout=120.0)
        resp = llm.stream_chat(messages)
        response = ""
        response_placeholder = st.empty()
        for r in resp:
            response += r.delta
            response_placeholder.write(response)
        logging.info(f"Model: {model}, Messages: {messages}, Response: {response}")
        return response
    except Exception as e:
        logging.error(f"Error during streaming: {str(e)}")
        raise e
```
* `stream_chat`: 此函数处理与 Llama 模型的交互。
* `Ollama(model=model, request_timeout=120.0)`: 使用指定的超时初始化 Llama 模型。
* `llm.stream_chat(messages)`: 从模型流式传输聊天响应。
* `response_placeholder = st.empty()`: 在 Streamlit 应用中创建一个占位符，以动态更新响应。
* `for` 循环将响应的每一部分附加到最终响应字符串，并更新占位符。
* `logging.info` 记录模型、消息和响应。
* `except` 块捕获并记录在流式传输过程中发生的任何错误。

## 主要功能


```python
def main():
    st.title("与LLMs模型聊天")
    logging.info("应用启动")

    model = st.sidebar.selectbox("选择模型", ["mymodel", "llama3.1 8b", "phi3", "mistral"])
    logging.info(f"选择的模型: {model}")

    if prompt := st.chat_input("你的问题"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        logging.info(f"用户输入: {prompt}")

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

        if st.session_state.messages[-1]["role"] != "assistant":
            with st.chat_message("assistant"):
                start_time = time.time()
                logging.info("生成响应")

                with st.spinner("正在写入..."):
                    try:
                        messages = [ChatMessage(role=msg["role"], content=msg["content"]) for msg in st.session_state.messages]
                        response_message = stream_chat(model, messages)
                        duration = time.time() - start_time
                        response_message_with_duration = f"{response_message}\n\n耗时: {duration:.2f} 秒"
                        st.session_state.messages.append({"role": "assistant", "content": response_message_with_duration})
                        st.write(f"耗时: {duration:.2f} 秒")
                        logging.info(f"响应: {response_message}, 耗时: {duration:.2f} s")

                    except Exception as e:
                        st.session_state.messages.append({"role": "assistant", "content": str(e)})
                        st.error("生成响应时发生错误。")
                        logging.error(f"错误: {str(e)}")

if __name__ == "__main__":
    main()
```
* `main`: 这是设置和运行Streamlit应用的主要功能。
* `st.title("与LLMs模型聊天")`: 设置应用的标题。
* `model = st.sidebar.selectbox("选择模型", ["mymodel", "llama3.1 8b", "phi3", "mistral"])`: 在侧边栏创建一个下拉菜单以选择模型。
* `if prompt := st.chat_input("你的问题")`: 获取用户输入并将其添加到聊天记录中。
* `for`循环显示聊天记录中的每条消息。
* `if`语句检查最后一条消息是否不是来自助手。如果为真，则从模型生成响应。
* `with st.spinner("正在写入...")`: 在生成响应时显示一个加载指示器。
* `messages = [ChatMessage(role=msg["role"], content=msg["content"]) for msg in st.session_state.messages]`: 为Llama模型准备消息。
* `response_message = stream_chat(model, messages)`: 调用`stream_chat`函数以获取模型的响应。
* `duration = time.time() - start_time`: 计算生成响应所需的时间。
* `response_message_with_duration = f"{response_message}\n\n耗时: {duration:.2f} 秒"`: 将耗时附加到响应消息中。
* `st.session_state.messages.append({"role": "assistant", "content": response_message_with_duration})`: 将助手的响应添加到聊天记录中。
* `st.write(f"耗时: {duration:.2f} 秒")`: 显示生成响应的耗时。
* `except`块处理生成响应期间的错误并显示错误消息。
