
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5dGKpy8jJmmkB015.png
date: '2024-07-26 11:13:25'
tags:
  - NLP
  - Transformers
  - 深度学习
title: LLM 架构解析NLP 基础第一部分

---


*深入探讨架构及构建利用 NLP 模型的实际应用，从 RNN 到 Transformers。*

# 本系列的帖子

1. ***NLP 基础***
2. ***词嵌入***
3. ***RNN、LSTM 和 GRU***
4. ***序列到序列模型***
5. ***注意力机制***
6. ***变换器***
7. ***大型语言模型 (LLM)***
8. ***微调***
9. ***推理***
10. ***优化***
11. ***部署***


# 1. 什么是自然语言处理（NLP）

自然语言处理（NLP）是构建能够以书面、口头和组织的方式操作人类语言或类似人类语言的数据的机器的学科。



NLP大致分为两个重叠的领域：

**自然语言理解（NLU）**，处理文本背后的含义解释，以及

**自然语言生成（NLG）**，专注于生成模仿人类写作的文本。尽管与语音识别不同——语音识别将口语转换为文本——但NLP通常与其协同工作。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*SAGVeS05oKR94gEV.png)

# 2. NLP的应用

NLP用于各种与语言相关的任务，包括回答问题、以不同方式分类文本以及与用户对话。

以下是NLP可以解决的一些任务：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bk9deDQ9OP4B_HXa.jpg)

**2.1 情感分析**：这涉及确定文本的情感基调。输入通常是一段文本，输出是一个概率分布，指示情感是积极、消极还是中立。技术范围从传统方法如TF-IDF和n-grams到深度学习模型如BERT和LSTM。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4VVD1lJIGK_JFEKD.png)

**2.2 有害内容分类**：一种专门的情感分析形式，有害内容分类识别敌意意图并将其分类为威胁或侮辱等特定类型。这用于在线内容的审核，确保更安全的数字空间。

**2.3 机器翻译**：这自动将文本从一种语言翻译成另一种语言。先进的模型如OpenAI的GPT-4和谷歌的基于Transformer的模型在提高翻译的准确性和上下文意识方面处于领先地位。

**2.4 命名实体识别（NER）**：NER模型从文本中提取和分类实体，如名称、组织和地点。这些模型在新闻摘要和打击虚假信息方面至关重要。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-astRp-MU40c_Uhu.png)

**2.5 垃圾邮件检测**：垃圾邮件检测模型将电子邮件分类为垃圾邮件或非垃圾邮件，帮助像Gmail这样的电子邮件服务过滤掉不需要的消息。这些模型通常依赖于逻辑回归、朴素贝叶斯或深度学习等技术。

**2.6 语法错误纠正**：纠正语法错误的模型在Grammarly等工具中被广泛使用。它们将语法纠正视为一个序列到序列的问题，其中输入是一个不正确的句子，输出是一个纠正后的版本。

**2.7 主题建模**：主题建模识别文档语料库中的抽象主题。像潜在狄利克雷分配（LDA）这样的技术在法律文档分析和内容推荐系统中常被使用。

**2.8 文本生成（NLG）**：NLG模型生成类人文本，适用于从聊天机器人到内容创作的各种应用。现代模型如GPT-4能够生成连贯且上下文适当的文本，涵盖各种体裁。

* **自动补全**：自动补全系统预测序列中的下一个单词，应用于搜索引擎和消息应用等。像GPT-3这样的模型在增强自动补全功能方面特别有效。
* **聊天机器人**：聊天机器人模拟人类对话，通过查询数据库或生成对话。谷歌的LaMDA就是一个能够进行细致且类人对话的聊天机器人示例。

**2.9 信息检索**：这涉及查找与查询相关的文档，对搜索引擎和推荐系统至关重要。谷歌的最新模型使用多模态方法同时处理文本、图像和视频数据。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EPKBAWiOOb-krAEM.png)

**2.10 摘要**是缩短文本以突出最相关信息的任务。摘要分为两种方法类别：

* **提取式摘要**专注于从长文本中提取最重要的句子，并将这些句子组合形成摘要。通常，提取式摘要对输入文本中的每个句子进行评分，然后选择几个句子形成摘要。
* **抽象式摘要**通过意译生成摘要。这类似于撰写摘要，其中包含原始文本中不存在的单词和句子。抽象式摘要通常建模为序列到序列任务，其中输入是长文本，输出是摘要。

**2.11 问答（QA）**处理以自然语言提出的问题的回答。最著名的问答示例之一是[Watson](https://www.ibm.com/ibm/history/ibm100/us/en/icons/watson/)，它在2011年参加了电视游戏节目*Jeopardy*，并以显著优势战胜了人类冠军。一般来说，问答任务有两种类型：

* **多项选择**：多项选择问题由一个问题和一组可能的答案组成。学习任务是选择正确的答案。
* **开放领域**：在开放领域问答中，模型提供自然语言问题的答案，而不提供任何选项，通常通过查询大量文本来实现。

# 3. NLP术语

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xVYHJQn8qzYb23oXRup7BQ.png)

## 3.1 文档

一个 **文档** 是一段单一的文本，可以是从一句话到整本书的任何内容。它是 NLP 模型处理的基本文本单元。文档的性质可以多种多样，例如电子邮件、网页、文章或推文。

**示例**：

* 一篇来自报纸的新闻文章。
* 一条推文：“刚看了一部精彩的电影！”
* 一封电子邮件：“亲爱的约翰，希望这封邮件能让你一切安好…”

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fZPZSp-FtztJoFbq)

## 3.2 语料库 (Corpora)

一个**语料库**（复数：corpora）是一个大型文档集合。它作为自然语言处理模型训练和评估的数据集。语料库通常包含按主题、语言或体裁相关的文档，用于分析语言模式和构建统计模型。

**示例**：

* 一年内某特定报纸的所有文章集合。
* 来自电子商务网站的客户评论数据集。
* 古腾堡语料库：来自古腾堡计划的文学文本集合。

## 3.3 特征

**特征**是文本的可测量属性或特征，用于机器学习模型。特征从文档中提取，可以表示文本的各个方面，例如特定单词的出现、句子的长度或特定模式的发生。

**示例**：

**词袋模型 (BoW)**：词汇表中的每个单词都是一个特征，值是该单词在文档中的计数。

* 文档：“我爱NLP。”
* 特征：{“我”: 1, “爱”: 1, “NLP”: 1}

**词频-逆文档频率 (TF-IDF)**：一种统计度量，用于评估一个单词在文档中相对于语料库的重要性。

* 文档：“机器学习很有趣。”
* 特征 (TF-IDF分数)：{“机器”: 0.5, “学习”: 0.5, “很”: 0.1, “有趣”: 0.7}

**词性标记 (POS) 标签**：指示每个单词的语法类别的特征（例如，名词、动词、形容词）。

* 文档：“快速的棕色狐狸跳跃。”
* 特征：{“快速的”: “形容词”, “棕色”: “形容词”, “狐狸”: “名词”, “跳跃”: “动词”}

例如，让我们考虑下面的两个文档：

```python
Sentences:
Dog hates a cat. It loves to go out and play.
Cat loves to play with a ball.
```
我们可以通过将上述两个文档组合在一起来构建一个语料库。

```python
Corpus = “Dog hates a cat. It loves to go out and play. Cat loves to play with a ball.”
```
特征将是所有唯一的单词：

```python
Fetaures: [‘and’, ‘ball’, ‘cat’, ‘dog’, ‘go’, ‘hates’, ‘it’, ‘loves’, ‘out’, ‘play’, ‘to’, ‘with’]
```
我们将其称为特征向量。在这里，我们记住将‘a’视为单个字符并将其移除。

# 4. 自然语言处理是如何工作的？

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TH7ixPJypLQTLtLf.png)

自然语言处理模型通过寻找语言组成部分之间的关系来工作——例如，文本数据集中找到的字母、单词和句子。自然语言处理架构使用各种方法进行数据预处理、特征提取和建模。

让我们简化并分解这些步骤，以便更好地理解。

# 4.1 数据预处理

理解自然语言处理的第一步集中在对话和话语在上下文框架内的意义。主要目标是促进语音机器人与人类之间的有意义对话。

例如，向聊天机器人发出命令，如“给我展示最佳食谱”或“播放派对音乐”，属于此步骤的范围。它涉及在正在进行的对话上下文中理解和响应用户请求。

在模型处理特定任务的文本之前，文本通常需要进行预处理，以提高模型性能或将单词和字符转换为模型可以理解的格式。在此数据预处理中可能使用各种技术：

## 4.1.1 分词

将文本拆分成称为标记的小单位的过程，这些单位可以是单词、子词或字符。

**类型**：

* **单词分词**：将文本拆分为单个单词。示例：

*“I study Machine Learning on GeeksforGeeks.” 将被分词为 [‘I’, ‘study’, ‘Machine’, ‘Learning’, ‘on’, ‘GeeksforGeeks’, ‘.’]*

* **句子分词**：将文本拆分为单个句子。示例：

*“I study Machine Learning on GeeksforGeeks. Currently, I’m studying NLP” 将被分词为 [‘I study Machine Learning on GeeksforGeeks.’, ‘Currently, I’m studying NLP.’]*

* **子词分词**：将单词拆分为更小的单位，如前缀、后缀或单个字符。

**重要性**：分词是许多NLP流程中的第一步，并影响后续处理阶段。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*x2RW4avK5SoKWtZV.png)

## 4.1.2 词干提取

**定义**：通过去除后缀和前缀将单词简化为其根形式的过程。

**示例**：单词“running”和“runner”被提取为“run”。

**与词形还原的区别**：与词形还原相比，词干提取是一种更粗糙的技术，可能并不总是产生真实的单词。

## 4.1.3 词形还原

**定义**：将一个词还原为其基本形式或词典形式的过程，称为词元。

**示例**：单词“running”和“ran”被还原为“run”。

**重要性**：通过将不同形式的词归类，有助于理解词的基本含义。

词干提取器比词形还原器更快，计算成本更低。


```python
from nltk.stem import WordNetLemmatizer
# create an object of class WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize("plays", 'v'))
print(lemmatizer.lemmatize("played", 'v'))
print(lemmatizer.lemmatize("play", 'v'))
print(lemmatizer.lemmatize("playing", 'v'))
```
**输出** :


```python
play
play
play
play
```
词形还原涉及将同一词的屈折形式归为一类。通过这种方式，我们可以找到任何词的基本形式，这在意义上是有意义的。这里的基本形式称为词元。


```python
from nltk.stem import WordNetLemmatizer

# create an object of class WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("Communication", 'v'))
```
**输出** :


```python
Communication
```
词形还原器比词干提取器更慢，计算成本更高。

## 4.1.4 规范化

自然语言处理（NLP）中的规范化是指将文本转换为标准形式的过程。规范化的目的是确保文本数据处理的一致性，使NLP模型能够更好地理解和解释文本。一些常见的规范化技术包括将文本转换为小写、词干提取、词形还原，以及去除停用词或标点符号。

**小写化**：

* 将文本中的所有字符转换为小写，以确保像“Apple”和“apple”这样的单词被视为相同的单词。
* **示例**：`"Apple"` -> `"apple"`

**去除标点符号**：

* 在NLP任务中，标点符号通常没有重要意义，因此通常会被去除。
* **示例**：`"Hello, world!"` -> `"Hello world"`

**去除停用词**：

* 停用词是一些常见的词（如“和”、“的”、“是”），这些词可能对文本的意义贡献不大，通常会被去除。
* **示例**：`"This is a pen"` -> `"pen"`


```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import re

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# Sample text
text = "Running is better than walking! Apples and oranges are different."
# Lowercasing
text_lower = text.lower()
print("Lowercased text:", text_lower)

# Removing punctuation
text_no_punct = re.sub(r'[^\w\s]', '', text_lower)
print("Text without punctuation:", text_no_punct)

# Tokenization
words = nltk.word_tokenize(text_no_punct)
print("Tokenized words:", words)

# Removing stop words
stop_words = set(stopwords.words('english'))
words_no_stop = [word for word in words if word not in stop_words]
print("Text without stopwords:", words_no_stop)

# Stemming
ps = PorterStemmer()
words_stemmed = [ps.stem(word) for word in words_no_stop]
print("Stemmed words:", words_stemmed)

# Lemmatization
lemmatizer = WordNetLemmatizer()
words_lemmatized = [lemmatizer.lemmatize(word) for word in words_no_stop]
print("Lemmatized words:", words_lemmatized)
```
Output:


```python
Lowercased text: running is better than walking! apples and oranges are different.
Text without punctuation: running is better than walking apples and oranges are different
Tokenized words: ['running', 'is', 'better', 'than', 'walking', 'apples', 'and', 'oranges', 'are', 'different']
Text without stopwords: ['running', 'better', 'walking', 'apples', 'oranges', 'different']
Stemmed words: ['run', 'better', 'walk', 'appl', 'orang', 'differ']
Lemmatized words: ['running', 'better', 'walking', 'apple', 'orange', 'different']
```

## 4.1.5 词性标注 (POS)

**定义**：将句子中的每个单词分配词性，如名词、动词、形容词等的过程。

**重要性**：有助于理解句子的语法结构和意义。

**示例**：*“GeeksforGeeks 是一个计算机科学平台。”让我们看看 NLTK 的词性标注器将如何标注这个句子。*


```python
from nltk import pos_tag
from nltk import word_tokenize

text = "GeeksforGeeks is a Computer Science platform."
tokenized_text = word_tokenize(text)
tags = tokens_tag = pos_tag(tokenized_text)
tags
```
**输出**：


```python
[('GeeksforGeeks', 'NNP'),
 ('is', 'VBZ'),
 ('a', 'DT'),
 ('Computer', 'NNP'),
 ('Science', 'NNP'),
 ('platform', 'NN'),
 ('.', '.')]
```

# 4.2 特征提取

大多数传统的机器学习技术依赖于特征——通常是描述文档与其包含的语料库之间关系的数字——这些特征由词袋模型、TF-IDF或通用特征工程（例如文档长度、词语极性和元数据（例如，文本是否有相关标签或评分））生成。更近期的技术包括Word2Vec、GLoVE，以及在神经网络的训练过程中学习特征。

## 4.2.1 词袋模型 (BoW)

**定义**：文本数据的一种表示方式，其中每个文档被表示为其单词的袋（多重集合），忽略语法和单词顺序，但保留词频。

词袋模型是自然语言处理（NLP）中常用的技术，其中文本中的每个单词被表示为一个特征，其频率被用作特征值。

**重要性**：简化文本处理，并用于各种NLP应用，如文本分类和信息检索。

```python
from sklearn.feature_extraction.text import CountVectorizer

# Sample text data
documents = [
    "Barack Obama was the 44th President of the United States",
    "The President lives in the White House",
    "The United States has a strong economy"
]

# Initialize the CountVectorizer
vectorizer = CountVectorizer()

# Fit the model and transform the documents into a Bag of Words
bow_matrix = vectorizer.fit_transform(documents)

# Get the feature names (unique words in the corpus)
feature_names = vectorizer.get_feature_names_out()

# Convert the Bag of Words matrix into an array
bow_array = bow_matrix.toarray()

# Display the Bag of Words
print("Feature Names (Words):", feature_names)
print("\nBag of Words Representation:")
print(bow_array)
```
输出：

```python
Feature Names (Words): ['44th' 'barack' 'economy' 'has' 'house' 'in' 'lives' 'obama' 'of'
 'president' 'states' 'strong' 'the' 'united' 'was' 'white']

Bag of Words Representation:
[[1 1 0 0 0 0 0 1 1 1 1 0 2 1 1 0]
 [0 0 0 0 1 1 1 0 0 1 0 0 2 0 0 1]
 [0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0]]
```
![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sK9pIrYSFfqlsDbC.png)

## 4.2.2 词频-逆文档频率 (TF-IDF)

**定义**：TF-IDF（词频-逆文档频率）是一种数值统计，用于评估一个词在文档中相对于文档集合（或语料库）的重要性。基本思想是，如果一个词在某个文档中频繁出现，但在其他文档中不多见，那么它应该被赋予更高的权重。

在语料库 ***D*** 中，文档 ***d*** 中词 ***t*** 的 TF-IDF 分数计算为两个指标的乘积：**词频 (TF)** 和 **逆文档频率 (IDF)**。

**1. 词频 (TF)**

词频 (TF) 衡量一个术语在文档中出现的频率。通常通过文档中的总词数进行归一化，以防止对较长文档的偏见。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lHcuLP6GhQjFm72nOTQKkQ.png)

**2. 逆文档频率 (IDF)**

逆文档频率 (IDF) 衡量一个术语在整个语料库中的重要性。它降低在许多文档中出现的术语的权重，并增加在较少文档中出现的术语的权重。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gKCRLUQrbnjpcLaHSNqZCw.png)

* 为了防止在术语未出现在任何文档中时出现除以零的情况，分母中添加了“+1”。

**3. 结合 TF 和 IDF：TF-IDF**

TF-IDF 分数通过将文档 ***d*** 中术语 ***t*** 的 **TF** 值与 **IDF** 值相乘来计算：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*k4AxTcAeMbn7kvo0iZqFiw.png)

**重要性**：有助于识别文档中的重要词汇，广泛应用于信息检索和文本挖掘。

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample text data (documents)
documents = [
    "The cat sat on the mat.",
    "The cat sat on the bed.",
    "The dog barked."
]

# Initialize the TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit the model and transform the documents into TF-IDF representation
tfidf_matrix = vectorizer.fit_transform(documents)

# Get the feature names (unique words in the corpus)
feature_names = vectorizer.get_feature_names_out()

# Convert the TF-IDF matrix into an array
tfidf_array = tfidf_matrix.toarray()

# Display the TF-IDF matrix
print("Feature Names (Words):", feature_names)
print("\nTF-IDF Matrix:")
print(tfidf_array)
```
Output:


```python
Feature Names (Words): ['barked' 'bed' 'cat' 'dog' 'mat' 'on' 'sat' 'the']

TF-IDF Matrix:
[[0.         0.         0.37420726 0.         0.49203758 0.37420726
  0.37420726 0.58121064]
 [0.         0.49203758 0.37420726 0.         0.         0.37420726
  0.37420726 0.58121064]
 [0.65249088 0.         0.         0.65249088 0.         0.
  0.         0.38537163]]
```
![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*bK8wXF-TtjQpCiVo.png)

## 4.2.3 N-grams

**定义**：来自给定文本或语音的连续 n 项（通常是单词或字符）序列。

**类型**：

* **单元语法（Unigram）**：单个单词。
* **双元语法（Bigram）**：一对单词。
* **三元语法（Trigram）**：三个单词的序列。

**重要性**：捕捉文本中的上下文和单词依赖关系。


```python
import nltk
from nltk.util import ngrams
from collections import Counter

# Sample text data
text = "The quick brown fox jumps over the lazy dog"

# Tokenize the text into words
tokens = nltk.word_tokenize(text)

# Generate Unigrams (1-gram)
unigrams = list(ngrams(tokens, 1))
print("Unigrams:")
print(unigrams)

# Generate Bigrams (2-gram)
bigrams = list(ngrams(tokens, 2))
print("\nBigrams:")
print(bigrams)

# Generate Trigrams (3-gram)
trigrams = list(ngrams(tokens, 3))
print("\nTrigrams:")
print(trigrams)

# Count frequency of each n-gram (for demonstration)
unigram_freq = Counter(unigrams)
bigram_freq = Counter(bigrams)
trigram_freq = Counter(trigrams)
# Print frequencies (optional)
print("\nUnigram Frequencies:")
print(unigram_freq)
print("\nBigram Frequencies:")
print(bigram_freq)
print("\nTrigram Frequencies:")
print(trigram_freq)
```
输出：


```python
Unigrams:
[('The',), ('quick',), ('brown',), ('fox',), ('jumps',), ('over',), ('the',), ('lazy',), ('dog',)]

Bigrams:
[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]

Trigrams:
[('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog')]

Unigram Frequencies:
Counter({('The',): 1, ('quick',): 1, ('brown',): 1, ('fox',): 1, ('jumps',): 1, ('over',): 1, ('the',): 1, ('lazy',): 1, ('dog',): 1})

Bigram Frequencies:
Counter({('The', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1})

Trigram Frequencies:
Counter({('The', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', 'jumps'): 1, ('fox', 'jumps', 'over'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1})
```

## 4.2.4 词嵌入

**定义**：词嵌入是词的密集向量表示，能够捕捉词的意义、句法属性以及与其他词的关系。与传统方法如词袋模型或TF-IDF不同，后者将词视为离散实体，词嵌入将词映射到一个连续的向量空间中，语义相似的词彼此靠近。

**示例**：Word2Vec, GloVe, FastText.

**重要性**：捕捉词之间的语义关系，并在各种自然语言处理模型和应用中使用。


```python
import gensim
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

# Sample text data
text = [
    "The cat sat on the mat",
    "The dog barked at the cat",
    "The cat chased the mouse",
    "The dog chased the cat",
]

# Tokenize the sentences into words
tokenized_text = [word_tokenize(sentence.lower()) for sentence in text]

# Train a Word2Vec model on the tokenized text
model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=0)

# Get the word embeddings for a specific word
cat_vector = model.wv['cat']

# Print the word embedding for 'cat'
print("Word Embedding for 'cat':")
print(cat_vector)

# Find words most similar to 'cat'
similar_words = model.wv.most_similar('cat', topn=3)
print("\nWords most similar to 'cat':")
print(similar_words)
```
Output:


```python
Word Embedding for 'cat':
[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03
  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03
 -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03
 -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03
  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04
  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03
  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03
 -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03
 -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03
  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03
  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04
 -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03
  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03
 -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03
  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03
 -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03
  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03
  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03
  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03
 -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03
 -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03
  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03
  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03
  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03
  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]

Words most similar to 'cat':
[('dog', 0.06797593832015991), ('on', 0.033640578389167786), ('at', 0.009391162544488907)]
```

## 4.2.5 上下文词嵌入

**定义**：上下文词嵌入根据句子中的上下文捕捉单词的含义。与传统的词嵌入（如 Word2Vec）不同，后者为每个单词提供一个单一的向量，而不考虑上下文，上下文嵌入则根据周围的单词为一个单词生成不同的向量。这些嵌入是使用像 BERT（双向编码器表示来自变换器）这样的模型生成的，这些模型在计算单词表示时考虑整个句子。

**示例**：ELMo, BERT, GPT.

**重要性**：通过考虑周围的上下文提供更准确的单词表示。


```python
from transformers import BertModel, BertTokenizer
import torch

# Load pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Example sentence
sentence = "The cat sat on the mat."

# Tokenize the input sentence and convert tokens to tensor
input_ids = tokenizer.encode(sentence, return_tensors='pt')

# Pass the input through the BERT model to get embeddings
with torch.no_grad():
    outputs = model(input_ids)
    last_hidden_states = outputs.last_hidden_state

# Print the shape of the last hidden states tensor
print("Shape of last hidden states:", last_hidden_states.shape)

# Convert the embeddings to numpy array (for easier manipulation)
embeddings = last_hidden_states.squeeze().numpy()

# Tokenize the sentence to match embeddings to words
tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())

# Print the tokens and their corresponding contextual embeddings
for token, embedding in zip(tokens, embeddings):
    print(f"Token: {token}")
    print(f"Embedding: {embedding[:10]}...")  # Print first 10 dimensions for brevity
    print()
```
输出：


```python
Shape of last hidden states: torch.Size([1, 9, 768])
Token: [CLS]
Embedding: [-0.3642237  -0.05305378 -0.36732262 -0.02967339 -0.460784   -0.10106134
  0.01669817  0.59577715 -0.11770311  0.10289837]...

Token: the
Embedding: [-0.3978658  -0.27210808 -0.68196577 -0.00734524  0.7860015   0.17661774
  0.05241349  0.72017133  0.07858636 -0.1736162 ]...

Token: cat
Embedding: [-0.35117194 -0.07356024 -0.06913986 -0.13987705  0.68294847  0.11350538
  0.20849192  0.56738263  0.4069492  -0.2134663 ]...

Token: sat
Embedding: [ 0.07117955 -0.31366652  0.09802647  0.06934201  0.4834015  -0.40465042
 -0.5492254   0.91489977 -0.19875513  0.16641603]...

Token: on
Embedding: [-0.5203689  -0.59298277  0.28364897  0.31230223  0.611251   -0.07076924
 -1.1455988   0.3248083  -0.40707844 -0.04888151]...

Token: the
Embedding: [-0.46198413 -0.5197541  -0.37599826  0.5099069   0.47716403 -0.41718286
 -0.4499631   0.41355488 -0.52844054 -0.38209906]...

Token: mat
Embedding: [-0.0415443  -0.10548864 -0.28080556  0.5944824   0.05494812 -0.33329046
  0.23721729  0.21435769 -0.5872034  -0.5192848 ]...

Token: .
Embedding: [-0.23536152 -0.4874898  -0.16314735  0.24718559  0.16603808 -0.10894424
 -0.47729397  0.72053766 -0.12877737 -0.6664553 ]...

Token: [SEP]
Embedding: [ 0.66511077  0.02249792 -0.41309452  0.34166738 -0.2383636  -0.40086323
  0.6143277   0.11614177  0.33811757  0.20712788]...
```

# 4.3 建模

在数据预处理后，将其输入到一个NLP架构中，以对数据进行建模以完成各种任务。

* 通过上述技术提取的数值特征可以根据手头的任务输入到各种模型中。例如，对于分类，TF-IDF向量化器的输出可以提供给逻辑回归、朴素贝叶斯、决策树或梯度提升树。或者，对于命名实体识别，我们可以使用隐马尔可夫模型以及n-grams。
* 深度 [神经网络](https://deeplearning.ai/the-batch/neural-networks-find-the-function/) 通常在不使用提取特征的情况下工作，尽管我们仍然可以将TF-IDF或词袋特征作为输入。
* 语言模型：简单来说，语言模型的目标是在给定一串输入单词时预测下一个单词。使用马尔可夫假设的概率模型就是一个例子：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Q4FNhR8AOsW29EHVUCJ3uw.png)

深度学习也用于创建这样的语言模型。深度学习模型以词嵌入作为输入，并在每个时间状态返回下一个单词的概率分布，作为字典中每个单词的概率。预训练的语言模型通过处理大量语料库（如维基百科）来学习特定语言的结构。然后可以对其进行微调以适应特定任务。例如，BERT已针对从 [事实检查](https://www.deeplearning.ai/the-batch/keeping-the-facts-straight/) 到 [撰写标题](https://www.deeplearning.ai/the-batch/ai-makes-headlines/) 的任务进行了微调。

## 4.3.1 命名实体识别 (NER)

**定义**：识别和分类文本中命名实体的过程，将其分为预定义的类别，如人名、组织、地点、日期等。

**示例**：在句子“巴拉克·奥巴马是美国第44任总统”中，“巴拉克·奥巴马”被识别为一个人，而“美国”被识别为一个GPE（地缘政治实体）。

```python
import spacy

# Load the pre-trained NLP model from spacy
nlp = spacy.load('en_core_web_sm')

# The sentence for which we want to perform NER
sentence = "Barack Obama was the 44th President of the United States."

# Process the sentence using the NLP model
doc = nlp(sentence)

# Print the named entities recognized in the sentence
print("Named Entities in the sentence:")
for ent in doc.ents:
    print(f"{ent.text}: {ent.label_}")
```
Output:

```python
Named Entities in the sentence:
Barack Obama: PERSON
44th: ORDINAL
the United States: GPE
```

## 4.3.2 语言模型

**定义**：语言模型是一种统计模型，用于预测一系列单词中的下一个单词。它为单词序列分配概率，帮助确定给定序列的可能性。语言模型在各种自然语言处理（NLP）任务中是基础，如文本生成、机器翻译和语音识别。

**类型**：

* **单元模型**：根据每个单词的概率独立预测每个单词。
* **二元/三元模型**：根据前一个或两个单词预测一个单词。
* **神经语言模型**：使用神经网络，如递归神经网络（RNN）或变换器，捕捉文本中更复杂的模式和依赖关系。

**重要性**：在文本生成、机器翻译和语音识别等任务中具有基础性作用。


```python
import nltk
from collections import defaultdict, Counter
import random

# Sample text data (corpus)
corpus = [
    "I love natural language processing",
    "I love machine learning",
    "I enjoy learning new things",
    "Natural language processing is fascinating"
]

# Tokenize the sentences into words
tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]

# Create bigrams from the tokenized corpus
bigrams = []
for sentence in tokenized_corpus:
    bigrams.extend(list(nltk.bigrams(sentence)))

# Calculate bigram frequencies
bigram_freq = defaultdict(Counter)
for w1, w2 in bigrams:
    bigram_freq[w1][w2] += 1

# Calculate bigram probabilities
bigram_prob = defaultdict(dict)
for w1 in bigram_freq:
    total_count = float(sum(bigram_freq[w1].values()))
    for w2 in bigram_freq[w1]:
        bigram_prob[w1][w2] = bigram_freq[w1][w2] / total_count

# Function to generate text using the bigram model
def generate_sentence(start_word, num_words=10):
    current_word = start_word
    sentence = [current_word]
    for _ in range(num_words - 1):
        if current_word in bigram_prob:
            next_word = random.choices(
                list(bigram_prob[current_word].keys()),
                list(bigram_prob[current_word].values())
            )[0]
            sentence.append(next_word)
            current_word = next_word
        else:
            break
    return ' '.join(sentence)

# Generate a sentence starting with the word "i"
generated_sentence = generate_sentence("i", num_words=4)
print("Generated sentence:", generated_sentence)
```
Output:


```python
Generated sentence: i love natural language
```

## 4.3.3 传统机器学习自然语言处理技术：

* [**逻辑回归**](https://deeplearning.ai/the-batch/logistic-regression-follow-the-curve/)是一种监督分类算法，旨在根据一些输入预测事件发生的概率。在自然语言处理（NLP）中，逻辑回归模型可以应用于解决情感分析、垃圾邮件检测和毒性分类等问题。
* [**朴素贝叶斯**](https://scikit-learn.org/stable/modules/naive_bayes.html)是一种监督分类算法，使用以下贝叶斯公式找到条件概率分布 *P(label | text)*：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K8HJjg3GJUmpbwpJYeIRkg.png)

并根据哪个联合分布具有最高概率进行预测。朴素贝叶斯模型中的朴素假设是个别单词是独立的。因此：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l99VM1dtEgWM1LMFaXRzBA.png)

在NLP中，这种统计方法可以应用于解决垃圾邮件检测或[查找软件代码中的错误](https://deeplearning.ai/the-batch/bug-squasher/)等问题。

* [**决策树**](https://deeplearning.ai/the-batch/decision-trees-from-root-to-leaves/)是一类监督分类模型，根据不同特征对数据集进行划分，以最大化这些划分中的[信息增益](https://mlwhiz.com/blog/2019/11/12/dtsplits/)。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PfRuFnd9AEy2qda7.png)

* [**潜在狄利克雷分配（LDA）**](https://dl.acm.org/doi/10.5555/944919.944937)用于主题建模。LDA试图将文档视为主题的集合，而主题则视为单词的集合。LDA是一种统计方法。其直觉是，我们可以仅使用语料库中的一小组单词来描述任何主题。
* [**隐马尔可夫模型**](https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)**:** 马尔可夫模型是一种概率模型，根据当前状态决定系统的下一个状态。例如，在NLP中，我们可能会根据前一个单词建议下一个单词。我们可以将其建模为马尔可夫模型，在该模型中，我们可能会找到从word1到word2的转移概率，即P(word1|word2)。然后，我们可以使用这些转移概率的乘积来找到句子的概率。隐马尔可夫模型（HMM）是一种概率建模技术，它在马尔可夫模型中引入了一个隐藏状态。隐藏状态是数据的一个属性，无法直接观察。HMM用于词性标注（POS），其中句子的单词是观察状态，而词性标签是隐藏状态。HMM引入了一个叫做发射概率的概念；给定隐藏状态的观察值的概率。在前面的例子中，这是给定其词性标签的单词的概率。HMM假设这个概率可以被反转：给定一个句子，我们可以根据一个单词具有某个词性标签的可能性以及特定词性标签跟随前一个单词的词性标签的概率来计算每个单词的词性标签。在实践中，这通过维特比算法解决。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*o_1_UxhYB3vMvI2_.png)

## 4.3.4 深度学习自然语言处理技术

[**卷积神经网络**](https://arxiv.org/abs/1511.08458) **(CNN):** 使用CNN对文本进行分类的想法最早在Yoon Kim的论文“[用于句子分类的卷积神经网络](https://www.aclweb.org/anthology/D14-1181)”中提出。其核心直觉是将文档视为图像。然而，输入不是像素，而是作为单词矩阵表示的句子或文档。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*iVl5sqhoUI9HVVD_.png)

[**递归神经网络 (RNN)**](https://arxiv.org/abs/1808.03314)**:** 许多使用深度学习的文本分类技术通过n-grams或窗口（CNNs）处理近距离的单词。它们可以将“New York”视为一个单独的实例。然而，它们无法捕捉特定文本序列提供的上下文。它们不学习数据的顺序结构，其中每个单词依赖于前一个单词或前一个句子中的单词。RNN通过隐藏状态记住先前的信息，并将其与当前任务连接。被称为门控递归单元（GRU）和长短期记忆（LSTM）的架构是旨在长时间记住信息的RNN类型。此外，双向LSTM/GRU在两个方向上保持上下文信息，这在文本分类中非常有用。RNN还被用于[生成数学证明](https://deeplearning.ai/the-batch/the-proof-is-in-the-network/)和[将人类思想](https://deeplearning.ai/the-batch/listening-to-the-brain-2/)翻译成文字。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*40BJQ1BBZimlU1N4.png)

[**自编码器**](https://www.deeplearningbook.org/contents/autoencoders.html)**:** 这些是深度学习编码器-解码器，近似从X到X的映射，即输入=输出。它们首先将输入特征压缩为低维表示（有时称为潜在编码、潜在向量或潜在表示），并学习重建输入。表示向量可以作为输入用于单独的模型，因此该技术可用于降维。在许多其他领域的专家中，遗传学家已将自编码器应用于[发现与疾病相关的突变](https://deeplearning.ai/the-batch/spot-the-bad-mutation/)在氨基酸序列中。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*x_yoaHFH_-y6qMuY.png)

[**编码器-解码器序列到序列**](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)**:** 编码器-解码器seq2seq架构是针对翻译、摘要和类似任务的自编码器的适应。编码器将文本中的信息封装到编码向量中。与自编码器不同，解码器的任务不是从编码向量重建输入，而是生成不同的期望输出，如翻译或摘要。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7RpgaSUSWH3GsuNh.png)

[**变换器**](https://www.deeplearning.ai/the-batch/transformer-takeover/)**:** 变换器是一种模型架构，首次在2017年的论文“[注意力机制是你所需要的一切](https://arxiv.org/pdf/1706.03762.pdf)”（Vaswani, Shazeer, Parmar等）中描述，摒弃了递归，而是完全依赖自注意力机制来建立输入和输出之间的全局依赖关系。由于该机制一次处理所有单词（而不是逐个处理），这降低了与RNN相比的训练速度和推理成本，尤其是因为它是可并行化的。变换器架构在近年来彻底改变了自然语言处理，导致了包括[BLOOM](https://deeplearning.ai/the-batch/large-language-models-unbound/)、[Jurassic-X](https://deeplearning.ai/the-batch/neural-nets-rules-truer-text/)和[Turing-NLG](https://deeplearning.ai/the-batch/toward-1-trillion-parameters/)等模型的出现。它还成功应用于各种不同的[视觉任务](https://deeplearning.ai/the-batch/less-data-for-vision-transformers/)，包括制作[3D图像](https://deeplearning.ai/the-batch/transformers-see-in-3d/)。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*X6c3klO-sqEaMFPH.png)

**变换器模型的关键概念**

**自注意力机制**：

* 自注意力允许模型在生成输出的每个部分时关注输入序列的不同部分。它计算输入特征的加权和，权重由每个特征与其他特征的相关性决定。

**多头注意力**：

* 模型使用不同的权重矩阵（头）多次计算注意力，然后将结果连接起来。这使得模型能够捕捉单词之间关系的不同方面。

**位置编码**：

* 由于变换器没有内置的单词顺序感知（与RNN不同），因此在输入嵌入中添加位置编码，以提供有关每个单词在序列中位置的信息。

**前馈神经网络**：

* 在注意力机制之后，数据通过前馈神经网络，该网络对每个位置分别且相同地应用。

**层归一化和残差连接**：

* 变换器模型中的每个子层通过残差连接连接，并随后进行层归一化，这有助于稳定深度模型的训练。
* **重要性**：由于其捕捉长距离依赖关系和并行计算的能力，在许多自然语言处理任务中处于最先进水平。

```python
import torch
import torch.nn as nn
import math

class TransformerModel(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, ffn_dim, max_seq_len, num_classes):
        super(TransformerModel, self).__init__()
        
        self.embedding = nn.Embedding(input_dim, model_dim)
        self.model_dim = model_dim
        self.layers = nn.ModuleList([
            TransformerLayer(model_dim, num_heads, ffn_dim)
            for _ in range(num_layers)
        ])
        self.fc_out = nn.Linear(model_dim, num_classes)
        
    def forward(self, x):
        seq_len = x.size(1)
        positional_encoding = self._generate_positional_encoding(seq_len)
        x = self.embedding(x) + positional_encoding
        for layer in self.layers:
            x = layer(x)
        return self.fc_out(x.mean(dim=1))
    
    def _generate_positional_encoding(self, seq_len):
        positional_encoding = torch.zeros(seq_len, self.model_dim)
        for pos in range(seq_len):
            for i in range(0, self.model_dim, 2):
                positional_encoding[pos, i] = math.sin(pos / (10000 ** (i / self.model_dim)))
                positional_encoding[pos, i + 1] = math.cos(pos / (10000 ** ((i + 1) / self.model_dim)))
        return positional_encoding.unsqueeze(0)

class TransformerLayer(nn.Module):
    def __init__(self, model_dim, num_heads, ffn_dim):
        super(TransformerLayer, self).__init__()
        self.multihead_attention = nn.MultiheadAttention(embed_dim=model_dim, num_heads=num_heads)
        self.norm1 = nn.LayerNorm(model_dim)
        self.ffn = nn.Sequential(
            nn.Linear(model_dim, ffn_dim),
            nn.ReLU(),
            nn.Linear(ffn_dim, model_dim)
        )
        self.norm2 = nn.LayerNorm(model_dim)
        
    def forward(self, x):
        attn_output, _ = self.multihead_attention(x, x, x)
        x = self.norm1(x + attn_output)
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        return x

# Example usage:
input_dim = 10000  # Vocabulary size
model_dim = 512
num_heads = 8
num_layers = 6
ffn_dim = 2048
max_seq_len = 100
num_classes = 10

# Instantiate the model
model = TransformerModel(input_dim, model_dim, num_heads, num_layers, ffn_dim, max_seq_len, num_classes)

# Example input (batch_size=32, sequence_length=50)
x = torch.randint(0, input_dim, (32, 50))

# Forward pass
output = model(x)
print("Output shape:", output.shape)
```
Output:


```python
Output shape: torch.Size([32, 10])
```

## 4.3.5 注意力机制

* **定义**：一种技术，允许模型在进行预测时关注输入序列的不同部分。
* **重要性**：通过提供上下文相关的信息，提高序列到序列任务的性能。

## 4.3.6 序列到序列（Seq2Seq）模型

* **定义**：一种用于输入和输出都是序列的任务的模型架构，例如机器翻译。
* **组件**：通常包括一个编码器和一个解码器，常使用 RNN、LSTM 或变换器。
* **重要性**：用于翻译、摘要和对话系统等任务。

## 4.3.7 迁移学习

* **定义**：在新的相关任务上使用预训练模型的技术，所需的额外训练最小化。
* **重要性**：允许利用现有知识，减少对大型标注数据集和广泛训练的需求。

## 4.3.8 微调

* **定义**：将预训练模型进行调整以适应特定任务的过程，使用额外的训练数据。
* **重要性**：提高预训练模型在特定任务应用上的性能。

## 4.3.9 零样本学习

* **定义**: 模型在没有明确训练的情况下，利用一般知识执行任务的能力。
* **重要性**: 展示了模型在没有特定任务训练数据的情况下，概括和适应新任务的能力。

## 4.3.10 少样本学习

* **定义**：模型从极少量示例中学习的能力。
* **重要性**：减少对大量标注数据的需求，并展示模型快速适应新任务的能力。

# 5. NLP模型和技术的比较分析

让我们分析每个模型和技术与其前身的比较，突出它们的优点和局限性：

* **词袋模型（BoW）与TF-IDF**：BoW计算文档中单词的出现次数，而TF-IDF根据单词在文档和整个语料库中的重要性为单词分配权重。TF-IDF通过对稀有单词给予更多重要性，解决了BoW的一个关键局限性，从而更有效地捕捉文档的含义。然而，这两种方法都未考虑单词的顺序和上下文，导致语义信息的丧失。
* **Word2Vec与BoW和TF-IDF**：Word2Vec是一种基于神经网络的技术，学习连续的单词嵌入，捕捉单词之间的语义关系。与BoW和TF-IDF不同，Word2Vec保留了上下文信息，并在稠密向量空间中表示单词，使其能够捕捉同义词、反义词和类比等语义关系。然而，Word2Vec也有其局限性，例如对词汇表外（OOV）单词的处理困难，以及无法根据上下文捕捉单词的不同含义（多义性）。
* **RNN（包括LSTM和GRU）与Word2Vec**：虽然Word2Vec专注于单词表示，但RNN（递归神经网络）旨在建模数据序列，包括文本。RNN能够处理不同长度的输入序列，并保持一个隐藏状态，以捕捉来自前一个时间步的信息。这使得RNN在涉及时间依赖的任务（如情感分析或机器翻译）中比Word2Vec更为合适。
* **LSTM和GRU与普通RNN**：普通RNN存在梯度消失问题，这妨碍了它们学习长距离依赖的能力。LSTM（长短期记忆）和GRU（门控递归单元）是旨在克服这一局限性的RNN变体，通过引入门控机制有效捕捉长期依赖。然而，包括LSTM和GRU在内的RNN在处理长序列时可能计算开销较大。
* **双向LSTM与LSTM**：双向LSTM通过同时处理输入序列的正向和反向，扩展了传统LSTM。这使得模型能够捕捉来自过去和未来上下文的信息，通常在命名实体识别和情感分析等任务中提高性能。然而，由于额外的反向传递，这种方法比标准LSTM计算上更为密集。
* **Transformer与RNN（LSTM，GRU）**：Transformer是一种神经网络架构，利用自注意力机制建模序列中单词之间的依赖关系。与RNN不同，Transformer可以并行处理输入序列，使其在处理长序列时更具计算效率。此外，Transformer在捕捉长距离依赖方面更为有效，因为它们不受序列长度的限制。然而，由于自注意力机制，Transformer可能会占用大量内存，并且通常需要大量训练数据以达到最佳性能。

# 6. 自然语言处理的演变

到现在为止，我们已经对自然语言处理（NLP）有了很好的理解。让我们深入探讨NLP的演变，以了解NLP演变的完整图景。我们将在即将到来的博客中详细讨论每一个阶段。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8fe53QLji5Ix3qIC.png)

自1950年代以来，自然语言处理（NLP）经历了显著的变革。这段旅程可以分为不同的时代，每个时代都以突破性的进展和创新为标志。让我们深入探讨NLP的演变，从早期的基于规则的系统到我们今天所看到的复杂的大型语言模型（LLMs）。

## 6.1 1950年代到1980年代中期：早期阶段与基于规则的方法

NLP的起源可以追溯到1950年代，重点关注基于规则的方法。在此期间，研究人员旨在将语言规则编码到计算机程序中，希望使机器能够理解和生成自然语言。

**关键里程碑**- **1957年：**诺姆·乔姆斯基的开创性著作，[*句法结构*](https://web.stanford.edu/~jurafsky/slp3/4.pdf)，引入了转换语法，对计算语言学产生了深远影响。- **1966年：**ALPAC报告对早期机器翻译的努力提出质疑，突显了语言处理的挑战和复杂性。

## 6.2 1980年代末到2000年：统计方法与首个网络架构

1980年代末，随着计算能力的提高和大型数据集的可用性，方法从基于规则转向统计方法。

**关键里程碑**- **1985年：** 循环神经网络（RNN）的引入为基于序列的数据处理奠定了基础。 [*通过反向传播误差学习表示*](https://www.researchgate.net/publication/221618248_Learning_Representations_by_Back-Propagating_Errors)- **1989年：** 隐马尔可夫模型（HMM）因其在语音识别和其他基于序列的任务中的应用而变得流行。 [*用于语音识别的隐马尔可夫模型*](https://web.stanford.edu/~jurafsky/slp3/8.pdf)- **1997年：** 由Hochreiter和Schmidhuber开发的长短期记忆（LSTM）网络，通过解决梯度消失问题改善了RNN。 [*长短期记忆*](https://www.bioinf.jku.at/publications/older/2604.pdf)

**统计方法的发展**

- **词袋模型（BoW）：** 一种基础方法，通过计算单词出现次数来表示文本数据，忽略语法和单词顺序。- **TF-IDF（词频-逆文档频率）：** 相较于BoW的增强方法，通过与文档出现频率成反比的方式加权单词频率，突出稀有且信息量大的单词。 [*信息检索导论*](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)

## 6.3 2000年代初至2018年：深度学习与神经网络的崛起

2000年代初期，深度学习技术的崛起，利用在庞大数据集上训练的大型神经网络，彻底改变了自然语言处理（NLP）。

**关键里程碑**

* **2013年：** Mikolov等人提出的Word2Vec，使得密集的词嵌入得以创建，捕捉词语之间的语义关系。 [*Efficient Estimation of Word Representations in Vector Space*](https://arxiv.org/pdf/1301.3781.pdf)
* **2014年：** Sutskever、Vinyals和Le提出的序列到序列学习和编码器-解码器架构，显著推动了机器翻译的发展。 [*Sequence to Sequence Learning with Neural Networks*](https://arxiv.org/abs/1409.3215)
* **2014年：** Bahdanau、Cho和Bengio引入了注意力机制，使模型在翻译过程中能够关注输入序列的特定部分。 [*Neural Machine Translation by Jointly Learning to Align and Translate*](https://arxiv.org/abs/1409.0473)
* **2017年：** Google的Transformer模型在论文 [*Attention is All You Need*](https://arxiv.org/abs/1706.03762) 中提出，完全依赖注意力机制，彻底改变了NLP领域。

**词嵌入方法的详细发展**

词嵌入改变了机器理解词语的方式。这个旅程始于Word2Vec，这是一种通过“向量化”词语来处理文本的神经网络，将语义相似的词语放置在连续的向量空间中。

* **2013年：** Word2Vec通过连续词袋模型（CBOW）和跳字模型（Skip-Gram）转变了词嵌入。 [*Efficient Estimation of Word Representations in Vector Space*](https://arxiv.org/pdf/1301.3781.pdf)
* **2014年：** 斯坦福大学的Pennington等人开发的GloVe，聚合了来自语料库的全局词-词共现统计数据。 [*GloVe: Global Vectors for Word Representation*](https://nlp.stanford.edu/pubs/glove.pdf)
* **2018年：** Facebook AI Research实验室的FastText，通过考虑子词信息改进了Word2Vec，使其在形态丰富的语言中表现出色。 [*Enriching Word Vectors with Subword Information*](https://arxiv.org/abs/1607.04606)

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*nsUdbZEQdqp6y_gF)

**注意力机制**

* **2014年：** Bahdanau等人引入的注意力机制，通过允许模型关注输入序列的相关部分，显著改善了翻译效果。 [*Neural Machine Translation by Jointly Learning to Align and Translate*](https://arxiv.org/abs/1409.0473)
* **2017年：** Vaswani等人提出的Transformer模型，专门使用注意力机制，提高了NLP任务的效率和性能。 [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)

**Transformer中的嵌入技术** Transformer引入了新颖的嵌入技术，包括位置前馈网络和位置编码，帮助模型理解句子中的词序。 [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)

## 6.4 2019年至今：大型语言模型（LLMs）

近年来，大型语言模型（LLMs）推动了自然语言处理（NLP）能力的边界。

**关键里程碑****2019**：像RoBERTa、BART、T5和GPT-2这样的模型为语言理解和生成设定了新的基准。

* [*RoBERTa: A Robustly Optimized BERT Pretraining Approach*](https://arxiv.org/abs/1907.11692)
* [*BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension*](https://arxiv.org/abs/1910.13461)
* [*Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*](https://arxiv.org/abs/1910.10683)
* [*Language Models are Unsupervised Multitask Learners*](https://openai.com/research/better-language-models)

**2020**：OpenAI的GPT-3，拥有1750亿个参数，展示了令人印象深刻的语言生成和理解能力。 [*Language Models are Few-Shot Learners*](https://arxiv.org/abs/2005.14165)

**2021**：随着DeBERTa、T0和开源的GPT-Neo等模型的出现，进展持续。

* [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
* [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)
* [*GPT-Neo*](https://github.com/EleutherAI/gpt-neo)

**2022**：像FlanT5、PaLM、BLOOM和ChatGPT这样的模型将细致和上下文感知的语言理解推向了前沿。

* [*Scaling Instruction-Finetuned Language Models*](https://arxiv.org/abs/2109.01652)
* [*PaLM: Scaling Language Modeling with Pathways*](https://arxiv.org/abs/2204.02311)
* [*BLOOM: A 176B-parameter Open-Access Multilingual Language Model*](https://arxiv.org/abs/2201.01739)
* [*ChatGPT*](https://openai.com/chatgpt)

**2023**：LLaMa、Bard、GPT-4和Claude等模型的发展展示了自然语言处理的持续进步。

* [*LLaMa: Large Language Model Meta AI*](https://arxiv.org/abs/2302.13971)
* [*GPT-4*](https://openai.com/research/gpt-4)
* Bard和Claude是专有模型，详细信息请参见各自公司的官方网站。

**大型语言模型的微调**微调在将预训练的LLMs适应特定任务中至关重要。这涉及在较小的、特定任务的数据集上进行额外训练，使模型能够专业化，同时利用其广泛的预训练知识。

* **2019**：使用BERT和RoBERTa等模型的监督和无监督微调技术变得普遍。
* **2020**：GPT-3突显了少量学习的潜力，即模型仅用少量示例进行微调，以在多样化任务上实现令人印象深刻的表现。 [*Language Models are Few-Shot Learners*](https://arxiv.org/abs/2005.14165)

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*nsUdbZEQdqp6y_gF)

自然语言处理的历史展示了过去七十年取得的巨大进步。从早期的基于规则的系统到今天复杂的大型语言模型，每个时代都对我们理解人类语言做出了重要贡献。随着我们继续开发更先进的模型，NLP的未来承诺将带来更令人兴奋的创新和应用，使我们更接近于真正理解和生成自然语言。

# 7. 结论

在这篇博客中，我们开始了一段旅程，以理解自然语言处理（NLP）的基本原理。我们首先定义了NLP，并探讨了其核心领域：自然语言理解（NLU）和自然语言生成（NLG）。接着，我们深入研究了NLP的各种应用，包括情感分析、机器翻译和命名实体识别，强调了这些任务如何通过不同的技术和模型来解决。

我们还涵盖了基本的NLP概念和术语，如分词、词干提取、词形还原，以及特征提取中诸如词袋模型（BoW）和TF-IDF等特征的作用。此外，我们讨论了先进的技术，如词嵌入和变换器，这些技术通过实现更复杂的语言理解和生成，彻底改变了该领域。

最后，我们探讨了NLP的发展历程，从20世纪50年代的基于规则的系统到今天强大的大型语言模型（LLMs），如GPT-3和GPT-4。这一历史背景提供了对该领域发展历程的全面概述，以及塑造现代NLP的重要里程碑。

通过理解这些基础概念和NLP的发展，读者将能够更好地探索未来讨论和应用中的NLP复杂性。

# 8. 测试你的知识！

## 8.1 基本面试问题

**1. NLP的两个主要领域是什么，它们有什么不同？** — 预期答案：自然语言理解（NLU）专注于解释文本背后的含义，而自然语言生成（NLG）则处理生成类人文本。

**2. 解释词干提取和词形还原之间的区别。在某些NLP任务中，为什么可能更倾向于使用其中一种？** — 预期答案：词干提取通过去除词缀将单词简化为其根形式，这可能导致非单词。另一方面，词形还原将单词简化为其基本或字典形式（词元），通常生成真实单词。当准确性至关重要时，通常更倾向于使用词形还原，而词干提取则更快且计算成本更低。

**3. 在NLP中，分词的目的是什么，分词的不同类型有哪些？** — 预期答案：分词将文本分解为称为标记的小单位，这些单位可以是单词、子词或字符。类型包括单词分词、句子分词和子词分词。

**4. 词频-逆文档频率（TF-IDF）与词袋模型（BoW）有什么不同，它解决了什么问题？** — 预期答案：TF-IDF在BoW的基础上改进，不仅计算单词出现的次数，还根据单词在文档中相对于整个语料库的重要性对单词进行加权。它通过降低常见单词的重要性来解决过度代表的问题。

**5. 什么是词嵌入，它们如何改进传统模型如BoW和TF-IDF？** — 预期答案：词嵌入是单词的密集向量表示，捕捉其含义、句法属性和与其他单词的关系。与将单词视为离散实体的BoW和TF-IDF不同，词嵌入将语义相似的单词放置在连续向量空间中相互靠近，从而保留上下文。

**6. 描述Transformer模型及其在NLP中的重要性。它与之前的模型如RNN有什么不同？** — 预期答案：Transformer模型完全依赖自注意力机制来处理序列数据，而无需按顺序处理，使其比按顺序处理数据的RNN更高效，并能够捕捉长距离依赖关系。

**7. 注意力机制在现代NLP模型中扮演什么角色，它如何提高翻译等任务的性能？** — 预期答案：注意力机制使模型在生成输出的每个部分时能够关注输入序列的特定部分，提供上下文相关的信息，并通过关注相关的单词或短语来提高翻译等任务的性能。

**8. 在大型语言模型的背景下，微调和零样本学习之间有什么区别？** — 预期答案：微调涉及使用额外的训练数据将预训练模型适应于特定任务，而零样本学习指的是模型通过使用一般知识执行未明确训练的任务的能力。

**9. NLP从基于规则的系统演变到大型语言模型（LLMs）如何改变了该领域？** — 预期答案：从基于规则的系统到LLMs的演变通过从手动编码的语言规则转向利用海量数据集和先进架构（如Transformer）的数据驱动方法，改变了NLP，导致模型能够进行更复杂的语言理解和生成。

**10. 在NLP的背景下，语料库是什么，它在模型训练中如何使用？** — 预期答案：语料库是一个大型文档集合，作为NLP模型训练和评估的数据集。它用于分析语言模式并构建统计模型。

## 8.2 高级问题

**1. 在您的NLP模型未能正确分类客户评论情感的情况下，您将采取哪些步骤来诊断和改善模型？** — 预期答案：步骤可能包括分析数据的类别不平衡，审查预处理管道（例如，分词、停用词去除），检查特征提取方法（例如，TF-IDF与词嵌入），探索替代模型（例如，使用变换器而不是简单的逻辑回归），以及在不同的验证集上验证性能以排除过拟合。

**2. 您将如何处理生产NLP系统中的词汇外（OOV）词？** — 预期答案：方法可以包括使用子词分词方法，如字节对编码（BPE）或使用字符级嵌入。另一种方法可能是用特殊标记替换OOV词，并微调模型以更好地处理这些情况。

**3. 想象一下，您被指派为法律文档语料库构建命名实体识别（NER）系统。您可能面临哪些挑战，您将如何解决这些挑战？** — 预期答案：挑战可能包括处理特定领域的语言、长且复杂的句子，以及由于法律影响而需要高准确率。为了解决这些问题，可以使用在法律文本上微调的预训练模型，结合特定领域的嵌入，并应用主动学习等技术来迭代改进模型的性能。

**4. 如果您的聊天机器人对用户查询提供无关的答案，您将如何调试这个问题？** — 预期答案：调试过程可能涉及检查意图识别系统，验证对话流程和上下文管理，确保训练数据多样且具有代表性，以及分析日志以查看对话偏离的地方。您还可以检查模型的置信度分数，以确定问题是出在模型的理解上还是响应生成逻辑上。

**5. 您将如何设计一个NLP系统，以自动生成研究论文的摘要？** 您会考虑哪些因素？ — 预期答案：考虑因素包括选择提取式与抽象式摘要方法，评估摘要的质量和长度，处理特定领域的术语，确保模型捕捉论文的关键贡献，以及解决冗余和连贯性等挑战。您还可以讨论在研究论文摘要数据集上微调预训练模型的重要性。

**6. 假设您正在使用TF-IDF特征的文本分类模型。您将如何结合额外信息，例如文本的情感或作者的人口统计信息，以提高性能？** — 预期答案：一种方法可能涉及为情感分数或作者人口统计信息创建额外特征，并在将它们输入分类器之前与TF-IDF向量连接。另一种方法可能是使用多输入模型，其中一个分支处理TF-IDF特征，另一个分支处理额外信息，在分类之前结合输出。

**7. 您将采用哪些策略来减少为招聘目的设计的NLP系统中的模型偏见？** — 预期答案：策略可能包括仔细选择训练数据以确保多样性，使用对抗去偏见等技术，监控模型的偏见输出，并在训练期间实施公平性约束。定期审计模型的决策以及在敏感决策中引入人类参与者也可能很重要。

**8. 您将如何向没有技术背景的人解释变换器中的注意力机制的概念？** — 预期答案：您可以解释说，注意力机制允许模型关注句子中的重要词，就像一个人在试图理解复杂句子时可能会关注某些词一样。例如，在句子“The cat sat on the mat”中，模型会更加关注“cat”和“mat”以理解主要思想。

**9. 您被要求优化一个在处理稀有词时表现不佳的机器翻译模型的性能。您会提出什么解决方案？** — 预期答案：解决方案可以包括实施子词分词，将稀有词拆分为更常见的子词单元，从一个在大型语料库上预训练的模型进行迁移学习，在包含稀有词的数据集上微调模型，或通过包含这些稀有词的同义句或合成数据来增强数据集。

**10. 在您的NLP模型的预测难以解释的情况下，您将如何改善模型的可解释性？** — 预期答案：方法可以包括使用决策树或逻辑回归等简单模型，在变换器模型中实施注意力可视化以查看模型关注输入的哪些部分，使用LIME或SHAP等技术来解释预测，并结合事后分析来分解模型如何得出特定决策。

## 8.3 DIY

1. NLP的一些实际应用是什么？
2. 我们如何表示词嵌入？
3. 为什么在NLP中需要CNN？
4. 什么是迁移学习？你会采取哪些步骤来执行迁移学习？
5. GPT和BERT系列之间的主要区别是什么？
6. 如果没有像NLTK和SpaCy这样的库，你会如何构建命名实体识别管道？
7. 你将如何决定文本中单词的位置？
8. 相较于其他模型如RNN，transformers有哪些优势？
9. CNN可以进行文本处理吗？为什么不推荐使用CNN而更倾向于使用RNN？
10. 什么是递归神经网络（RNN），它是如何处理NLP中的序列数据的？
11. 抽象摘要和提取摘要之间有什么区别？

