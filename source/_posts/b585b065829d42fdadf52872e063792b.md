
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p2tKvGs8lN_wM7aW6YkGlQ.png
date: '2024-04-04 03:38:41'
tags:
  - 提示压缩
  - 大型语言模型
  - 算法解析
title: 高级RAG 09提示压缩

---


## 方法分类、算法原理与代码解析

RAG 过程可能会遇到两个问题：

* 大型语言模型（LLM）通常有上下文长度限制。因此，输入文本越长，过程越耗时且成本越高。
* 检索到的上下文可能并不总是有用。可能只有较大块的一小部分与答案相关。在某些情况下，可能需要结合多个块来回答特定问题。即使在重新排序的情况下，这个问题依然存在。

LLM 的提示压缩是一种解决这些问题的方法。本质上，目标是保留提示中的关键信息，使输入令牌更有价值。这种方法提高了模型的性能并降低了成本。如图1右下角所示。



值得注意的是，如图1中紫色虚线所示，一些压缩器也可以直接应用于检索到的上下文。

总的来说，提示压缩方法可以分为四大类：

* 基于信息熵的方法，如[Selective Context](https://arxiv.org/pdf/2304.12102.pdf)、[LLMLingua](https://arxiv.org/pdf/2310.05736.pdf)、[LongLLMLingua](https://arxiv.org/pdf/2310.06839.pdf)。这些方法使用小型语言模型计算原始提示中每个令牌的自信息或困惑度，然后删除困惑度较低的令牌。
* 基于软提示调优的方法，如[AutoCompressor](https://arxiv.org/pdf/2305.14788.pdf)和[GIST](https://arxiv.org/pdf/2304.08467.pdf)。这些方法需要对LLM参数进行微调，使其适用于特定领域，但不能直接应用于黑盒LLM。
* 首先从LLM进行数据蒸馏，然后训练模型生成更具解释性的文本摘要。这些可以在不同语言模型之间转移，并应用于不需要梯度更新的黑盒LLM。代表性方法有[LLMLingua-2](https://arxiv.org/pdf/2403.12968.pdf)和[RECOMP](https://arxiv.org/pdf/2310.04408.pdf)。
* 基于令牌合并或令牌剪枝的方法，如[ToMe](https://arxiv.org/pdf/2210.09461.pdf)和[AdapLeR](https://aclanthology.org/2022.acl-long.1.pdf)。这些方法通常需要在推理过程中进行模型微调或生成中间结果。

鉴于第四类方法最初是为ViT或BERT等较小模型提出的，本文将介绍前三类方法中代表性算法的原理。

# 选择性上下文

## 洞察

图2展示了LLMs能够在不需要完整上下文或对话历史的情况下响应用户查询。即使相关信息被省略，LLMs仍能产生预期的回应。这可能归因于LLMs从上下文线索和预训练期间获得的先验知识中推断缺失信息的能力。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sk8NzIk7ucIfet4EYwSKVA.png)

因此，通过过滤掉较少信息含量的内容来优化上下文长度是可能的，且不会影响性能。这是[选择性上下文](https://arxiv.org/pdf/2304.12102.pdf)的关键洞察。

选择性上下文使用小型语言模型（SLM）来确定给定上下文中词汇单元（如句子、短语或标记）的自信息。然后利用这些自信息来评估它们的信息量。通过有选择地保留具有更高自信息的内容，选择性上下文为LLM提供了一个更简洁高效的上下文表示，且在不同任务中不影响其性能。

## 自信息

Selective Context 利用自信息来评估内容质量。

自信息，又称惊异度或信息量，是信息论中的一个关键概念。它量化了一个事件所传递的信息量。自信息定义为该符号出现概率的对数的负值：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8v3y3q8PX6e-7kWJ72oRQ.png)

其中 `**I(x)**` 表示符号 `**x**` 的自信息，`**P(x)**` 表示其输出概率。

在信息论中，自信息量化了与事件相关的惊异程度或不确定性。传递更多信息的罕见事件具有更高的自信息，而传递较少信息的常见事件则具有较低的自信息。

## 算法

为了更方便地解释原理，让我们深入源代码。

首先，通过安装相应的Python库并下载Spacy模型来设置环境。

```python
(base) Florian:~ Florian$ conda create -n "selective_context" python=3.10 
(base) Florian:~ Florian$ conda activate selective_context
(selective_context) Florian:~ Florian$ pip install selective-context
(selective_context) Florian:~ Florian$ python -m spacy download en_core_web_sm
```
安装完成后，版本如下：

```python
(selective_context) Florian:~ Florian$ pip list | grep selective
selective-context   0.1.4
```
测试代码如下：

```python
from selective_context import SelectiveContext

sc = SelectiveContext(model_type='gpt2', lang='en')
text = "INTRODUCTION Continual Learning ( CL ) , also known as Lifelong Learning , is a promising learning paradigm to design models that have to learn how to perform multiple tasks across different environments over their lifetime [To uniform the language and enhance the readability of the paper we adopt the unique term continual learning ( CL ) .]. Ideal CL models in the real world should be deal with domain shifts , researchers have recently started to sample tasks from two different datasets . For instance , proposed to train and evaluate a model on Imagenet first and then challenge its performance on the Places365 dataset . considers more scenarios , starting with Imagenet or Places365 , and then moving on to the VOC/CUB/Scenes datasets. Few works propose more advanced scenarios built on top of more than two datasets."
context, reduced_content = sc(text)

# 我们也可以调整压缩比例
# context_ratio, reduced_content_ratio = sc(text, reduce_ratio = 0.5)
```
初次运行会下载GPT-2模型，大小约为500MB。测试代码的结果如图3所示。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3fz1-1cwPvQgmQVtZ4bSnQ.png)

接下来，让我们探索函数`**sc(text)**`。[内部源代码](https://github.com/liyucheng09/Selective_Context/blob/v0.1.0rc1/src/selective_context/__init__.py#L273)如下：

```python
class SelectiveContext:
    ...
    ...
    def __call__(self, text: str, reduce_ratio: float = 0.35, reduce_level :str = 'phrase') -> List[str]:
        context = self.beautify_context(text)

        self.mask_ratio = reduce_ratio

        sents = [sent.strip() for sent in re.split(self.sent_tokenize_pattern, context) if sent.strip()]

        # 你希望在句子级别、短语级别还是词级别进行压缩？
        assert reduce_level in ['sent', 'phrase', 'token'], f"reduce_level 应该是 ['sent', 'phrase', 'token'] 之一，得到的是 {reduce_level}"
        sent_lus, phrase_lus, token_lus = self._lexical_unit(sents)
        lexical_level = {
            'sent': sent_lus,
            'phrase': phrase_lus,
            'token': token_lus
        }

        # context 是压缩后的上下文，masked_sents 表示被过滤掉的上下文
        context, masked_sents = self.self_info_mask(lexical_level[reduce_level].text, lexical_level[reduce_level].self_info, reduce_level)
        return context, masked_sents
```
上述代码主要涉及三个步骤：

* 计算上下文中每个词的自信息。
* 根据词汇单元（如短语或句子）合并词及其自信息。
* 有选择地保留信息上下文。

**步骤1：计算自信息**

给定上下文`**C = x0, x1, …, xn**`，其中每个`**xi**`代表一个词，我们使用因果语言模型（如GPT-2、OPT和LLaMA）来计算每个词`**xi**`的自信息：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZDEL3w_OoH0fdaAjjZGMmA.png)

如果你使用的是GPT-2，这里是[相应的代码](https://github.com/liyucheng09/Selective_Context/blob/v0.1.0rc1/src/selective_context/__init__.py#L100)：

```python
class SelectiveContext:
    ...
    ...    
    def _get_self_info_via_gpt2(self, text: str) -> Tuple[List[str], List[float]]:
        if self.lang == 'en':
            text = f"<|endoftext|>{text}"
        elif self.lang == 'zh':
            text = f"[CLS]{text}"
        with torch.no_grad():
            encoding = self.tokenizer(text, add_special_tokens=False, return_tensors='pt')
            encoding = encoding.to(self.device)
            outputs = self.model(**encoding)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            self_info = -torch.log(probs)
        
        input_ids = encoding['input_ids']
        input_ids_expaned = input_ids[:, 1:].unsqueeze(-1)
```
**步骤2：合并为词汇单元**

直接在词级别进行选择性上下文过滤可能会导致上下文不连贯。例如，原始提示中的“2009”可能会被压缩为“209”。

因此，除了词级别的过滤外，还需要在短语和句子级别实施过滤过程。过滤的基本单位称为词汇单元，可以是词、短语或句子。

如何计算每个词汇单元`**u = (xt, …, xt+α)**`的自信息？我们可以根据自信息的可加性原则，将构成`**u**`的每个词的自信息相加：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JMDzhofaP8E3YplLvhzRnQ.png)

[相应的代码](https://github.com/liyucheng09/Selective_Context/blob/v0.1.0rc1/src/selective_context/__init__.py#L146)如下，已添加某些变量的调试信息：

```python
class SelectiveContext:
    ...
    ...
    def _lexical_unit(self, sents):

        if self.sent_level_self_info:
            sent_self_info = []
            all_noun_phrases = []
            all_noun_phrases_info = []
            all_tokens = []
            all_token_self_info = []

            for sent in sents:
                # print(sent)
                tokens, self_info = self.get_self_information(sent)
                '''
                ipdb> sent
                'INTRODUCTION Continual Learning ( CL ) , also known as Lifelong Learning , is a promising learning paradigm to design models that have to learn how to perform multiple tasks across different environments over their lifetime [To uniform the language and enhance the readability of the paper we adopt the unique term continual learning ( CL ) .].'

                ipdb> tokens
                ['IN', 'TR', 'ODUCT', 'ION', ' Contin', 'ual', ' Learning', ' (', ' CL', ' )', ',', ' also', ' known', ' as', ' Lif', 'elong', ' Learning', ',', ' is', ' a', ' promising', ' learning', ' paradigm', ' to', ' design', ' models', ' that', ' have', ' to', ' learn', ' how', ' to', ' perform', ' multiple', ' tasks', ' across', ' different', ' environments', ' over', ' their', ' lifetime', ' [', 'To', ' uniform', ' the', ' language', ' and', ' enhance', ' the', ' read', 'ability', ' of', ' the', ' paper', ' we', ' adopt', ' the', ' unique', ' term', ' continual', ' learning', ' (', ' CL', ' )', '.', '].']

                ipdb> self_info
                [7.514791011810303, 1.632637619972229, 0.024813441559672356, 0.006853647995740175, 12.09920597076416, 2.1144468784332275, 9.457701683044434, 2.4503376483917236, 10.236454963684082, 0.8689146041870117, 5.269547939300537, 4.641763210296631, 0.22138957679271698, 0.010370315983891487, 10.071824073791504, 0.6905602216720581, 0.01698811538517475, 1.5882389545440674, 0.4495090842247009, 0.45371606945991516, 6.932497978210449, 6.087430477142334, 3.66465425491333, 3.3969509601593018, 7.337691307067871, 5.881226539611816, 1.7340556383132935, 4.599822521209717, 6.482723236083984, 4.045308589935303, 4.762691497802734, 0.21346867084503174, 3.7985599040985107, 4.6389899253845215, 0.33642446994781494, 4.918881416320801, 2.076707601547241, 3.3553669452667236, 5.5081071853637695, 5.625778675079346, 0.7966060638427734, 6.347291946411133, 12.772034645080566, 13.792041778564453, 4.11267614364624, 6.583715915679932, 3.3618998527526855, 8.434362411499023, 1.2423189878463745, 5.8330583572387695, 0.0013973338063806295, 0.3090735077857971, 1.1139129400253296, 4.160390853881836, 3.744772434234619, 7.2841596603393555, 1.4088190793991089, 7.86871337890625, 4.305004596710205, 9.69282341003418, 0.08665203303098679, 1.6127821207046509, 1.6296097040176392, 0.46206924319267273, 3.0398476123809814, 6.892032623291016]
                '''
                sent_self_info.append(np.mean(self_info))

                all_tokens.extend(tokens)
                all_token_self_info.extend(self_info)

                noun_phrases, noun_phrases_info = self._calculate_lexical_unit(tokens, self_info)
                '''
                ipdb> noun_phrases
                ['INTRODUCTION Continual Learning', ' (', ' CL', ' )', ',', ' also', ' known', ' as', ' Lifelong Learning', ',', ' is', ' a promising learning paradigm', ' to', ' design', ' models', ' that', ' have', ' to', ' learn', ' how', ' to', ' perform', ' multiple tasks', ' across', ' different environments', ' over', ' their lifetime', ' [', 'To', ' uniform', ' the language', ' and', ' enhance', ' the readability', ' of', ' the paper', ' we', ' adopt', ' the unique term continual learning', ' (', ' CL', ' )', '.', ']', '.']
                
                ipdb> noun_phrases_info
                [4.692921464797109, 2.4503376483917236, 10.236454963684082, 0.8689146041870117, 5.269547939300537, 4.641763210296631, 0.22138957679271698, 0.010370315983891487, 3.5931241369495788, 1.5882389545440674, 0.4495090842247009, 4.284574694931507, 3.3969509601593018, 7.337691307067871, 5.881226539611816, 1.7340556383132935, 4.599822521209717, 6.482723236083984, 4.045308589935303, 4.762691497802734, 0.21346867084503174, 3.7985599040985107, 2.487707197666168, 4.918881416320801, 2.7160372734069824, 5.5081071853637695, 3.2111923

在计算出每个词汇单元的自信息后，接下来的问题是：如何评估它们的信息量？本文提出了一种自适应方法，采用基于百分位数的过滤方法来选择最具信息量的内容。相较于使用固定阈值或保留固定数量的前k个词汇单元，这种方法更为可取。

首先，我们根据词汇单元的自信息值对其进行降序排列。然后，计算所有词汇单元自信息值的第p百分位数。接着，我们选择性地保留自信息值大于或等于第p百分位数的词汇单元。

[对应的代码](https://github.com/liyucheng09/Selective_Context/blob/v0.1.0rc1/src/selective_context/__init__.py#L236)如下：

```python
class SelectiveContext:
    ...
    ...

    def self_info_mask(self, sents: List[str], self_info: List[float], mask_level):
        # mask_level: mask sentences, phrases, or tokens
        sents_after_mask = []
        masked_sents = []
                
        self.ppl_threshold = np.nanpercentile(self_info, self.mask_ratio * 100)

        # if title is not None:
        #     with open(os.path.join(self.path, title+'_prob_token.tsv'), 'w', encoding='utf-8') as f:
        #         for token, info in zip(tokens, self_info):
        #             f.write(f"{token}\t{info}\n")
        #     with open(os.path.join(self.path, title+'_prob_sent.tsv'), 'w', encoding='utf-8') as f:
        #         for sent, info in zip(sents, sent_self_info):
        #             f.write(f"{sent}\n{info}\n\n")

        for sent, info in zip(sents, self_info):
            if info < self.ppl_threshold:
                masked_sents.append(sent)
                sents_after_mask.append(self.mask_a_sent(sent, mask_level))
            else:
                sents_after_mask.append(sent)
        masked_context = " ".join(sents_after_mask) if mask_level == 'sent' else "".join(sents_after_mask)
        
        return masked_context, masked_sents
```

# LLMLingua

## 概述

[LLMLingua](https://arxiv.org/pdf/2310.05736.pdf) 指出，[Selective Context](https://arxiv.org/pdf/2304.12102.pdf) 往往忽视了压缩内容之间的相互联系以及大语言模型与用于提示压缩的小语言模型之间的关联。LLMLingua 精准地解决了这些问题。

具体而言，如图4所示，LLMLingua 采用预算控制器，动态为原始提示中的不同组成部分（如指令、示例和问题）分配不同的压缩比率。它还执行粗粒度的示例级压缩，即使在高度压缩的情况下也能保持语义完整性。此外，LLMLingua 引入了一种针对细粒度提示压缩的令牌级迭代算法。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pfi62OMNdujoehaaE2RBeA.png)

与 Selective Context 相比，LLMLingua 在考虑令牌间条件依赖的同时，能更有效地保留提示中的关键信息。它可以将提示压缩至原来的二十分之一。

## 预算控制器

预算控制器是LLMLingua的关键组件，用于动态地为原始提示中的不同部分分配不同的压缩比率。

提示中的不同部分对压缩的敏感度各异。例如，指令和问题对压缩更为敏感，而示例则相对不敏感。预算控制器的作用是为指令和问题分配较低的压缩比率，从而保留关键信息。相反，可以为示例分配较高的压缩比率，以消除冗余信息。

预算控制器的算法如图5所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6N_uaN8oMXcwxyV4XOrBBQ.png)

主要变量包括：

* `**M𝑠**`：小型语言模型，如GPT-2或LLaMA。
* `**x = (x^ins , x^dems , x^que)**`：包含指令、示例和问题的原始提示。
* `**𝐿**`, `**𝐿_ins**`, `**𝐿_dems**`, 和 `**𝐿_que**` 表示 `**x**`, `**x^ins**`, `**x^dems**`, 和 `**x^que**` 中的token数量。
* `**𝜏_dems**`：根据目标总体压缩率 `**𝜏**` 和预定义的指令与问题的压缩率 `**𝜏_ins**` 和 `**𝜏_que**` 计算出的示例压缩率。
* `**D**`：该集合将包含压缩后的示例。

主要流程如下：

1. 计算示例的压缩率
2. 使用小型语言模型（如GPT-2或LLaMA）计算原始示例集中每个示例的困惑度。
3. 按困惑度降序排列所有示例。
4. 迭代选择示例并将其添加到集合 `**D**` 中。
5. 压缩示例后，将剩余预算分配给指令和问题。
6. 输出经过粗粒度压缩后的集合D。

通过示例级别的处理，预算控制器能够在压缩过程中保留关键信息，有效减小原始提示的大小。这种方法特别适用于包含多个示例的提示。

相关代码位于函数 [control\_context\_budget](https://github.com/microsoft/LLMLingua/blob/v0.2.1/llmlingua/prompt_compressor.py#L1108) 中。

## 迭代式词级别提示压缩（ITPC）

使用困惑度进行提示压缩存在一个固有限制：独立性假设。这一假设将提示中的每个词视为独立的。换句话说，一个词出现的概率仅取决于前一个词，与其他词无关。

这一假设的问题在于，它忽略了自然语言中词与词之间常常存在的复杂依赖关系，而这些依赖关系对于理解上下文和保持语义完整性至关重要。

这种忽视可能导致压缩过程中关键信息的丢失。例如，在高比例压缩中，如果一个词在上下文中提供了关键的推理步骤或逻辑联系，仅根据其困惑度决定是否保留该词可能导致推理过程不完整。

为了解决这一问题，LLMLingua 引入了迭代式词级别提示压缩（ITPC）算法。该方法不仅依赖于词的独立概率，而是在提示压缩过程中更精确地评估每个词的重要性。它通过迭代处理提示中的每个片段，并考虑每个词在当前上下文中的条件概率，从而更好地保持词与词之间的依赖关系。

图 6 展示了 ITPC 的详细步骤：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPd5eb9VRuvUI1C0zzHL2g.png)

通过这一过程，ITPC 算法能够有效压缩提示长度，同时保持提示语义的完整性，从而降低 LLM 的推理成本。

相关代码位于函数 [iterative\_compress\_prompt](https://github.com/microsoft/LLMLingua/blob/v0.2.1/llmlingua/prompt_compressor.py#L1458) 中。

## 指令调优

图4展示了指令调优在LLMLingua中也是一个关键步骤。其目的是缩小用于压缩提示的小型语言模型与大型语言模型（LLM）之间的分布差异。

图7展示了指令调优的步骤：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eXUxnF06Shv75zuthXmTLw.png)

## 代码演示

现在开始进行代码演示。首先，设置环境

```python
(base) Florian:~ Florian$ conda create -n "llmlingua" python=3.11

(base) Florian:~ Florian$ conda activate llmlingua

(llmlingua) Florian:~ Florian$ pip install llmlingua
```
安装的版本如下：

```python
llmlingua          0.2.1
```
测试代码如下：

```python
from llmlingua import PromptCompressor

GSM8K_PROMPT = "Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\nLet's think step by step\nAngelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\nThey will need to plan to study 4 days to allow for all the time they need.\nThe answer is 4\n\nQuestion: You can buy 4 apples or 1 watermelon for the same price. You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50. How much does 1 apple cost if your total bill was $66?\nLet's think step by step\nIf 36 fruits were evenly split between 3 types of fruits, then I bought 36/3 = 12 units of each fruit\nIf 1 orange costs $0.50 then 12 oranges will cost $0.50 * 12 = $6\nIf my total bill was $66 and I spent $6 on oranges then I spent $66 - $6 = $60 on the other 2 fruit types.\nAssuming the price of watermelon is W, and knowing that you can buy 4 apples for the same price and that the price of one apple is A, then 1W=4A\nIf we know we bought 12 watermelons and 12 apples for $60, then we know that $60 = 12W + 12A\nKnowing that 1W=4A, then we can convert the above to $60 = 12(4A) + 12A\n$60 = 48A + 12A\n$60 = 60A\nThen we know the price of one apple (A) is $60/60= $1\nThe answer is 1\n\nQuestion: Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.  At the start of the school year, Susy had 100 social media followers.  She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.  Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.  After three weeks, how many social media followers did the girl with the most total followers have?\nLet's think step by step\nAfter one week, Susy has 100+40 = 140 followers.\nIn the second week, Susy gains 40/2 = 20 new followers.\nIn the third week, Susy gains 20/2 = 10 new followers.\nIn total, Susy finishes the three weeks with 140+20+10 = 170 total followers.\nAfter one week, Sarah has 50+90 = 140 followers.\nAfter the second week, Sarah gains 90/3 = 30 followers.\nAfter the third week, Sarah gains 30/3 = 10 followers.\nSo, Sarah finishes the three weeks with 140+30+10 = 180 total followers.\nThus, Sarah is the girl with the most total followers with a total of 180.\nThe answer is 180"

llm_lingua = PromptCompressor()

## 或者使用 phi-2 模型，
# llm_lingua = PromptCompressor("microsoft/phi-2")

## 或者使用量化模型，如 TheBloke/Llama-2-7b-Chat-GPTQ，仅需 <8GB GPU 内存。
## 在此之前，需要 pip install optimum auto-gptq
# llm_lingua = PromptCompressor("TheBloke/Llama-2-7b-Chat-GPTQ", model_config={"revision": "main"})

compressed_prompt = llm_lingua.compress_prompt(GSM8K_PROMPT.split("\n\n")[0], instruction="", question="", target_token=200)

print('-' * 100)
print("original:")
print(GSM8K_PROMPT.split("\n\n")[0])

print('-' * 100)
print("compressed_prompt:")
print(compressed_prompt)
```
默认模型将在首次运行时下载。或者，我们可以选择使用量化模型。运行结果如图8所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*X4_g1T6LuQf-TfE89i47Fg.png)

# LongLLMLingua

LLMLingua的问题在于，它在压缩过程中没有考虑用户的问题，可能会保留不相关的信息。

[LongLLMLingua](https://arxiv.org/pdf/2310.06839.pdf)旨在通过将用户问题纳入压缩过程来解决这一问题。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iGcNYTLKUONd0CSR1iBazw.png)

如图9所示，LongLLMLingua提出了四个新组件，以增强LLMs对关键信息的感知：

* 问题感知的粗粒度和细粒度压缩
* 文档重排机制
* 动态压缩比
* 子序列恢复算法

## 问题感知粗粒度压缩

LongLLMLingua 提出使用问题 `**x^que**` 在不同上下文 `**x^doc_k**` 下的困惑度来表示它们的关联。可以在 `**x^que**` 后添加一个限制性陈述 `**x^restrict = "我们可以在给定文档中找到这个问题的答案"**`。这个陈述加强了 `**x^que**` 和 `**x^doc_k**` 之间的联系，并作为一个正则化项，减少幻觉效应。这可以表示为：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8-qgQYdMU2mHj94bqO6JhQ.png)

为什么不计算在问题 `**x^que**` 条件下的文档级困惑度？这是因为文档通常包含大量无关信息。即使受到 `**x^que**` 的限制，为整个文档计算的困惑度分数可能仍然不够明显，使其不足以作为文档级压缩的衡量标准。

相关代码可以在函数 [get\_distance\_longllmlingua](https://github.com/microsoft/LLMLingua/blob/v0.2.1/llmlingua/prompt_compressor.py#L1967) 中找到。

## 问题感知细粒度压缩

LongLLMLingua 引入了对比困惑度的概念。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-QCv7zhUJ6bV1pe7L31qgQ.png)

首先，我们计算一个标记在不考虑问题时的困惑度，表示为 `**perplexity(x_i | x<i)**`。然后，我们再次测量困惑度，这次包括问题，表示为 `**perplexity(x_i | x^que, x<i)**`。这衡量了在给定问题 `**x^que**` 的情况下，看到标记 `**x_i**` 之前的所有标记的惊讶程度。

目标是确定每个标记的惊讶程度在考虑问题时变化的程度。如果一个词在包含问题时变得不那么令人惊讶，它可能与问题高度相关。

## 文档重排机制

如图10所示，在推理过程中，LLM倾向于使用提示的开头和结尾内容，而忽略中间部分。这一问题被称为“中间迷失”问题。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sv1VNkLFO6qRulP8BoAVwg.png)

图10还表明，当相关信息置于开头时，LLM表现最佳。因此，LongLLMLingua根据粗粒度压缩的结果，将段落按分数从高到低从前到后排列。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2NHtX6CMSZRG-OL7P0fMFA.png)

## 动态压缩比

由于关键信息密度在不同文档中有所差异，我们应为与问题更相关的文档分配更多预算（即更低的压缩比）。

LongLLMLingua 利用粗粒度压缩中的重要性分数来指导细粒度压缩阶段的预算分配。

具体而言，首先使用 LLMLingua 的预算控制器为保留的文档设置初始预算。然后在细粒度压缩阶段，根据文档的重要性分数排名指数动态分配压缩预算，该排名指数在粗粒度压缩阶段确定。

LongLLMLingua 采用线性调度器进行自适应分配，每个令牌 `**xi**` 的预算可以表示为：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Qutsl9kbsM2c1WNNL8r1DQ.png)

其中 `**Nd**` 表示文档数量，`**δτ**` 是一个控制动态分配总预算的超参数。

相应的代码可以在函数 [get\_dynamic\_compression\_ratio](https://github.com/microsoft/LLMLingua/blob/v0.2.1/llmlingua/prompt_compressor.py#L958) 中找到。

## 子序列恢复算法

如图11所示，在细粒度的逐标记压缩过程中，关键实体的某些标记可能会被丢弃。例如，原始提示中的“2009”可能会被压缩为“209”，“Wilhelm Conrad Rontgen”可能会被压缩为“Wilhelmgen”。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KS9zzP0tL87QTGCgXa53OA.png)

LongLLMLingua提出了一种子序列恢复算法，可以从LLMs的响应中恢复原始内容，如图12所示。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_YtXhHr7lcvcwU1UADHACw.png)

主要过程包括以下步骤：

* 遍历LLMs响应中的标记`**yl**`，并选择在压缩提示`**x˜**`中出现的最长子串`**y˜key,l**`
* 在原始提示`**x**`中找到与`**y˜key,l**`对应的最大公共最短子序列`**xi,j**`
* 将LLMs响应中对应的标记`**y˜key,l**`替换为`**xi,j**`。

相应的代码可以在函数[recover](https://github.com/microsoft/LLMLingua/blob/v0.2.1/llmlingua/prompt_compressor.py#L1686)中找到。

## 代码演示

设置环境的步骤与LLMLingua相同。以下是测试代码：

```python
from llmlingua import PromptCompressor

GSM8K_PROMPT = "Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\nLet's think step by step\nAngelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\nThey will need to plan to study 4 days to allow for all the time they need.\nThe answer is 4\n\nQuestion: You can buy 4 apples or 1 watermelon for the same price. You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50. How much does 1 apple cost if your total bill was $66?\nLet's think step by step\nIf 36 fruits were evenly split between 3 types of fruits, then I bought 36/3 = 12 units of each fruit\nIf 1 orange costs $0.50 then 12 oranges will cost $0.50 * 12 = $6\nIf my total bill was $66 and I spent $6 on oranges then I spent $66 - $6 = $60 on the other 2 fruit types.\nAssuming the price of watermelon is W, and knowing that you can buy 4 apples for the same price and that the price of one apple is A, then 1W=4A\nIf we know we bought 12 watermelons and 12 apples for $60, then we know that $60 = 12W + 12A\nKnowing that 1W=4A, then we can convert the above to $60 = 12(4A) + 12A\n$60 = 48A + 12A\n$60 = 60A\nThen we know the price of one apple (A) is $60/60= $1\nThe answer is 1\n\nQuestion: Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.  At the start of the school year, Susy had 100 social media followers.  She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.  Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.  After three weeks, how many social media followers did the girl with the most total followers have?\nLet's think step by step\nAfter one week, Susy has 100+40 = 140 followers.\nIn the second week, Susy gains 40/2 = 20 new followers.\nIn the third week, Susy gains 20/2 = 10 new followers.\nIn total, Susy finishes the three weeks with 140+20+10 = 170 total followers.\nAfter one week, Sarah has 50+90 = 140 followers.\nAfter the second week, Sarah gains 90/3 = 30 followers.\nAfter the third week, Sarah gains 30/3 = 10 followers.\nSo, Sarah finishes the three weeks with 140+30+10 = 180 total followers.\nThus, Sarah is the girl with the most total followers with a total of 180.\nThe answer is 180"
QUESTION = "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?"

llm_lingua = PromptCompressor()

compressed_prompt = llm_lingua.compress_prompt(
    GSM8K_PROMPT.split("\n\n")[0],
    question = QUESTION,
    # ratio=0.55
    # Set the special parameter for LongLLMLingua
    condition_in_question = "after_condition",
    reorder_context = "sort",
    dynamic_context_compression_ratio = 0.3, # or 0.4
    condition_compare = True,
    context_budget = "+100",
    rank_method = "longllmlingua",
)

print('-' * 100)
print("original:")
print(GSM8K_PROMPT.split("\n\n")[0])

print('-' * 100)
print("compressed_prompt:")
print(compressed_prompt)
```
运行结果如图13所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IOq6MyzWNxnRHc96uSbrmg.png)

# AutoCompressor

与前述方法不同，[AutoCompressor](https://arxiv.org/pdf/2305.14788.pdf) 是一种基于软提示的方法。

它通过扩展词汇表并利用“摘要标记”和“摘要向量”来浓缩上下文信息，智能地微调现有模型。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DpoasvCpdAi1gcYpgIVnJg.png)

图14展示了AutoCompressor的架构，其操作步骤如下：

1. **扩展词汇表：** 此步骤涉及向模型的现有词汇中添加“摘要标记”。这些标记使模型能够将大量信息浓缩成较小的向量。
2. **分割文档：** 待处理的文档被分割成小片段，每个片段都附加有摘要标记。这些标记还携带了前序片段的摘要信息，形成摘要累积。
3. **微调训练：** 采用无监督训练方法，利用“下一个词预测”任务来微调模型。该任务的目标是根据当前标记之前的标记和当前片段之前片段的摘要向量来预测下一个词。
4. **反向传播：** AutoCompressor 通过时间反向传播（BPTT）和梯度检查点对每个片段进行操作，以减小计算图的大小。对整个文档进行反向传播，使模型能够学习到完整上下文的关联。

## 代码

AutoCompressor 提供了 [代码](https://github.com/princeton-nlp/AutoCompressors)，感兴趣的读者可以尝试。

```python
import torch
from transformers import AutoTokenizer
from auto_compressor import LlamaAutoCompressorModel, AutoCompressorModel

# 通过压缩 6k 个 token 并进行 4 次压缩步骤训练的 AutoCompressor
tokenizer = AutoTokenizer.from_pretrained("princeton-nlp/AutoCompressor-Llama-2-7b-6k")
# 需要 bfloat16 + cuda 来运行带有 flash attention 的 Llama 模型
model = LlamaAutoCompressorModel.from_pretrained("princeton-nlp/AutoCompressor-Llama-2-7b-6k", torch_dtype=torch.bfloat16).eval().cuda()

prompt = 'The first name of the current US president is "'
prompt_tokens = tokenizer(prompt, add_special_tokens=False, return_tensors="pt").input_ids.cuda()

context = """Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\nBiden's early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons' hospital bedsides.\nHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\nIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\nAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\nIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\nAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\nThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans."""
context_tokens = tokenizer(context, add_special_tokens=False, return_tensors="pt").input_ids.cuda()

summary_vectors = model(context_tokens, output_softprompt=True).softprompt
print(f"压缩 {context_tokens.size(1)} 个 token 到 {summary_vectors.size(1)} 个摘要向量")
# >>> 压缩 660 个 token 到 50 个摘要向量

generation_with_summary_vecs = model.generate(prompt_tokens, do_sample=False, softprompt=summary_vectors, max_new_tokens=12)[0]
print("使用摘要向量的生成结果:\n" + tokenizer.decode(generation_with_summary_vecs))
# >>> The first name of the current US president is "Joe" and the last name is "Biden".

next_tokens_without_context = model.generate(prompt_tokens, do_sample=False, max_new_tokens=11)[0]
print("无上下文的生成结果:\n" + tokenizer.decode(next_tokens_without_context))
# >>> The first name of the current US president is "Donald" and the last name is "Trump".
```

# LLMLingua-2

[LLMLingua-2](https://arxiv.org/pdf/2403.12968.pdf) 指出了通过删除基于因果语言模型（如LLaMa-7B）的信息熵的令牌或词汇单元来压缩提示的两个问题：

(1) 用于确定信息熵的小型语言模型与提示压缩目标不一致。

(2) 它仅利用单向上下文，可能无法涵盖提示压缩所需的所有信息。

这些问题的核心在于，信息熵可能是压缩的一个次优度量标准。

LLMLingua-2的整体架构如图15所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*UnRL6hNWbUzjz3ak4zY-sA.png)

为了解决第一个问题，LLMLingua-2引入了数据蒸馏过程。该过程从大型语言模型（LLM）中提取知识，以压缩提示而不丢失关键信息，同时构建一个抽取式文本压缩数据集。在此数据集上进行训练有助于有效对齐小型语言模型与提示压缩。

为了应对第二个问题，LLMLingua-2将提示压缩视为一个令牌分类问题。这种方法确保了压缩后的提示与原始提示的忠实度。它采用transformer编码器作为底层架构，从完整的双向上下文中捕获提示压缩所需的所有必要信息。

## 如何构建有效的提示压缩数据集

**数据蒸馏**

数据蒸馏涉及从大型语言模型（如GPT-4）中提取知识，以有效压缩提示而不丢失关键信息。

在LLMLingua-2中，指令经过精心设计，如图16所示。这些指令要求GPT-4通过从原始文本中省略非关键单词来压缩文本，同时在生成过程中不添加任何新词。

同时，指令并未设定压缩比率限制。相反，GPT-4被提示尽可能多地压缩原始文本，同时保留最大信息量。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lyYuG-e4T8X6vUa7cmxnMA.png)

如图17所示，GPT-4在处理极长上下文时通常采用高压缩比。这可能是由于其处理长上下文的能力有限。这种激进的压缩导致大量信息丢失，严重影响后续任务的性能。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cBVWxs358VewNiO081Kxog.png)

为了缓解这一问题，LLMLingua-2采用了分块压缩方法，将长文本分割成不超过512个令牌的多个块，然后引导GPT-4分别压缩每个块。

**数据标注**

我们目前已通过数据蒸馏获得了原始文本及其压缩版本对。数据标注的目标是为原始文本中的每个令牌分配一个二进制标签，确定该令牌在压缩后是否应保留。

由于GPT-4可能无法准确遵循指令，LLMLingua-2采用滑动窗口技术限制搜索范围，并利用模糊匹配处理GPT-4在压缩过程中可能对原始词的改动。

**质量控制**

LLMLingua-2使用两种质量控制指标来评估GPT-4蒸馏生成的压缩文本和自动标注标签的质量：变异率（VR）和对齐差距（AG）。

变异率衡量压缩文本中与原始文本不同的单词百分比。对齐差距评估自动标注标签的质量。

通过这些指标，LLMLingua-2可以排除低质量样本，确保数据集的质量。

## 压缩器

**视为二分类问题**

起初，提示压缩问题可以转化为一个二分类问题。其基本思路是将每个词汇单元视为独立实体，并为其分配一个标签，即“保留”或“丢弃”。这种方法在简化模型设计的同时，保持了压缩后提示内容的完整性。

**模型架构**

采用基于Transformer编码器的特征编码器，并在其上添加一个线性分类层。

该架构能够捕捉每个词汇单元的双向上下文信息，为压缩任务提供必要的信息。

**压缩策略**

原始提示`**x**`的压缩策略是一个三步过程。目标压缩比为`**1/τ**`，其中`**τ**`定义为压缩后提示与原始提示`**x**`中单词数量的商。

* 首先，我们确定在压缩后提示`**x˜**`中要保留的目标令牌数量：`**N˜ = τN**`。
* 然后，我们使用令牌分类模型预测每个单词`**xi**`被标记为`**‘保留’**`的概率`**pi**`。
* 最后，我们从原始提示`**x**`中保留具有最高`**pi**`值的前`**N˜**`个单词，保持其原有顺序，形成压缩后的提示`**x˜**`。

## 代码

从上述内容可以清楚地看出，LLMLingua-2 的主要工作涉及构建压缩器。那么，一旦获得压缩器，我们该如何使用呢？

请参考以下代码（设置环境的方法与 LLMLingua 相同）。主要内部流程可以在函数 [compress\_prompt\_llmlingua2](https://github.com/microsoft/LLMLingua/blob/v0.2.1/llmlingua/prompt_compressor.py#L661) 中查看。

```python
from llmlingua import PromptCompressor

PROMPT = "John: So, um, I've been thinking about the project, you know, and I believe we need to, uh, make some changes. I mean, we want the project to succeed, right? So, like, I think we should consider maybe revising the timeline.\n\nSarah: I totally agree, John. I mean, we have to be realistic, you know. The timeline is, like, too tight. You know what I mean? We should definitely extend it."

llm_lingua = PromptCompressor(
    model_name = "microsoft/llmlingua-2-xlm-roberta-large-meetingbank",
    use_llmlingua2 = True,
)
compressed_prompt = llm_lingua.compress_prompt(PROMPT, rate=0.33, force_tokens = ['\n', '?'])

## 或者使用 LLMLingua-2-small 模型
# llm_lingua = PromptCompressor(
#     model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
#     use_llmlingua2=True,
# )

print('-' * 100)
print("原始文本:")
print(PROMPT)

print('-' * 100)
print("压缩后的文本:")
print(compressed_prompt)
```
运行结果如图 18 所示：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r14tLR-PifG6pAQ_kWc0cg.png)

# RECOMP

[RECOMP](https://arxiv.org/pdf/2310.04408.pdf) 引入了两种训练有素的压缩器：抽取式和抽象式。抽取式压缩器从检索到的文档中选择有用的句子，而抽象式压缩器则结合多个文档的信息来生成摘要。

图19展示了压缩器在RECOMP中的位置。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tgba2CPf-XdcHUNXAi3RtQ.png)

## 抽取式压缩器

给定输入文档集中的 `**n**` 个句子 `**[s1, s2, …, sn]**`，我们训练一个双编码器模型。该模型将句子 `**si**` 和输入序列 `**x**` 嵌入到固定维度的嵌入中。这些嵌入的内积表示将 si 添加到输入 x 中以生成目标输出序列对大型语言模型（LLM）的益处。

压缩器生成的最终摘要 `**s**` 由与输入内积最高的 `**N**` 个句子组成，按其内积排序。

## 抽象压缩器

抽象压缩器是一种编码器-解码器模型。它接受输入序列 `**x**` 和检索到的文档集合的连接，并输出一个摘要 `**s**`。

该方法涉及使用大型语言模型（如GPT-3）生成训练数据集，过滤这些数据，然后使用过滤后的数据集训练编码器-解码器模型。

## 代码

由于[RECOMP的代码](https://github.com/carriex/recomp)目前尚处于早期阶段，此处不予展示。感兴趣的读者可自行尝试。

# 结论

本文介绍了提示压缩的方法，包括方法分类、算法原理及代码解析。

在讨论的方法中，LongLLMLingua 可能是一个较优的选择。我们已在研究项目中实施了它。若我发现 LongLLMLingua 的缺陷或找到更好的方法，将更新此文。此外，LLMLingua-2 亦可尝试，它在速度和内存使用上具有优势。

若对 RAG 技术感兴趣，欢迎查阅我的其他文章。

最后，若本文存在错误或遗漏，或有任何疑问，请在评论区指出。
