
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bo0JwTdru5quxDiPFa1TvA.png
date: '2024-01-11 12:16:45'
tags:
  - 检索增强生成
  - 大型语言模型
  - 自然语言处理
title: 检索增强生成RAG简述

---


## 引入概念与图示

检索增强生成（Retrieval Augmented Generation，简称RAG）[1] **于2020年首次提出**，作为一种端到端的方法，它结合了预训练的检索器和预训练的生成器。当时，其主要目标是通过模型微调来提升性能。

**2022年12月ChatGPT的发布标志着RAG的一个重要转折点**。自那时起，RAG更多地侧重于利用大型语言模型（LLM）的推理能力，通过整合外部知识来实现更优的生成效果。

RAG技术消除了开发者为每个特定任务重新训练整个大规模模型的需求。相反，他们只需连接相关知识库，为模型提供额外输入，从而提高答案的准确性。

**本文简要介绍了RAG的概念、目的及其特点。**

# 什么是检索增强生成（RAG）？

检索增强生成（Retrieval Augmented Generation，简称RAG）[1] 是一种通过整合外部知识源的额外信息来增强大型语言模型（LLMs）的过程。这使得LLMs能够生成更准确、更具上下文感知能力的答案，同时减少幻觉现象。

在回答问题或生成文本时，首先从现有知识库或大量文档中检索相关信息。然后，利用LLM生成答案，通过结合这些检索到的信息来提高回答质量，而不是仅依赖LLM自身生成答案。

RAG的典型工作流程如图1所示。



如图1所示，RAG主要包括以下步骤：

1. **索引（Indexing）**：索引过程是一个关键的初始步骤，通常离线进行。它首先对原始数据进行清洗和提取，将PDF、HTML和Word等多种文件格式转换为标准化的纯文本。为了适应语言模型的上下文限制，这些文本被分割成更小、更易管理的数据块，这一过程称为分块（chunking）。然后，这些数据块通过嵌入模型转换为向量表示。最后，创建一个索引，将这些文本块及其向量嵌入作为键值对存储，从而实现高效且可扩展的搜索能力。
2. **检索（Retrieval）**：利用用户查询从外部知识源检索相关上下文。为此，用户查询通过编码模型处理，生成语义相关的嵌入。然后，在向量数据库上进行相似性搜索，检索出最接近的前k个数据对象。
3. **生成（Generation）**：将用户查询和检索到的额外上下文填充到提示模板中。最后，从检索步骤得到的增强提示输入到LLM中。

# 为什么我们需要RAG？

既然已经有了LLM，为什么我们还需要RAG？原因很简单：LLM无法解决RAG能解决的问题。这些问题包括：

* **模型幻觉问题**：LLM的文本生成基于概率。在没有足够事实支撑的情况下，可能会生成看似严肃但缺乏连贯性的内容。
* **时效性问题**：LLM的参数规模越大，训练成本越高，所需时间越长。因此，时效性强的数据可能无法及时纳入训练，导致模型无法直接回答时效性问题。
* **数据安全问题**：通用LLM无法访问企业内部或用户私有数据。为了确保数据安全同时使用LLM，一个好的解决方案是将数据本地存储并在本地进行所有数据计算。云端LLM仅用于信息汇总。
* **答案约束问题**：RAG为LLM生成提供了更多控制。例如，当一个问题涉及多个知识点时，通过RAG检索的线索可以用来限制LLM生成的边界。

# RAG 的特性是什么？

RAG 具备以下特性，使其能够有效解决上述问题：

(1) 可扩展性：RAG 减少了模型规模和训练成本，并促进了知识的快速扩展。

(2) 准确性：该模型基于事实提供答案，最大限度地减少了幻觉现象的发生。

(3) 可控性：RAG 允许进行知识更新和定制。

(4) 可解释性：检索到的相关信息作为模型预测的参考。

(5) 多功能性：RAG 可以针对问答、摘要、对话等各种任务进行微调和定制。

# 结论

在图像方面，RAG 可以比作 LLM 的开卷考试。类似于开卷考试，学生被允许携带参考资料，以便在回答问题时查阅相关信息。

本文仅对 RAG 的基础知识进行了简要介绍。未来将介绍更多高级 RAG 技术。

最后，若本文存在任何错误或遗漏，敬请不吝指正。

# 参考文献

[1]: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin 等。[知识密集型NLP任务的检索增强生成](https://arxiv.org/pdf/2005.11401.pdf)。arXiv预印本arXiv:2005.11401，2023年。

# PlainEnglish.io 🚀

*感谢您成为In Plain English社区的一员！在您离开之前：*

* *请务必**点赞**并**关注**作者***️**
* *了解如何[**为In Plain English撰写文章**](https://plainenglish.io/blog/how-to-write-for-in-plain-english)️*
* *关注我们：[**X**](https://twitter.com/inPlainEngHQ) **| [LinkedIn](https://www.linkedin.com/company/inplainenglish/) | [YouTube](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) | [Discord](https://discord.gg/in-plain-english-709094664682340443) | [Newsletter](https://newsletter.plainenglish.io/)***
* *访问我们的其他平台：[**Stackademic**](https://stackademic.com/) **| [CoFeed](https://cofeed.app/) | [Venture](https://venturemagazine.net/)***
