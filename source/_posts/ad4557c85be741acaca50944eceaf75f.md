
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iaiGPR-OEeuUM6kguTEtpg.gif
date: '2024-08-01 21:24:14'
tags:
  - å¤šæ¨¡æ€RAG
  - è§†è§‰é—®ç­”
  - GPT-4o
title: æ„å»ºä¸€ä¸ªå¤šæ¨¡æ€RAGç³»ç»Ÿç”¨äºè§†è§‰é—®ç­”

---


## ä½¿ç”¨LangChainå’ŒGPT-4oæ„å»ºå…·æœ‰è§†è§‰é—®ç­”èƒ½åŠ›çš„å¤šæ¨¡æ€RAGèŠå¤©æœºå™¨äººã€‚



# æ¦‚è¿°

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†æŒ‡å¯¼æ‚¨æ„å»ºä¸€ä¸ªä½¿ç”¨ OpenAI çš„ GPT-4o æ¨¡å‹çš„å¤šæ¨¡æ€ RAG èŠå¤©åº”ç”¨ç¨‹åºã€‚æ‚¨å°†å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š

* **å¤šæ¨¡æ€ RAG èŠå¤©åº”ç”¨ç¨‹åº**ï¼šåˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œé€šè¿‡ä» PDF æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯æ¥å®ç°è§†è§‰é—®ç­”ã€‚
* **æ— ç¼è§£æ**ï¼šä½¿ç”¨ Unstructured åº“æ— ç¼è§£ææ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒã€‚
* **æ€§èƒ½è¯„ä¼°**ï¼šä½¿ç”¨ DeepEval åº“æä¾›çš„å„ç§æŒ‡æ ‡è¯„ä¼°èŠå¤©æœºå™¨äººçš„æ€§èƒ½ã€‚
* **Streamlit UI**ï¼šé€šè¿‡ Streamlit åº”ç”¨ç¨‹åºæ¼”ç¤ºè¯¥åº”ç”¨ç¨‹åºã€‚

# ä¸ºä»€ä¹ˆè¦é˜…è¯»è¿™ä¸ªï¼Ÿ

ä½ æ˜¯å¦æœ‰å…´è¶£åˆ©ç”¨åƒ GPT-4o è¿™æ ·çš„å…ˆè¿›åŸºç¡€æ¨¡å‹çš„å¤šæ¨¡æ€èƒ½åŠ›æ¥æ„å»ºè‡ªå·±çš„ AI åº”ç”¨ç¨‹åºï¼Ÿé‚£ä¹ˆä½ æ¥å¯¹åœ°æ–¹äº†ï¼

æ— è®ºä½ æ˜¯å¯»æ±‚å¸‚åœºç ”ç©¶æŠ¥å‘Šè§è§£çš„å¸‚åœºè¥é”€ä¸“ä¸šäººå£«ï¼Œåˆ†æå¤šæ¨¡æ€åŒ»ç–—æ–‡ä»¶çš„åŒ»ç–—ä»ä¸šè€…ï¼Œè¿˜æ˜¯å¤„ç†å¤æ‚æ³•å¾‹æ–‡ä»¶çš„æ³•å¾‹ä¸“ä¸šäººå£«ï¼Œè¿™ç¯‡æ–‡ç« éƒ½ä¸ºä½ æä¾›äº†å®è´µçš„è§è§£ã€‚

æˆ‘å°†è¯¦ç»†è§£é‡Šæ¯ä¸ªæ¦‚å¿µï¼Œå¹¶æä¾›æ‰€æœ‰ä»£ç çš„è¯¦ç»†è¯´æ˜ã€‚è¯è™½å¦‚æ­¤ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼ ğŸ¬

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WaE-c0xa8MrmgnGXO5jlrA.png)

# å¤šæ¨¡æ€ RAG çš„å´›èµ·

ä»åŸºäºæ–‡æœ¬çš„ RAG æ¨¡å‹è¿‡æ¸¡åˆ°å¤šæ¨¡æ€ RAG ç³»ç»Ÿæ ‡å¿—ç€ AI èƒ½åŠ›çš„é‡å¤§é£è·ƒã€‚ä»¥ä¸‹æ˜¯å¿«é€Ÿæ¦‚è¿°ï¼š

* **èµ·æº**ï¼šRAG è¿™ä¸ªæœ¯è¯­æ˜¯åœ¨ 2021 å¹´ 4 æœˆæå‡ºçš„ï¼Œé€šè¿‡åŸºäºæ–‡æœ¬çš„çŸ¥è¯†å¢å¼ºè¯­è¨€è¾“å‡ºã€‚
* **è¿›å±•**ï¼šéšç€ 2024 å¹´ 5 æœˆå‘å¸ƒçš„ GPT-4o ç­‰æ¨¡å‹ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ•´åˆè§†è§‰ä¿¡æ¯ï¼Œå…è®¸åŒæ—¶å¤„ç†å›¾åƒã€è¡¨æ ¼å’Œæ–‡æœ¬ã€‚
* **æ–°å¯èƒ½æ€§**ï¼šè¿™ä¸€æ¼”å˜ä½¿å¾—æ›´å…¨é¢å’Œä¸Šä¸‹æ–‡ä¸°å¯Œçš„ AI åº”ç”¨æˆä¸ºå¯èƒ½ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å±•ç¤ºä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œä½¿ç”¨å¤šæ¨¡æ€ RAG æ¡†æ¶å¯¹æˆ‘åœ¨ Neurocomputing ä¸Šå‘è¡¨çš„ç ”ç©¶æ–‡ç« è¿›è¡Œé—®ç­”ã€‚è¯¥æ–‡ç« åŒ…å«æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾å½¢ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ GPT-4o çš„è§†è§‰èƒ½åŠ›å¦‚ä½•å›ç­”å¤æ‚é—®é¢˜ã€‚

è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ï¼ğŸ¬

# ç›®å½•

1. è®¾ç½®è™šæ‹Ÿç¯å¢ƒå’Œå®‰è£…Pythonåº“
2. é¢„å¤„ç†éç»“æ„åŒ–æ•°æ®
3. æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒæ‘˜è¦
4. å¤šæ¨¡æ€æ£€ç´¢å™¨
5. å¤šæ¨¡æ€RAGé“¾
6. LLMè¯„ä¼°
7. ä½¿ç”¨Streamlitçš„ç”¨æˆ·ç•Œé¢

# è®¾ç½®è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£… Python åº“

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¾ç½®è™šæ‹Ÿç¯å¢ƒï¼š


```python
python3.10 -m venv venv
```
ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®‰è£…å¿…è¦çš„è½¯ä»¶åŒ…ã€‚æ‚¨å¯ä»¥åœ¨ GitHub ä»“åº“çš„ä¸»ç›®å½•ä¸­çš„â€œrequirements.txtâ€ä¸­æ‰¾åˆ°å®ƒä»¬ã€‚


```python
pip install -r requirements.txt
```
ç°åœ¨ï¼Œæ‰“å¼€ä¸€ä¸ª Jupyter Notebookï¼Œæ¯”å¦‚â€œyour-project.ipynbâ€ï¼Œå¼€å§‹ç¼–å†™æ‚¨çš„ä»£ç ã€‚å°±è¿™æ ·ï¼æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿›å…¥ä¸»è¦ç»†èŠ‚ã€‚

è¿™æ˜¯å¤šæ¨¡æ€ RAG é¡¹ç›®çš„å·¥ä½œ GitHub ä»“åº“ï¼š

# é¢„å¤„ç†éç»“æ„åŒ–æ•°æ®

è¦æ„å»ºä¸€ä¸ª RAG åº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯å°†ä¸Šä¸‹æ–‡åŠ è½½åˆ°æ•°æ®åº“ä¸­ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯ PDF æ–‡æ¡£ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•å°†æ•´ä¸ªæ–‡æ¡£ç›´æ¥å­˜å‚¨å¹¶ä¼ é€’åˆ°æç¤ºä¸­ã€‚è¿™æ ·åšå¾ˆå¯èƒ½ä¼šå¯¼è‡´é”™è¯¯ï¼Œå› ä¸ºå®ƒè¶…è¿‡äº†æœ€å¤§æ ‡è®°æ•°ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†é¦–å…ˆä»æ–‡æ¡£ä¸­æå–ä¸åŒçš„å…ƒç´ â€”â€”å›¾åƒã€æ–‡æœ¬å’Œè¡¨æ ¼ã€‚ä¸ºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [**Unstructured**](https://unstructured.io) åº“ã€‚

## å®‰è£…

é¦–å…ˆè®©æˆ‘ä»¬å®‰è£…è¿™ä¸ªåŒ…ï¼ˆå¦‚æœå°šæœªé€šè¿‡ pip å®‰è£…çš„è¯ï¼‰


```python
#%brew install tesseract poppler
%pip install -q "unstructured[all-docs]"
```
è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ç³»ç»Ÿä¸­çš„â€œtesseractâ€å’Œâ€œpopplerâ€åº“ï¼Œä»¥ä¾¿ unstructured åº“èƒ½å¤Ÿå¤„ç†æ–‡æœ¬æå–ä»¥åŠä»å›¾åƒä¸­æå–æ–‡æœ¬ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ [homebrew](https://docs.brew.sh/Installation) å®‰è£…è¿™ä¸¤ä¸ªåŒ…ï¼ˆè¯·å‚è§æ³¨é‡Šè¡Œï¼‰ã€‚

## åˆ†åŒºå’Œå—åˆ’åˆ†


```python
from unstructured.partition.pdf import partition_pdf

elements = partition_pdf(
    filename="TAGIV.pdf", # mandatory
    strategy="hi_res",                                     # mandatory to use ``hi_res`` strategy
    extract_images_in_pdf=True,                            # mandatory to set as ``True``
    extract_image_block_types=["Image", "Table"],          # optional
    extract_image_block_to_payload=False,                  # optional
    extract_image_block_output_dir="saved_images",  # optional - only works when ``extract_image_block_to_payload=False``
    )
```
æˆ‘ä»¬å°†ä½¿ç”¨ `partition_pdf` æ¨¡å—å¯¹æ–‡æ¡£è¿›è¡Œåˆ†åŒºï¼Œå¹¶ä»æˆ‘ä»¬çš„æ–‡ä»¶â€˜TAGIV.pdfâ€™ä¸­æå–ä¸åŒçš„å…ƒç´ ã€‚æˆ‘ä»¬å°†è®¾ç½® `hi_res` ç­–ç•¥ä»¥æå–é«˜è´¨é‡çš„å›¾åƒå’Œè¡¨æ ¼ã€‚å¯é€‰å‚æ•° `extract_image_block_types` å’Œ `extract_image_block_output_dir` æŒ‡å®šä»…æå–å›¾åƒå’Œè¡¨æ ¼ï¼Œå¹¶å°†å…¶ä¿å­˜åˆ°åä¸º "saved\_images" çš„ç›®å½•ä¸­ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `chunk_by_title` æ–¹æ³•å¯¹å…ƒç´ è¿›è¡Œå—åˆ’åˆ†ï¼Œè¯¥æ–¹æ³•ç”¨äºæ ¹æ®â€œæ ‡é¢˜æˆ–æ ‡é¢˜â€å°†æå–çš„å…ƒç´ åˆ’åˆ†ä¸ºå—ã€‚è¿™é€‚ç”¨äºé€šå¸¸ç”±ä¸åŒéƒ¨åˆ†å’Œå­éƒ¨åˆ†ç»„æˆçš„ç ”ç©¶æ–‡ç« ï¼Œä¾‹å¦‚å¼•è¨€ã€æ–¹æ³•ã€ç»“æœç­‰ã€‚


```python
from unstructured.chunking.title import chunk_by_title # might be better for an article 
from typing import Any

chunks = chunk_by_title(elements)

# different category in the document
category_counts = {}

for element in chunks:
    category = str(type(element))
    if category in category_counts:
        category_counts[category] += 1
    else:
        category_counts[category] = 1
    
# Unique_categories will have unique elements
unique_categories = set(category_counts.keys())
category_counts   
```

```python
{"<class 'unstructured.documents.elements.CompositeElement'>": 200,
 "<class 'unstructured.documents.elements.Table'>": 3,
 "<class 'unstructured.documents.elements.TableChunk'>": 2}
```
å—åˆ’åˆ†æ˜¾ç¤ºæœ‰ä¸‰ä¸ªç‹¬ç‰¹çš„ç±»åˆ«ï¼š

* CompositeElements
* Table
* TableChunk

â€˜**CompositeElements**â€™ æ˜¯ä¸åŒæ–‡æœ¬çš„é›†åˆï¼Œå¯èƒ½æ˜¯æ®µè½ã€éƒ¨åˆ†ã€é¡µè„šã€å…¬å¼ç­‰ã€‚è¿˜æœ‰ä¸‰ä¸ª â€˜**Table**â€™ ç»“æ„ï¼Œä»¥åŠä¸¤ä¸ª â€˜**TableChunk**â€™ï¼Œé€šå¸¸è¡¨ç¤ºè¡¨æ ¼çš„ä¸€éƒ¨åˆ†æˆ–ç‰‡æ®µã€‚å› æ­¤ï¼Œå¯èƒ½ä¸€ä¸ªè¡¨æ ¼è·¨é¡µåˆ†å‰²ï¼Œåªæœ‰ä¸€éƒ¨åˆ†è¢«åˆ’åˆ†ã€‚

*æˆ‘åœ¨æ–‡æ¡£ä¸­ç¡®å®æœ‰å››ä¸ªè¡¨æ ¼ï¼Œä½†åªæœ‰ä¸‰ä¸ªè¢«å®Œå…¨è§£æã€‚*ğŸ¤”

## è¿‡æ»¤

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ç®€åŒ–æ–‡æ¡£å…ƒç´ ï¼Œä»¥ä¾¿åˆ†åˆ«å¤„ç†æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®ä»¥è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª Pydantic æ¨¡å‹ï¼Œä»¥æ ‡å‡†åŒ–æ–‡æ¡£å…ƒç´ ï¼Œå¹¶æ ¹æ®å…¶ç±»å‹å°†å…¶åˆ†ç±»ä¸ºâ€œ**text**â€æˆ–â€œ**table**â€ã€‚

```python
from pydantic import BaseModel

class Element(BaseModel):
    type: str
    text: Any


# æŒ‰ç±»å‹åˆ†ç±»
categorized_elements = []
for element in chunks:
    if "unstructured.documents.elements.CompositeElement" in str(type(element)):
        categorized_elements.append(Element(type="text", text=str(element)))
    elif "unstructured.documents.elements.Table" in str(type(element)):
        categorized_elements.append(Element(type="table", text=str(element)))

# æ–‡æœ¬
text_elements = [e for e in categorized_elements if e.type == "text"]

# è¡¨æ ¼
table_elements = [e for e in categorized_elements if e.type == "table"]
```
æˆ‘ä»¬å°†éå†æ–‡æ¡£å…ƒç´ çš„å—ï¼Œè¯†åˆ«æ¯ä¸ªå…ƒç´ çš„ç±»å‹ï¼Œå¹¶å°†å…¶é™„åŠ åˆ°åˆ†ç±»åˆ—è¡¨ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ­¤åˆ—è¡¨è¿‡æ»¤ä¸ºæ–‡æœ¬å’Œè¡¨æ ¼å…ƒç´ çš„å•ç‹¬åˆ—è¡¨ã€‚è‡³æ­¤ï¼Œé¢„å¤„ç†æ­¥éª¤å·²å®Œæˆã€‚

# æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒæ‘˜è¦

ä¸ºäº†ä¸ºåé¢ä½¿ç”¨å¤šå‘é‡æ£€ç´¢å™¨åšå‡†å¤‡ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒå…ƒç´ åˆ›å»ºæ‘˜è¦ã€‚è¿™äº›æ‘˜è¦å°†å­˜å‚¨åœ¨å‘é‡å­˜å‚¨ä¸­ï¼Œä»¥ä¾¿åœ¨æˆ‘ä»¬å°†è¾“å…¥æŸ¥è¯¢ä¼ é€’åˆ°æç¤ºä¸­æ—¶å®ç°è¯­ä¹‰æœç´¢ã€‚

## æ–‡æœ¬å’Œè¡¨æ ¼æ‘˜è¦

è®©æˆ‘ä»¬å¼€å§‹æ–‡æœ¬å’Œè¡¨æ ¼æ‘˜è¦ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼ŒæŒ‡ç¤ºAIå……å½“ä¸“å®¶ç ”ç©¶åŠ©ç†ï¼Œè´Ÿè´£æ€»ç»“è¡¨æ ¼å’Œæ–‡æœ¬ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªé“¾ï¼Œå¤„ç†æ¯ä¸ªæ–‡æœ¬å’Œè¡¨æ ¼å…ƒç´ ï¼Œé€šè¿‡è¿™ä¸ªæç¤ºå’ŒGPT-4oæ¨¡å‹ï¼Œç”Ÿæˆç®€æ´çš„æ‘˜è¦ã€‚

ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å°†åŒæ—¶æ‰¹é‡å¤„ç†äº”ä¸ªæ–‡æœ¬æˆ–è¡¨æ ¼å…ƒç´ ï¼Œä½¿ç”¨`max_concurrency`å‚æ•°ã€‚


```python
%pip install -q langchain langchain-chroma unstructured[all-docs] pydantic lxml langchainhub langchain-openai

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

## æ£€ç´¢å™¨

# æç¤º
prompt_text = """You are an expert Research Assistant tasked with summarizing tables and texts from research articles. \ 
Give a concise summary of the text. text chunk: {element} """

prompt = ChatPromptTemplate.from_template(prompt_text)

# æ‘˜è¦é“¾
model = ChatOpenAI(temperature=0, model="gpt-4o")
summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

# åº”ç”¨äºæ–‡æœ¬
texts = [i.text for i in text_elements]
text_summaries = summarize_chain.batch(texts, {"max_concurrency": 5})

# åº”ç”¨äºè¡¨æ ¼
tables = [i.text for i in table_elements]
table_summaries = summarize_chain.batch(tables, {"max_concurrency": 5})
```

## å›¾åƒæ‘˜è¦

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®¾ç½®ä¸€äº›å‡½æ•°æ¥å¸®åŠ©æˆ‘ä»¬æ€»ç»“å›¾åƒã€‚æˆ‘ä»¬å°†å®šä¹‰ä¸‰ä¸ªå…³é”®å‡½æ•°ï¼š`encode_image`ã€`image_summarize` å’Œ `generate_img_summaries`ã€‚

1. **encode\_image**ï¼šæ­¤å‡½æ•°ä»¥äºŒè¿›åˆ¶è¯»å–æ¨¡å¼ï¼ˆâ€˜rbâ€™ï¼‰æ‰“å¼€å›¾åƒæ–‡ä»¶ï¼Œå¹¶è¿”å›å…¶ base64 ç¼–ç çš„å­—ç¬¦ä¸²è¡¨ç¤ºã€‚
2. **image\_summarize**ï¼šæ­¤å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåŒ…å«æç¤ºçš„ HumanMessage å¯¹è±¡ï¼ŒæŒ‡ç¤ºæ¨¡å‹å¦‚ä½•æ€»ç»“å›¾åƒã€‚å®ƒè¿˜åŒ…æ‹¬ base64 ç¼–ç çš„å›¾åƒæ•°æ®ï¼Œæ ¼å¼ä¸ºæ•°æ® URLï¼Œä»¥ä¾¿ç›´æ¥åœ¨å†…å®¹ä¸­åµŒå…¥å›¾åƒã€‚
3. **generate\_img\_summaries**ï¼šæ­¤å‡½æ•°å¤„ç†ç»™å®šç›®å½•ä¸­çš„æ‰€æœ‰ JPG å›¾åƒï¼Œä¸ºæ¯ä¸ªå›¾åƒç”Ÿæˆæ‘˜è¦ï¼Œå¹¶è¿”å› base64 ç¼–ç çš„å›¾åƒã€‚

è¿™äº›å‡½æ•°å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°æ€»ç»“å’Œå¤„ç†å›¾åƒï¼Œå°†å…¶æ— ç¼é›†æˆåˆ°æˆ‘ä»¬çš„å¤šæ¨¡æ€ RAG åº”ç”¨ä¸­ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´ä»£ç ï¼š

```python
## getting image summaries
import base64
import os

from langchain_core.messages import HumanMessage


def encode_image(image_path):
    """Getting the base64 string"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

def image_summarize(img_base64, prompt):
    """Make image summary"""
    chat = ChatOpenAI(model="gpt-4o", max_tokens=1024)

    msg = chat.invoke(
        [
            HumanMessage(
                content=[
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpg;base64,{img_base64}"},
                    },
                ]
            )
        ]
    )
    return msg.content


def generate_img_summaries(path):
    """
    Generate summaries and base64 encoded strings for images
    path: Path to list of .jpg files extracted by Unstructured
    """

    # Store base64 encoded images
    img_base64_list = []

    # Store image summaries
    image_summaries = []

    # Prompt
    prompt = """You are an assistant tasked with summarizing images for retrieval. \
    These summaries will be embedded and used to retrieve the raw image. \
    Give a concise summary of the image that is well optimized for retrieval."""

    # Apply to images
    for img_file in sorted(os.listdir(path)):
        if img_file.endswith(".jpg"):
            img_path = os.path.join(path, img_file)
            base64_image = encode_image(img_path)
            img_base64_list.append(base64_image)
            image_summaries.append(image_summarize(base64_image, prompt))

    return img_base64_list, image_summaries


fpath = "saved_images"

# Image summaries
img_base64_list, image_summaries = generate_img_summaries(fpath)


```

# å¤šæ¨¡æ€æ£€ç´¢å™¨

åœ¨æˆ‘ä»¬çš„æ‘˜è¦å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„å¤šæ¨¡æ€æ£€ç´¢å™¨ã€‚

## å¤šå‘é‡æ£€ç´¢å™¨

æˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ª**å¤šå‘é‡æ£€ç´¢å™¨**ï¼Œå®ƒä»¥vectorstoreã€docstoreã€id_keyå’Œsearch_kwargsä½œä¸ºè¾“å…¥ã€‚è¿™ç§æ–¹æ³•å…è®¸æˆ‘ä»¬å•ç‹¬ç´¢å¼•å†…å®¹æ‘˜è¦ï¼ŒåŒæ—¶å­˜å‚¨åŸå§‹å†…å®¹ï¼Œä»è€Œä¿ƒè¿›é«˜æ•ˆæ£€ç´¢ã€‚è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯æ‰§è¡Œå¤šæ¨¡æ€RAGçš„ä¸€ç§æ–¹å¼ï¼›å¦ä¸€ç§æ–¹æ³•å¯èƒ½æ¶‰åŠä½¿ç”¨***å¤šæ¨¡æ€åµŒå…¥***æ¥åµŒå…¥æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿ç”¨[**CLIP**](https://openai.com/index/clip/)ï¼Œç„¶åå°†åŸå§‹å›¾åƒå’Œæ–‡æœ¬å—ä¼ é€’ç»™å¤šæ¨¡æ€LLMã€‚æˆ‘å¯èƒ½ä¼šåœ¨æœªæ¥çš„åšå®¢æ–‡ç« ä¸­æ¢è®¨è¿™ä¸ªä¸»é¢˜ã€‚ğŸ™‚

æˆ‘ä»¬çš„æ£€ç´¢å™¨åˆ©ç”¨Chroma vectorstoreå­˜å‚¨å†…å®¹æ‘˜è¦çš„åµŒå…¥ï¼Œä½¿ç”¨InMemoryStoreå­˜å‚¨å®Œæ•´å†…å®¹ã€‚è¿™ç§è®¾ç½®ä½¿å¾—é€šè¿‡æ‘˜è¦è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ŒåŒæ—¶åœ¨éœ€è¦æ—¶æ£€ç´¢ç›¸åº”çš„å®Œæ•´å†…å®¹ã€‚æ¯ä¸ªæ–‡æ¡£éƒ½ä½¿ç”¨UUIDåˆ†é…ä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œè¿™æ˜¯æ£€ç´¢å™¨æ‰€éœ€çš„ã€‚

ä¸ºäº†ç®€åŒ–å°†æ‘˜è¦æ·»åŠ åˆ°vectorstoreå’Œå°†åŸå§‹å†…å®¹æ·»åŠ åˆ°docstoreçš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªåä¸º`add_documents`çš„è¾…åŠ©å‡½æ•°ã€‚è¯¥å‡½æ•°ç¡®ä¿ä»…æ·»åŠ å¯ç”¨æ‘˜è¦ã€‚

```python
import uuid

from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

def create_multi_vector_retriever(
    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):
    """
    Create retriever that indexes summaries, but returns raw images, table, or texts
    """

    # Initialize the storage layer
    store = InMemoryStore()
    id_key = "doc_id"

    # Create the multi-vector retriever
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key=id_key,
        search_kwargs={"k": 2}  # Limit to top 2 results
    )

    # Helper function to add documents to the vectorstore and docstore
    def add_documents(retriever, doc_summaries, doc_contents):
        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
        summary_docs = [
            Document(page_content=s, metadata={id_key: doc_ids[i]})
            for i, s in enumerate(doc_summaries)
        ]
        retriever.vectorstore.add_documents(summary_docs)
        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))

    # Add texts, tables, and images
    # Check that text_summaries is not empty before adding
    if text_summaries:
        add_documents(retriever, text_summaries, texts)
    # # Check that table_summaries is not empty before adding
    if table_summaries:
        add_documents(retriever, table_summaries, tables)
    # Check that image_summaries is not empty before adding
    if image_summaries:
        add_documents(retriever, image_summaries, images)

    return retriever
```

## åˆ›å»ºæ£€ç´¢å™¨

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ OpenAI åµŒå…¥æ¨¡å‹åˆ†é…ä¸€ä¸ª Chroma å‘é‡å­˜å‚¨å¹¶åˆ›å»ºæˆ‘ä»¬çš„æ£€ç´¢å™¨ã€‚

```python
# The vectorstore to use to index the summaries
vectorstore = Chroma(
    collection_name="mm_tagiv_paper", embedding_function=OpenAIEmbeddings()
)

# Create retriever
retriever_multi_vector_img = create_multi_vector_retriever(
    vectorstore,
    text_summaries,
    texts,
    table_summaries,
    tables,
    image_summaries,
    img_base64_list,
)
```

## æµ‹è¯•

è®©æˆ‘ä»¬ç”¨è¿™ä¸ªæŸ¥è¯¢æ¥æµ‹è¯•æˆ‘ä»¬çš„æ£€ç´¢å™¨ï¼Œçœ‹çœ‹å“ªäº›æ–‡æ¡£è¢«æ£€ç´¢åˆ°äº†ã€‚

```python
retriever_multi_vector_img.invoke("How is the performance of TAGI-V for the Boston dataset compared to the other methods?")
```

```python
['TAGI-V are averaged over 3 random seeds. The test log-likelihood values show that TAGI-V performs better than all other methods in 4 out of the 5 datasets. The TAGI-V method is also competitive for RMSE values where it provides the best results in 2 out of the 5 datasets, i.e., Elevators and KeggD, while it is second best for KeggU and Pol. Both PCA+ VI and NL outperform the others in two datasets.']
```
å“åº”æ˜¾ç¤ºå®ƒèƒ½å¤Ÿä»æ–‡æ¡£ä¸­æ‰¾åˆ°å…·ä½“ä¿¡æ¯ã€‚å®Œç¾ï¼ğŸš€

# å¤šæ¨¡æ€ RAG é“¾

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºæˆ‘ä»¬çš„å¤šæ¨¡æ€é“¾ã€‚æˆ‘ä»¬éœ€è¦å‡ ä¸ªè¾…åŠ©å‡½æ•°æ¥ç®¡ç† base64 ç¼–ç çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®ã€‚

## è¾…åŠ©å‡½æ•°

* `plt_image_base64(img_base64)` : ä½¿ç”¨ HTML æ˜¾ç¤º base64 ç¼–ç çš„å›¾åƒã€‚


```python
def plt_img_base64(img_base64):
    image_html = f'<img src="data:image/jpg;base64,{img_base64}" />'
    display(HTML(image_html))
```
* `looks_like_base64(sb)`: æ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦çœ‹èµ·æ¥æ˜¯ base64 ç¼–ç çš„ã€‚


```python
def looks_like_base64(sb):
    return re.match("^[A-Za-z0-9+/]+[=]{0,2}$", sb) is not None
```
* `is_image_data(b64data)`: é€šè¿‡æ£€æŸ¥å…¶å¤´éƒ¨éªŒè¯ base64 æ•°æ®æ˜¯å¦ä»£è¡¨å›¾åƒã€‚


```python
def is_image_data(b64data):
    image_signatures = {
        b"\xff\xd8\xff": "jpg",
        b"\x89\x50\x4e\x47\x0d\x0a\x1a\x0a": "png",
        b"\x47\x49\x46\x38": "gif",
        b"\x52\x49\x46\x46": "webp",
    }
    try:
        header = base64.b64decode(b64data)[:8]
        for sig, format in image_signatures.items():
            if header.startswith(sig):
                return True
        return False
    except Exception:
        return False
```
* `resize_base64_image(base64_string, size=(128, 128))`: å°† base64 ç¼–ç çš„å›¾åƒè°ƒæ•´ä¸ºæŒ‡å®šçš„å°ºå¯¸ã€‚


```python
def resize_base64_image(base64_string, size=(128, 128)):
    img_data = base64.b64decode(base64_string)
    img = Image.open(io.BytesIO(img_data))
    resized_img = img.resize(size, Image.LANCZOS)
    buffered = io.BytesIO()
    resized_img.save(buffered, format=img.format)
    return base64.b64encode(buffered.getvalue()).decode("utf-8")
```
* `split_image_text_types(docs)`: å°†æ–‡æ¡£åˆ—è¡¨åˆ†å‰²ä¸º base64 ç¼–ç çš„å›¾åƒå’Œæ–‡æœ¬ã€‚


```python
def split_image_text_types(docs):
    b64_images = []
    texts = []
    for doc in docs:
        if isinstance(doc, Document):
            doc = doc.page_content
        if looks_like_base64(doc) and is_image_data(doc):
            doc = resize_base64_image(doc, size=(1300, 600))
            b64_images.append(doc)
        else:
            texts.append(doc)
    return {"images": b64_images, "texts": texts}
```

## æç¤ºå‡½æ•°

`img_prompt_func(data_dict)` å‡½æ•°æ ¼å¼åŒ–è¾“å…¥æ•°æ®ä»¥ä¾› AI æ¨¡å‹ä½¿ç”¨ã€‚å®ƒå°†æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç»„åˆæˆä¸€ä¸ªå•ä¸€çš„æç¤ºï¼Œå…¶ä¸­åŒ…æ‹¬â€œç”¨æˆ·é—®é¢˜â€å’Œâ€œèŠå¤©è®°å½•â€ã€‚

```python
def img_prompt_func(data_dict):
    formatted_texts = "\n".join(data_dict["context"]["texts"])
    messages = []

    if data_dict["context"]["images"]:
        for image in data_dict["context"]["images"]:
            image_message = {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpg;base64,{image}"},
            }
            messages.append(image_message)
            
    chat_history = data_dict.get("chat_history", [])
    formatted_chat_history = "\n".join([f"{m.type}: {m.content}" for m in chat_history])

    text_message = {
        "type": "text",
        "text": (
            "You are a Research Assistant tasked with answering questions on research articles.\n"
            "You will be given a mixed of text, tables, and image(s) usually of tables, charts or graphs.\n"
            "Use this information to provide accurate information related to the user question. \n"
            f"User-provided question: {data_dict['question']}\n\n"
            "Text and / or tables:\n"
            f"{formatted_texts}"
            "Chat History:\n"
            f"{formatted_chat_history}\n\n"
        ),
    }
    messages.append(text_message)
    return [HumanMessage(content=messages)]
```

## å¤šæ¨¡æ€ RAG é“¾

æœ€åï¼Œ`multi_modal_rag_chain(retriever, memory=None)` å‡½æ•°ç”¨äºè®¾ç½®æˆ‘ä»¬çš„ RAG é“¾ã€‚ä»¥ä¸‹æ˜¯è¯¥é“¾çš„å·¥ä½œåŸç†ï¼š

* å®ƒä»¥ `RunnableParallel` ç»„ä»¶å¼€å§‹ï¼Œè¯¥ç»„ä»¶å¹¶è¡Œæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨ `split_image_text_types` å‡½æ•°å°†å…¶åˆ†ä¸ºæ–‡æœ¬å’Œå›¾åƒã€‚åŒæ—¶ï¼Œå®ƒå°†ç”¨æˆ·çš„é—®é¢˜åŸå°ä¸åŠ¨åœ°ä¼ é€’ï¼Œå¹¶ä»å†…å­˜ä¸­æ£€ç´¢å¯¹è¯å†å²ã€‚è¿™ç§å¹¶è¡Œå¤„ç†ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿…é€Ÿæœ‰æ•ˆåœ°æ”¶é›†ã€‚
* æ­¤æ­¥éª¤çš„è¾“å‡ºç”± `img_prompt_func` æ ¼å¼åŒ–ä¸ºç»“æ„åŒ–æç¤ºï¼Œå°†ç”¨æˆ·æŸ¥è¯¢ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å’ŒèŠå¤©å†å²æ•´åˆä¸ºé€‚åˆ AI æ¨¡å‹çš„è¿è´¯æ ¼å¼ã€‚
* è¿™ä¸ªç»“æ„åŒ–æç¤ºéšåä¼ é€’ç»™ GPT-4o æ¨¡å‹ï¼Œç”ŸæˆåŸºäºæä¾›ä¿¡æ¯çš„å“åº”ã€‚
* æœ€åï¼Œ`StrOutputParser` ç¡®ä¿æ¨¡å‹çš„è¾“å‡ºæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œå‡†å¤‡è¿›ä¸€æ­¥ä½¿ç”¨ã€‚

è¿™ä¸€è®¾è®¡ä½¿ç³»ç»Ÿèƒ½å¤Ÿçµæ´»å¤„ç†éœ€è¦ç†è§£å’Œæ•´åˆæ–‡æœ¬ä¸è§†è§‰æ•°æ®çš„å¤æ‚æŸ¥è¯¢ï¼ŒåŒæ—¶ä¿æŒæ­£åœ¨è¿›è¡Œçš„å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚

```python
def multi_modal_rag_chain(retriever, memory=None):
    if memory is None:
        memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
    
    model = ChatOpenAI(temperature=0, model="gpt-4o", max_tokens=1024)

    chain = (
        RunnableParallel(
            {
            "context": retriever | RunnableLambda(split_image_text_types),
            "question": RunnablePassthrough(),
            "chat_history": lambda x: memory.load_memory_variables({})["chat_history"]
        })
        | RunnableLambda(img_prompt_func)
        | model
        | StrOutputParser()
    )

    def run_chain(query):
        result = chain.invoke(query)
        memory.save_context({"input": query}, {"output": result})
        return result

    return run_chain


# åˆ›å»º RAG é“¾
chain_mm_rag = multi_modal_rag_chain(retriever=retriever_multi_vector_img)
```

## æµ‹è¯•æ—¶é—´ï¼

è®©æˆ‘ä»¬æå‡ºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œçœ‹çœ‹æˆ‘ä»¬é“¾æ¡çš„å“åº”ã€‚

```python
# First Question
query = "How is the performance of TAGI-V for the Boston dataset compared to the other methods?"
print(chain_mm_rag(query))
```

```python
è¦ç¡®å®šTAGI-Våœ¨æ³¢å£«é¡¿æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å¦‚ä½•ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥çœ‹è¯¥æ•°æ®é›†æä¾›çš„å…·ä½“æŒ‡æ ‡ã€‚æ–‡æœ¬æåˆ°ï¼ŒTAGI-Våœ¨5ä¸ªæ•°æ®é›†ä¸­çš„4ä¸ªæ•°æ®é›†çš„æµ‹è¯•å¯¹æ•°ä¼¼ç„¶å€¼ä¸Šè¡¨ç°ä¼˜äºæ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨RMSEå€¼ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨5ä¸ªæ•°æ®é›†ä¸­çš„2ä¸ªæ•°æ®é›†ä¸­æä¾›äº†æœ€ä½³ç»“æœã€‚

ç„¶è€Œï¼Œæ–‡æœ¬å¹¶æ²¡æœ‰æ˜ç¡®æŒ‡å‡ºTAGI-Våœ¨æ³¢å£«é¡¿æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚è¦æä¾›å‡†ç¡®çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬éœ€è¦TAGI-Vå’Œå…¶ä»–æ–¹æ³•åœ¨æ³¢å£«é¡¿æ•°æ®é›†ä¸Šçš„å…·ä½“æµ‹è¯•å¯¹æ•°ä¼¼ç„¶å€¼å’ŒRMSEå€¼ã€‚

æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼š
- TAGI-Våœ¨æµ‹è¯•å¯¹æ•°ä¼¼ç„¶å€¼ä¸Šé€šå¸¸è¡¨ç°å¼ºåŠ²ã€‚
- TAGI-Våœ¨RMSEå€¼ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨å…¶ä»–æ•°æ®é›†ä¸­æ’åç¬¬äºŒã€‚

å¦‚æœæ³¢å£«é¡¿æ•°æ®é›†æ˜¯TAGI-Vä¸æ˜¯æœ€ä½³çš„é‚£äº›æ•°æ®é›†ä¹‹ä¸€ï¼Œå®ƒå¯èƒ½ä¼šè¢«PCA+VIæˆ–NLè¶…è¶Šï¼Œè¿™ä¸¤è€…åœ¨å„è‡ªçš„ä¸¤ä¸ªæ•°æ®é›†ä¸­è¢«æåŠä¸ºæœ€ä½³è¡¨ç°è€…ã€‚

åœ¨æ²¡æœ‰æ³¢å£«é¡¿æ•°æ®é›†å…·ä½“å€¼çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­TAGI-Vå¯èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨è¿™ä¸ªç‰¹å®šæ•°æ®é›†ä¸­å¯èƒ½ä¸æ˜¯æœ€ä½³è¡¨ç°è€…ã€‚è¦è¿›è¡Œæ˜ç¡®çš„æ¯”è¾ƒï¼Œéœ€è¦æ‰€æœ‰æ–¹æ³•åœ¨æ³¢å£«é¡¿æ•°æ®é›†ä¸Šçš„ç¡®åˆ‡æµ‹è¯•å¯¹æ•°ä¼¼ç„¶å€¼å’ŒRMSEå€¼ã€‚
```
å“åº”æ˜¯æ­£ç¡®çš„ã€‚æˆ‘å¯ä»¥éªŒè¯ï¼Œå› ä¸ºæˆ‘æ˜¯æ–‡ç« çš„ä½œè€…ã€‚ğŸ˜€

è®©æˆ‘ä»¬å°è¯•ç¬¬äºŒä¸ªé—®é¢˜ã€‚

```python
# Second Question
query = "What is the performance of the same method for the Concrete dataset compared to the other methods?"
print(chain_mm_rag(query))
```

```python
è¦è¯„ä¼°åŒä¸€æ–¹æ³•åœ¨æ··å‡åœŸæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å¦‚ä½•ï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒæä¾›çš„å›¾è¡¨å’Œè¡¨æ ¼ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æï¼š

### æ€§èƒ½æŒ‡æ ‡ï¼š
1. **RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰**ï¼š
   - æ··å‡åœŸæ•°æ®é›†çš„RMSEå€¼åœ¨å›¾è¡¨ä¸­æ˜¾ç¤ºã€‚æ¯”è¾ƒçš„æ–¹æ³•åŒ…æ‹¬PCA+ESSã€PCA+VIã€SWAGã€TAGI-Vã€TAGI-V2Lã€TAGIã€PBPã€MC-dropoutã€PBP-MVã€VMGã€Ensembleã€DVIå’ŒNNã€‚
   - ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼ŒTAGI-VåŠå…¶å˜ä½“ï¼ˆTAGI-V2Lã€TAGIï¼‰åœ¨RMSEå€¼ä¸Šä¸å…¶ä»–æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚æ–‡æœ¬ä¸­æ²¡æœ‰æ˜ç¡®æä¾›ç¡®åˆ‡çš„RMSEå€¼ï¼Œä½†è§†è§‰è¡¨ç°è¡¨æ˜TAGI-Vè¡¨ç°è‰¯å¥½ã€‚

2. **è®­ç»ƒæ—¶é—´**ï¼š
   - æ··å‡åœŸæ•°æ®é›†çš„è®­ç»ƒæ—¶é—´åœ¨æ—¶é—´ï¼ˆç§’ï¼‰ä¸RMSEçš„å›¾è¡¨ä¸­æ˜¾ç¤ºã€‚
   - TAGI-VåŠå…¶å˜ä½“ï¼ˆTAGI-V2Lã€TAGIï¼‰ç›¸æ¯”äºPCA+ESSã€PCA+VIã€PBP-MVå’ŒVMGç­‰æ–¹æ³•æ˜¾ç¤ºå‡ºæ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚TAGI-Væ˜¾è‘—æ›´å¿«ï¼Œå¤§çº¦æ¯”PCA+ESSå’ŒPCA+VIå¿«100å€ï¼Œæ¯”PBPå¿«çº¦10å€ï¼Œæ¯”Ensembleå¿«çº¦3å€ã€‚

### æ¯”è¾ƒåˆ†æï¼š
- **TAGI-V**ï¼š
  - **RMSE**ï¼šTAGI-Væ˜¾ç¤ºå‡ºå…·æœ‰ç«äº‰åŠ›çš„RMSEå€¼ï¼Œè¡¨æ˜è‰¯å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚
  - **è®­ç»ƒæ—¶é—´**ï¼šTAGI-Vçš„è®­ç»ƒæ—¶é—´æ˜¾è‘—å¿«äºå¤§å¤šæ•°å…¶ä»–æ–¹æ³•ã€‚

- **å…¶ä»–æ–¹æ³•**ï¼š
  - **PCA+ESSå’ŒPCA+VI**ï¼šè¿™äº›æ–¹æ³•çš„RMSEå€¼è¾ƒé«˜ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œç›¸æ¯”äºTAGI-Vã€‚
  - **SWAGã€PBPã€MC-dropoutã€PBP-MVã€VMGã€Ensembleã€DVIã€NN**ï¼šè¿™äº›æ–¹æ³•çš„RMSEå€¼å’Œè®­ç»ƒæ—¶é—´ä¹Ÿé«˜äºTAGI-Vã€‚

### æ€»ç»“ï¼š
TAGI-Våœ¨æ··å‡åœŸæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ— è®ºæ˜¯åœ¨RMSEè¿˜æ˜¯è®­ç»ƒæ—¶é—´æ–¹é¢ã€‚å®ƒåœ¨RMSEå€¼ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶é—´ä¸Šæ˜¾è‘—æ›´å¿«ï¼Œç›¸æ¯”äºå…¶ä»–æ–¹æ³•ã€‚è¿™ä½¿å¾—TAGI-Væˆä¸ºæ··å‡åœŸæ•°æ®é›†çš„é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚
```
æ‰€ä»¥ï¼Œå®ƒè®°å¾—æˆ‘ä»¬åœ¨è¯¢é—®TAGI-Vï¼Œè¡¨æ˜åº”ç”¨ç¨‹åºç°åœ¨æ˜¯***å¯¹è¯å¼çš„***ã€‚ğŸ˜

è®©æˆ‘ä»¬æ£€æŸ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œçœ‹çœ‹ä¸€ä¸ªé—®é¢˜æ˜¯å¦ä¼šè¿”å›ä¸€ä¸ªbase64ç¼–ç çš„å›¾åƒã€‚

```python
# Check retrieval
query = "How is the performance of He compared to modified He for the various datasets such as Boston, Concrete etc.?"
docs = retriever_multi_vector_img.invoke(query, limit=6)
```
ç¡®å®æœ‰ä¸€ä¸ªæ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å›¾åƒæ–‡ä»¶ã€‚ğŸ™ŒğŸ¼

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WH_m85Gk4jDzOm815PIqxw.png)

# LLMè¯„ä¼°

ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåä¸º[***DeepEval***](https://docs.confident-ai.com)çš„å¼€æºLLMè¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶æä¾›äº†å¤šä¸ªæŒ‡æ ‡æ¥æµ‹è¯•æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œæ ¹æ®è¾“å…¥æŸ¥è¯¢ç»™å‡ºçš„æœ€ç»ˆå“åº”ã€‚åœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š

* **å¿ å®åº¦æŒ‡æ ‡**ï¼šè¡¡é‡æ¨¡å‹è¾“å‡ºä¸æä¾›çš„ä¸Šä¸‹æ–‡çš„å¯¹é½ç¨‹åº¦ã€‚
* **ä¸Šä¸‹æ–‡ç›¸å…³æ€§æŒ‡æ ‡**ï¼šè¯„ä¼°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸ç»™å®šæŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚
* **ç­”æ¡ˆç›¸å…³æ€§æŒ‡æ ‡**ï¼šè¯„ä¼°æ¨¡å‹çš„å“åº”ä¸è¾“å…¥æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚
* **å¹»è§‰æŒ‡æ ‡**ï¼šæ£€æµ‹æ¨¡å‹è¾“å‡ºæ˜¯å¦åŒ…å«åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚

DeepEvalè¿˜æä¾›äº†æ›´å¤šæŒ‡æ ‡ï¼Œé¼“åŠ±æ‚¨åœ¨åº“æ–‡æ¡£ä¸­è¿›è¡Œæ¢ç´¢ã€‚

æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªåä¸º`LLM_Metric`çš„ç±»ï¼Œè¯¥ç±»åŒ…å«æ¯ä¸ªæŒ‡æ ‡çš„å‡½æ•°ã€‚æ¯ä¸ªæŒ‡æ ‡çš„è¾“å‡ºä¸ä»…æ˜¯ä¸€ä¸ªåˆ†æ•°ï¼Œè¿˜æä¾›äº†è¯¥åˆ†æ•°çš„åŸå› ï¼Œä»è€Œæä¾›å¯¹æ¨¡å‹æ€§èƒ½çš„æ›´æ·±å…¥çš„æ´å¯Ÿã€‚

```python
class LLM_Metric:
    def __init__(self, query, retrieval_context, actual_output):
        self.query = query
        self.retrieval_context = retrieval_context
        self.actual_output = actual_output

    # Faithfulness
    def get_faithfulness_metric(self):
        metric = FaithfulnessMetric(
            threshold=0.7,
            model="gpt-4o",
            include_reason=True
        )
        test_case = LLMTestCase(
            input=self.query,
            actual_output=self.actual_output,
            retrieval_context=self.retrieval_context
        )

        metric.measure(test_case)
        return metric.score, metric.reason

    # Contextual Relevancy
    def get_contextual_relevancy_metric(self):
        metric = ContextualRelevancyMetric(
            threshold=0.7,
            model="gpt-4o",
            include_reason=True
        )
        test_case = LLMTestCase(
            input=self.query,
            actual_output=self.actual_output,
            retrieval_context=self.retrieval_context
        )
        
        metric.measure(test_case)
        return metric.score, metric.reason
    
    # Answer Relevancy
    def get_answer_relevancy_metric(self):
        metric = AnswerRelevancyMetric(
        threshold=0.7,
        model="gpt-4o",
        include_reason=True
        )
        test_case = LLMTestCase(
            input=self.query,
            actual_output=self.actual_output
        )
        metric.measure(test_case)
        return metric.score, metric.reason
    
    # Hallucination
    def get_hallucination_metric(self):
        metric = HallucinationMetric(threshold=0.5)
        test_case = LLMTestCase(
        input=self.query,
        actual_output=self.actual_output,
        context=self.retrieval_context  
        )
        metric.measure(test_case)
        return metric.score, metric.reason
```

# ä½¿ç”¨ Streamlit çš„ç”¨æˆ·ç•Œé¢

é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»¥æ¨¡å—åŒ–çš„æ–¹å¼æ„å»ºä»£ç ã€‚æˆ‘ä»¬å°†æ‰€æœ‰å‡½æ•°æ”¾åœ¨ä¸€ä¸ªåä¸º `utils` çš„æ–‡ä»¶å¤¹ä¸­ã€‚ç›®å½•ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
advanced-RAG-app/
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ __init__.py
â”‚ â”œâ”€â”€ image_processing.py
â”‚ â”œâ”€â”€ rag_chain.py
â”‚ â”œâ”€â”€ rag_evaluation.py
â”‚ â””â”€â”€ retriever.py
â”‚
â””â”€â”€ main.py
â””â”€â”€ requirements.txt

```
è¿™äº›å‡½æ•°åœ¨ä¹‹å‰çš„å¸–å­ä¸­å·²ç»è§£é‡Šè¿‡ã€‚è¿™é‡Œæˆ‘ä»¬åªæ˜¯è°ƒæ•´äº†ä¸€ä¸‹ç»“æ„ï¼Œä»¥ä¾¿èƒ½å¤Ÿä»ä¸»åº”ç”¨æ–‡ä»¶ä¸­è°ƒç”¨æ‰€æœ‰å‡½æ•°ã€‚

è¯è™½å¦‚æ­¤ï¼è®©æˆ‘ä»¬å¡«å……æˆ‘ä»¬çš„ `main.py` æ–‡ä»¶ã€‚

```python
import streamlit as st
from unstructured.partition.pdf import partition_pdf
from unstructured.chunking.title import chunk_by_title
from typing import Any
from pydantic import BaseModel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from utils.image_processing import generate_img_summaries
from utils.retriever import create_multi_vector_retriever
from utils.rag_chain import multi_modal_rag_chain, plt_img_base64
from utils.rag_evaluation import LLM_Metric
from io import BytesIO
import base64
from PIL import Image
import io

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'retriever' not in st.session_state:
    st.session_state.retriever = None
if 'chain' not in st.session_state:
    st.session_state.chain = None

# Streamlit åº”ç”¨è®¾ç½®
st.set_page_config(page_title='å¤šæ¨¡æ€ RAG åº”ç”¨', page_icon='random', layout='wide', initial_sidebar_state='auto')

def process_document(uploaded_file):
    # å¤„ç† PDF
    with st.spinner('æ­£åœ¨å¤„ç† PDF...'):
        st.sidebar.info('æ­£åœ¨ä» PDF ä¸­æå–å…ƒç´ ...')
        pdf_bytes = uploaded_file.read()
        elements = partition_pdf(
            file=BytesIO(pdf_bytes),
            strategy="hi_res",
            extract_images_in_pdf=True,
            extract_image_block_types=["Image", "Table"],
            extract_image_block_to_payload=False,
            extract_image_block_output_dir="docs/saved_images",
        )
        st.sidebar.success('PDF å…ƒç´ æå–æˆåŠŸï¼')

    # æŒ‰æ ‡é¢˜åˆ›å»ºå—
    with st.spinner('æ­£åœ¨åˆ†å—å†…å®¹...'):
        st.sidebar.info('æ­£åœ¨æŒ‰æ ‡é¢˜åˆ›å»ºå—...')
        chunks = chunk_by_title(elements)
        st.sidebar.success('åˆ†å—å®Œæˆï¼')

    # åˆ†ç±»å…ƒç´ 
    class Element(BaseModel):
        type: str
        text: Any

    categorized_elements = []
    for element in chunks:
        if "unstructured.documents.elements.CompositeElement" in str(type(element)):
            categorized_elements.append(Element(type="text", text=str(element)))
        elif "unstructured.documents.elements.Table" in str(type(element)):
            categorized_elements.append(Element(type="table", text=str(element)))

    text_elements = [e for e in categorized_elements if e.type == "text"]
    table_elements = [e for e in categorized_elements if e.type == "table"]

    # æç¤º
    prompt_text = """æ‚¨æ˜¯ä¸€ä½ä¸“å®¶ç ”ç©¶åŠ©ç†ï¼Œè´Ÿè´£æ€»ç»“ç ”ç©¶æ–‡ç« ä¸­çš„è¡¨æ ¼å’Œæ–‡æœ¬ã€‚ \
    è¯·ç»™å‡ºæ–‡æœ¬çš„ç®€æ˜æ€»ç»“ã€‚æ–‡æœ¬å—ï¼š{element} """

    prompt = ChatPromptTemplate.from_template(prompt_text)

    # æ€»ç»“é“¾
    model = ChatOpenAI(temperature=0, model="gpt-4o", max_tokens=1024)
    summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()

    texts = [i.text for i in text_elements]
    text_summaries = summarize_chain.batch(texts, {"max_concurrency": 5})

    tables = [i.text for i in table_elements]
    table_summaries = summarize_chain.batch(tables, {"max_concurrency": 5})

    # å›¾åƒæ€»ç»“
    fpath = "docs/saved_images"
    img_base64_list, image_summaries = generate_img_summaries(fpath)

    # å‘é‡å­˜å‚¨
    vectorstore = Chroma(
        collection_name="mm_tagiv_paper", embedding_function=OpenAIEmbeddings()
    )

    # åˆ›å»ºæ£€ç´¢å™¨
    st.session_state.retriever = create_multi_vector_retriever(
        vectorstore,
        text_summaries,
        texts,
        table_summaries,
        tables,
        image_summaries,
        img_base64_list,
    )

    # åˆ›å»º RAG é“¾
    st.session_state.chain = multi_modal_rag_chain(retriever=st.session_state.retriever)
    st.session_state.processed = True

with st.sidebar:
    # æ–‡ä»¶ä¸Šä¼ 
    st.subheader('æ·»åŠ æ‚¨çš„ PDF')
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡ä»¶", type=["pdf"])
    if st.button('æäº¤'):
        if uploaded_file is not None:
            process_document(uploaded_file)
            st.success('æ–‡æ¡£å¤„ç†æˆåŠŸï¼')
        else:
            st.error('è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶ã€‚')

# æŸ¥è¯¢å“åº”å’Œè¯„ä¼°çš„ä¸»é¡µé¢
st.subheader("RAG åŠ©æ‰‹")
query = st.text_input("è¾“å…¥æ‚¨çš„æŸ¥è¯¢ï¼š")

if query and st.session_state.processed:
    # æ‰§è¡Œ
    retrieval_context = st.session_state.retriever.invoke(query, limit=1)
    actual_output = st.session_state.chain(query)

    # è¯„ä¼°
    llm_metric = LLM_Metric(query, retrieval_context, actual_output)
    faith_score, faith_reason = llm_metric.get_faithfulness_metric()
    relevancy_score, relevancy_reason = llm_metric.get_contextual_relevancy_metric()
    answer_relevancy_score, answer_relevancy_reason = llm_metric.get_answer_relevancy_metric()
    hallucination_score, hallucination_reason = llm_metric.get_hallucination_metric()

    # æ˜¾ç¤ºç»“æœ
    st.subheader("æŸ¥è¯¢å“åº”")
    st.write(actual_output)

    st.subheader("è¯„ä¼°æŒ‡æ ‡")
    st.write(f"å¯ä¿¡åº¦è¯„åˆ†ï¼š{faith_score}ï¼ŒåŸå› ï¼š{faith_reason}")
    st.write(f"ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„åˆ†ï¼š{relevancy_score}ï¼ŒåŸå› ï¼š{relevancy_reason}")
    st.write(f"ç­”æ¡ˆç›¸å…³æ€§è¯„åˆ†ï¼š{answer_relevancy_score}ï¼ŒåŸå› ï¼š{answer_relevancy_reason}")
    st.write(f"å¹»è§‰è¯„åˆ†ï¼š{hallucination_score}ï¼ŒåŸå› ï¼š{hallucination_reason}")

    
elif query and not st.session_state.processed:
    st.warning("è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ã€‚")
```
ä¸è¦æ„Ÿåˆ°ä¸çŸ¥æ‰€æªï¼æˆ‘ä¼šè¯¦ç»†æŒ‡å¯¼æ‚¨å®Œæˆæ¯ä¸€ä¸ªæ­¥éª¤ã€‚

é¦–å…ˆï¼Œåº”ç”¨ç¨‹åºåˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼Œä»¥è·Ÿè¸ªæ–‡æ¡£æ˜¯å¦å·²è¢«å¤„ç†ï¼Œå¹¶å­˜å‚¨æ£€ç´¢å™¨å’Œ RAG é“¾å¯¹è±¡ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ `process_document` å‡½æ•°ï¼Œè¯¥å‡½æ•°å¤„ç†ä¸Šä¼ çš„ PDF æ–‡ä»¶çš„æ ¸å¿ƒå¤„ç†ã€‚è¿™åŒ…æ‹¬ï¼š

* PDF æå–
* åˆ†å—
* åˆ†ç±»
* æ€»ç»“
* å›¾åƒæ€»ç»“
* å‘é‡å­˜å‚¨åˆå§‹åŒ–
* æ£€ç´¢å™¨åˆ›å»º
* RAG é“¾åˆ›å»º

ä¾§è¾¹æ å…è®¸ç”¨æˆ·ä¸Šä¼  PDF æ–‡ä»¶ï¼Œè§¦å‘æ–‡æ¡£å¤„ç†åŠŸèƒ½ã€‚ä¸€æ—¦æ–‡æ¡£æˆåŠŸå¤„ç†ï¼Œä¸»é¡µé¢å…è®¸ç”¨æˆ·è¾“å…¥æŸ¥è¯¢å¹¶æŸ¥çœ‹å“åº”åŠè¯„ä¼°æŒ‡æ ‡ã€‚

æœ€åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œåº”ç”¨ï¼š

```python
streamlit run main.py
```
å°±è¿™æ ·ï¼ 

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iaiGPR-OEeuUM6kguTEtpg.gif)

# æœ€åçš„æƒ³æ³•

æˆ‘ä»¬å·²ç»å®Œæˆäº†é¡¹ç›®ï¼Œæ¢è®¨äº†å¦‚ä½•ä¸ºPDFæ–‡æ¡£åˆ›å»ºä¸€ä¸ªå¤šæ¨¡æ€çš„RAGåº”ç”¨ç¨‹åºï¼Œä»¥å®ç°è§†è§‰é—®ç­”ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ‰€æ¶µç›–çš„å†…å®¹ï¼š

* åˆ©ç”¨Unstructuredåº“å°†æ–‡æ¡£æ‹†åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ã€‚
* é‡‡ç”¨å¤šå‘é‡æ£€ç´¢å™¨å°†æ–‡æœ¬ã€è¡¨æ ¼å’Œå›¾åƒæ‘˜è¦å­˜å‚¨åœ¨å‘é‡å­˜å‚¨ä¸­ï¼ŒåŒæ—¶å°†åŸå§‹å†…å®¹ä¿ç•™åœ¨æ–‡æ¡£å­˜å‚¨ä¸­ã€‚
* åˆ©ç”¨DeepEvalåº“è¯„ä¼°æˆ‘ä»¬çš„LLMå“åº”ã€‚
* ä½¿ç”¨Streamlitæ„å»ºäº†ä¸€ä¸ªç®€å•çš„ç”¨æˆ·ç•Œé¢ã€‚

æˆ‘å¾ˆæƒ³å¬å¬ä½ å¯¹è¿™ç¯‡æ–‡ç« çš„çœ‹æ³•ï¼Œè¯·åœ¨è¯„è®ºåŒºç•™è¨€ã€‚æˆ‘å¸Œæœ›è¿™èƒ½æˆä¸ºä½ æ„‰å¿«çš„å‘¨æœ«é¡¹ç›®ã€‚ä¸‹æ¬¡è§ï¼

Ã€ BientÃ´t ğŸ™‚

*å—¨ï¼Œæˆ‘æ˜¯Bhargobï¼ğŸ‘‹*

*æˆ‘æ˜¯ä¸€åæœºå™¨å­¦ä¹ ç ”ç©¶å‘˜ï¼Œå¯¹æ„å»ºGenAIåº”ç”¨ç¨‹åºå……æ»¡çƒ­æƒ…ã€‚*

*éå¸¸æ„Ÿè°¢æ‰€æœ‰å…³æ³¨æˆ‘å·¥ä½œçš„æœ‹å‹ä»¬ã€‚æˆ‘å¸Œæœ›æˆ‘çš„æ–‡ç« èƒ½æ¿€åŠ±ä½ åœ¨äººå·¥æ™ºèƒ½å­¦ä¹ ä¹‹æ—…ä¸­å‰è¡Œã€‚ä½ å¯ä»¥åœ¨[**LinkedIn**](https://www.linkedin.com/in/bhargobdeka)å’Œ[**GitHub**](https://github.com/bhargobdeka)ä¸Šä¸æˆ‘è”ç³»â€”â€”æˆ‘å§‹ç»ˆæ¬¢è¿ä¸æœ‰è¶£çš„äººå’Œæ¿€åŠ¨äººå¿ƒçš„ç”ŸæˆAIé¡¹ç›®åˆä½œã€‚å¦‚æœä½ éœ€è¦**AIè§£å†³æ–¹æ¡ˆæ¥æ¨åŠ¨ä½ çš„ä¸šåŠ¡**ï¼Œè¯·åœ¨LinkedInä¸Šä¸æˆ‘è”ç³»ï¼Œæˆ‘ä»¬å¯ä»¥èŠèŠï¼*

*æœŸå¾…ä¸ä½ çš„è”ç³»ï¼*

# èµ„æº

*å¦‚æœæ‚¨å¯¹ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„æ¦‚å¿µä¸å¤ªç†Ÿæ‚‰ï¼Œæˆ‘å¼ºçƒˆå»ºè®®æ‚¨å…³æ³¨æˆ‘çš„LangChainæ¡†æ¶åˆå­¦è€…ç³»åˆ—ã€‚ä»åŸºæœ¬æ¦‚å¿µåˆ°é«˜çº§éƒ¨ç½²ç­–ç•¥ï¼Œè¿™ä¸ªç³»åˆ—æ¶µç›–äº†æ‚¨å¼€å§‹æ„å»ºè‡ªå·±çš„AIåº”ç”¨æ‰€éœ€çš„ä¸€åˆ‡ã€‚*

*ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ‰€æ¶µç›–å†…å®¹çš„å¿«é€Ÿå›é¡¾ï¼š*

*1ï¸âƒ£ [ä½¿ç”¨LangChainæ„å»ºèŠå¤©æœºå™¨äººï¼šPythonçš„æ¸©å’Œä»‹ç»](https://readmedium.com/gitconnected/chatbots-with-langchain-a-gentle-introduction-with-python-62348fc2e5f1)2ï¸âƒ£ [åˆ›å»ºæ£€ç´¢é“¾ï¼šä½¿ç”¨LangChainçš„RAGèŠå¤©æœºå™¨äºº](https://readmedium.com/gitconnected/creating-retrieval-chain-with-langchain-f359261e0b85)3ï¸âƒ£ [ä½¿ç”¨LangChainåˆ›å»ºRAGä»£ç†](https://readmedium.com/gitconnected/creating-rag-agents-with-langchain-e8759735922d)4ï¸âƒ£ [ä½¿ç”¨LangGraphè®¾è®¡RAGä»£ç†å·¥ä½œæµ](https://readmedium.com/gitconnected/designing-rag-agent-workflow-with-langgraph-861b002d9380)5ï¸âƒ£ [LangGraphã€FastAPIå’ŒStreamlit/Gradioï¼šAIå¼€å‘çš„å®Œç¾ç»„åˆ](https://readmedium.com/gitconnected/langgraph-fastapi-and-streamlit-gradio-the-perfect-trio-for-ai-development-f1a82775496a)6ï¸âƒ£ [ä»æœ¬åœ°åˆ°äº‘ï¼šä½¿ç”¨Dockerå’ŒAWS EC2éƒ¨ç½²LLMåº”ç”¨](https://readmedium.com/gitconnected/from-local-to-cloud-deploying-llm-application-with-docker-and-aws-ec2-191380d07089)7ï¸âƒ£ [é€šè¿‡Gitæ¨é€è¿›è¡Œäº‘éƒ¨ç½²ï¼šæ‚¨æ‰€éœ€çš„ä»…æ˜¯GitHub Actions](https://readmedium.com/gitconnected/cloud-deployment-with-a-git-push-github-action-is-all-you-need-53783be49acc)*

*æˆ‘è¿˜å‘å¸ƒäº†ä¸€äº›å…³äºé«˜çº§AIä¸»é¢˜çš„æ–‡ç« ï¼ŒåŒ…æ‹¬åˆ›å»ºä¸€ä¸ªå¤šä»£ç†æ¡†æ¶å’Œä¸¤ä¸ªç«¯åˆ°ç«¯é¡¹ç›®ï¼š*

*8ï¸âƒ£ [æ„å»ºé…’åº—æ¨èç³»ç»Ÿï¼šä½¿ç”¨CrewAIã€Ollamaå’ŒGradioçš„å¤šä»£ç†æ¡†æ¶](https://readmedium.com/building-a-hotel-recommender-multi-agent-framework-with-crewai-ollama-and-gradio-c81645776183)9ï¸âƒ£ [ä½¿ç”¨CrewAIä»£ç†åˆ›å»ºç«¯åˆ°ç«¯æ•°æ®ç§‘å­¦é¡¹ç›®](https://readmedium.com/creating-an-end-to-end-data-science-project-with-crewai-agents-f98d02b4e203)*

*æˆ‘å¸Œæœ›è¿™äº›èµ„æºèƒ½å¸®åŠ©æ‚¨åœ¨AIçš„æ—…ç¨‹ä¸­ã€‚ç¥å­¦ä¹ æ„‰å¿«ï¼*
