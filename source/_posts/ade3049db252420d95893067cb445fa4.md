
---
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*a_lPn9Gs4AmnGZ14uWn0NQ.png
date: '2024-06-17 14:27:31'
tags:
  - ç§»åŠ¨åº”ç”¨
  - å¤§å‹è¯­è¨€æ¨¡å‹
  - Androidå¼€å‘
title: åœ¨Androidä¸Šä½¿ç”¨Gemma 2Bè¿›è¡Œè®¾å¤‡ç«¯LLMå¤„ç†  AIèŠå¤©åº”ç”¨

---




éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸æ–­è¿›æ­¥ï¼Œå°†å®ƒä»¬é›†æˆåˆ°ç§»åŠ¨åº”ç”¨ä¸­å˜å¾—è¶Šæ¥è¶Šå¯è¡Œä¸”æœ‰ç›Šã€‚è®¾å¤‡ç«¯LLMå¤„ç†å…·æœ‰å¤šç§ä¼˜åŠ¿ï¼Œå¦‚é™ä½å»¶è¿Ÿã€å¢å¼ºéšç§ä¿æŠ¤å’Œç¦»çº¿åŠŸèƒ½ã€‚

é€šè¿‡ç›´æ¥åœ¨è®¾å¤‡ä¸Šè¿è¡ŒLLMsï¼Œåº”ç”¨ç¨‹åºå¯ä»¥æä¾›å®æ—¶å“åº”ï¼Œæ— éœ€ä¾èµ–æŒç»­çš„äº’è”ç½‘è¿æ¥æˆ–å°†æ•æ„Ÿæ•°æ®æš´éœ²ç»™å¤–éƒ¨æœåŠ¡å™¨ã€‚

æœ¬åšå®¢æ¢è®¨äº†åœ¨Androidä¸Šè¿›è¡Œè®¾å¤‡ç«¯LLMå¤„ç†çš„æ¦‚å¿µï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Kotlinå®ç°è¿™ä¸€åŠŸèƒ½ã€‚æˆ‘ä»¬å°†é€æ­¥ä»‹ç»åˆ©ç”¨LLMsè¿›è¡Œå®æ—¶æ–‡æœ¬ç”Ÿæˆå’Œå¤„ç†çš„Androidåº”ç”¨ç¨‹åºçš„å…³é”®ç»„ä»¶ï¼Œæä¾›ä¸€ç§é«˜æ•ˆä¸”å®‰å…¨çš„æ–¹å¼æ¥ç›´æ¥åœ¨è®¾å¤‡ä¸Šå¤„ç†è¯­è¨€æ¨¡å‹ã€‚

# å…¥é—¨æŒ‡å—

æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯[Gemma 2B](https://www.kaggle.com/models/google/gemma/tfLite/)ï¼ŒGemma æ˜¯ä¸€ç³»åˆ—è½»é‡çº§ã€å¼€æºçš„æ¨¡å‹ï¼ŒåŸºäºè°·æ­Œåˆ›å»º Gemini æ¨¡å‹æ—¶æ‰€ç”¨çš„ç ”ç©¶å’ŒæŠ€æœ¯çš„æˆæœã€‚æ‚¨å¯ä»¥ä»æä¾›çš„é“¾æ¥ä¸‹è½½è¯¥æ¨¡å‹ï¼Œè§£å‹åå³å¯ä½¿ç”¨ã€‚

è¦å¼€å§‹ä½¿ç”¨ï¼Œè¯·åˆ›å»ºä¸€ä¸ªæ–°çš„ Android é¡¹ç›®ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Composeã€‚æˆ‘ä»¬å°†å€ŸåŠ©è°·æ­Œçš„ [Mediapipe](https://ai.google.dev/edge/mediapipe/solutions/guide) ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚MediaPipe Solutions æä¾›äº†ä¸€ç³»åˆ—åº“å’Œå·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©æ‚¨å¿«é€Ÿå°†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åŠŸèƒ½é›†æˆåˆ°æ‚¨çš„åº”ç”¨ç¨‹åºä¸­ã€‚

# å°†æ¨¡å‹å¤åˆ¶åˆ°è®¾å¤‡

1. åœ¨æ‚¨çš„è®¡ç®—æœºä¸Š**ä¸‹è½½æ–‡ä»¶å¤¹**ä¸­è§£å‹ä¸‹è½½çš„æ¨¡å‹ï¼Œå¹¶è¿æ¥æ‚¨çš„ç§»åŠ¨è®¾å¤‡ã€‚
2. åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨**adb**è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
adb shell rm -r /data/local/tmp/llm/
adb shell mkdir -p /data/local/tmp/llm/ 
adb push gemma2b.bin /data/local/tmp/llm/gemma2b.bin
```
è¿™äº›å‘½ä»¤å°†æŠŠ**gemma2b.bin**æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°ä¸´æ—¶ç›®å½•ã€‚ç°åœ¨æ¨¡å‹å·²ä½äºæ­£ç¡®ä½ç½®ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¼–ç éƒ¨åˆ†äº†ã€‚

# å¼€å§‹ç¼–ç 

åœ¨ **AndroidManifest** æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ä»¥æ”¯æŒåŸç”Ÿåº“ï¼š

```python
<uses-native-library
            android:name="libOpenCL.so"
            android:required="false" />
<uses-native-library
            android:name="libOpenCL-car.so"
            android:required="false" />
<uses-native-library
            android:name="libOpenCL-pixel.so"
            android:required="false" />
```
åœ¨æ‚¨çš„ Android åº”ç”¨çš„ **build.gradle** æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹ä¾èµ–é¡¹ï¼š

```python
dependencies {
    implementation 'com.google.mediapipe:tasks-genai:0.10.14'
}
```

# LLMTask ç±»

æˆ‘ä»¬å®ç°çš„æ ¸å¿ƒç»„ä»¶æ˜¯ **LLMTask** ç±»ã€‚è¯¥ç±»è´Ÿè´£ LLM æ¨ç†çš„åˆå§‹åŒ–å’Œæ‰§è¡Œï¼Œç¡®ä¿æ¨¡å‹åœ¨è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œã€‚è®©æˆ‘ä»¬åˆ†è§£è¿™ä¸ªç±»çš„å…³é”®å…ƒç´ ï¼š

```python
class LLMTask(context: Context) {
    private val _partialResults = MutableSharedFlow<Pair<String, Boolean>>(
        extraBufferCapacity = 1,
        onBufferOverflow = BufferOverflow.DROP_OLDEST
    )
    val partialResults: SharedFlow<Pair<String, Boolean>> = _partialResults.asSharedFlow()
    private var llmInference: LlmInference

       init {
        val options = LlmInference.LlmInferenceOptions.builder()
            .setModelPath(MODEL_PATH)
            .setMaxTokens(2048)
            .setTopK(50)
            .setTemperature(0.7f)
            .setRandomSeed(1)
            .setResultListener { partialResult, done ->
                _partialResults.tryEmit(partialResult to done)
            }
            .build()

        llmInference = LlmInference.createFromOptions(
            context,
            options
        )
    }


    fun generateResponse(prompt: String) {
        llmInference.generateResponseAsync(prompt)
    }

    companion object {
        private const val MODEL_PATH = "/data/local/tmp/llm/gemma2b.bin"
        private var instance: LLMTask? = null
        fun getInstance(context: Context): LLMTask {
            return if (instance != null) {
                instance!!
            } else {
                LLMTask(context).also { instance = it }
            }
        }
    }
}
```

## å…³é”®ç»„ä»¶

1. **MutableSharedFlow å’Œ SharedFlowï¼š** è¿™äº›ç”¨äºç®¡ç†ä»LLMæ¨ç†ä¸­å¾—åˆ°çš„ä¸­é—´ç»“æœæµã€‚MutableSharedFlowå…è®¸æˆ‘ä»¬å‘é€æ–°çš„ç»“æœï¼Œè€ŒSharedFlowåˆ™å°†è¿™äº›ç»“æœæš´éœ²ç»™åº”ç”¨ç¨‹åºçš„å…¶ä»–éƒ¨åˆ†ã€‚
2. **LlmInference åˆå§‹åŒ–ï¼š** LlmInferenceå®ä¾‹é€šè¿‡é€‰é¡¹åˆå§‹åŒ–ï¼ŒåŒ…æ‹¬æ¨¡å‹è·¯å¾„ã€æœ€å¤§ä»¤ç‰Œæ•°å’Œä¸€ä¸ªå¤„ç†ä¸­é—´ç»“æœçš„ç»“æœç›‘å¬å™¨ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹é…ç½®é€‰é¡¹æ¥åˆå§‹åŒ–LlmInferenceï¼Œ

1. **modelPathï¼š** æ¨¡å‹åœ¨é¡¹ç›®ç›®å½•ä¸­çš„å­˜å‚¨è·¯å¾„ã€‚
2. **maxTokensï¼š** æ¨¡å‹å¤„ç†çš„æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆè¾“å…¥ä»¤ç‰Œ + è¾“å‡ºä»¤ç‰Œï¼‰ã€‚é»˜è®¤å€¼ä¸º512ã€‚
3. **topKï¼š** æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸€æ­¥è€ƒè™‘çš„ä»¤ç‰Œæ•°é‡ã€‚é™åˆ¶é¢„æµ‹ä¸ºæœ€å¯èƒ½çš„kä¸ªä»¤ç‰Œã€‚é»˜è®¤å€¼ä¸º40ã€‚
4. **temperatureï¼š** ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥çš„éšæœºæ€§é‡ã€‚è¾ƒé«˜çš„æ¸©åº¦ä¼šå¯¼è‡´ç”Ÿæˆæ–‡æœ¬æ›´å…·åˆ›é€ æ€§ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦åˆ™äº§ç”Ÿæ›´å¯é¢„æµ‹çš„ç”Ÿæˆç»“æœã€‚é»˜è®¤å€¼ä¸º0.8ã€‚
5. **randomSeedï¼š** æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨çš„éšæœºç§å­ã€‚é»˜è®¤å€¼ä¸º0ã€‚
6. **loraPathï¼š** è®¾å¤‡ä¸ŠLoRAæ¨¡å‹çš„ç»å¯¹è·¯å¾„ã€‚æ³¨æ„ï¼šè¿™ä»…å…¼å®¹GPUæ¨¡å‹ã€‚
7. **resultListenerï¼š** è®¾ç½®ç»“æœç›‘å¬å™¨ä»¥å¼‚æ­¥æ¥æ”¶ç»“æœã€‚ä»…åœ¨ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆæ–¹æ³•æ—¶é€‚ç”¨ã€‚
8. **errorListenerï¼š** è®¾ç½®ä¸€ä¸ªå¯é€‰çš„é”™è¯¯ç›‘å¬å™¨ã€‚

# ä½¿ç”¨LLMStateç®¡ç†çŠ¶æ€

```python
sealed class LLMState {
    data object LLMModelLoading : LLMState()
    data object LLMModelLoaded : LLMState()
    data object LLMResponseLoading : LLMState()
    data object LLMResponseLoaded : LLMState()

    val isLLMModelLoading get() = this is LLMModelLoading
    val isLLMResponseLoading get() = this is LLMResponseLoading
}
```
è¿™ä¸ªå¯†å°ç±»æœ‰åŠ©äºç®¡ç†å’Œå“åº”ä¸åŒçš„çŠ¶æ€ï¼Œä¾‹å¦‚æ¨¡å‹åŠ è½½ä¸­ã€æ¨¡å‹å·²åŠ è½½ä»¥åŠå“åº”ç”Ÿæˆæ—¶ã€‚

# ChatState ç±»

**ChatState** ç±»è´Ÿè´£ç»´æŠ¤èŠå¤©çŠ¶æ€ï¼ŒåŒ…æ‹¬ç”¨æˆ·æ¶ˆæ¯å’ŒLLMå“åº”ã€‚

```python
class ChatState(
    messages: List<ChatDataModel> = emptyList()
) {
    private val _chatMessages: MutableList<ChatDataModel> = messages.toMutableStateList()
    val chatMessages: List<ChatDataModel>
        get() = _chatMessages.map { model ->
            val isUser = model.isUser
            val prefixToRemove =
                if (isUser) USER_PREFIX else MODEL_PREFIX
            model.copy(
                chatMessage = model.chatMessage
                    .replace(
                        START_TURN + prefixToRemove + "\n",
                        ""
                    )
                    .replace(
                        END_TURN,
                        ""
                    )
            )
        }.reversed()

    val fullPrompt
        get() =
            _chatMessages.takeLast(5).joinToString("\n") { it.chatMessage }

    fun createLLMLoadingMessage(): String {
        val chatMessage = ChatDataModel(
            chatMessage = "",
            isUser = false
        )
        _chatMessages.add(chatMessage)
        return chatMessage.id
    }

    fun appendFirstLLMResponse(
        id: String,
        message: String,
    ) {
        appendLLMResponse(
            id,
            "$START_TURN$MODEL_PREFIX\n$message",
            false
        )
    }

    fun appendLLMResponse(
        id: String,
        message: String,
        done: Boolean
    ) {
        val index = _chatMessages.indexOfFirst { it.id == id }
        if (index != -1) {
            val newText = if (done) {
                _chatMessages[index].chatMessage + message + END_TURN
            } else {
                _chatMessages[index].chatMessage + message
            }
            _chatMessages[index] = _chatMessages[index].copy(chatMessage = newText)
        }
    }

    fun appendUserMessage(
        message: String,
    ) {
        val chatMessage = ChatDataModel(
            chatMessage = "$START_TURN$USER_PREFIX\n$message$END_TURN",
            isUser = true
        )
        _chatMessages.add(chatMessage)
    }

    fun addErrorLLMResponse(e: Exception) {
        _chatMessages.add(
            ChatDataModel(
                chatMessage = e.localizedMessage ?: "Error generating message",
                isUser = false
            )
        )
    }

    companion object {
        private const val MODEL_PREFIX = "model"
        private const val USER_PREFIX = "user"
        private const val START_TURN = "<start_of_turn>"
        private const val END_TURN = "<end_of_turn>"
    }
}
```

**å…³é”®æ–¹æ³•ï¼š**

1. **createLLMLoadingMessage**ï¼šå‘èŠå¤©çŠ¶æ€ä¸­æ·»åŠ ä¸€æ¡æ–°çš„åŠ è½½æ¶ˆæ¯å¹¶è¿”å›å…¶IDã€‚
2. **appendFirstLLMResponse å’Œ appendLLMResponse**ï¼šè¿™äº›æ–¹æ³•å¤„ç†å°†éƒ¨åˆ†å’Œå®Œæ•´çš„LLMå“åº”è¿½åŠ åˆ°èŠå¤©æ¶ˆæ¯ä¸­ã€‚
3. **appendUserMessage**ï¼šå°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©çŠ¶æ€ä¸­ã€‚
4. **addErrorLLMResponse**ï¼šå¦‚æœåœ¨LLMå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œåˆ™æ·»åŠ ä¸€æ¡é”™è¯¯æ¶ˆæ¯ã€‚
5. **fullPrompt**ï¼šæ‹¼æ¥æœ€å5æ¡æ¶ˆæ¯ï¼Œä¸ºLLMæä¾›æ›´å¥½çš„ä¸Šä¸‹æ–‡ã€‚

# ChatViewModel ç±»

ChatViewModel ç±»ç®¡ç† UI ä¸ LLM å¤„ç†é€»è¾‘ä¹‹é—´çš„äº¤äº’ã€‚å®ƒä½¿ç”¨ Kotlin åç¨‹æ¥ç®¡ç†å¼‚æ­¥ä»»åŠ¡ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–° UI çŠ¶æ€ã€‚

```python
@HiltViewModel
class ChatViewModel @Inject constructor(@ApplicationContext private val context: Context) :
    ViewModel() {
    private val _llmState = MutableStateFlow<LLMState>(LLMState.LLMModelLoading)
    val llmState = _llmState.asStateFlow()
    private val _chatState: MutableStateFlow<ChatState> = MutableStateFlow(ChatState())
    val chatState: StateFlow<ChatState> = _chatState.asStateFlow()

    fun initLLMModel() {
        viewModelScope.launch(Dispatchers.IO) {
            _llmState.emit(LLMState.LLMModelLoading)
            LLMTask.getInstance(context)
        }.invokeOnCompletion {
            _llmState.value = LLMState.LLMModelLoaded
        }
    }

    fun sendMessage(message: String) {
        viewModelScope.launch(Dispatchers.IO) {
            _chatState.value.appendUserMessage(message)
            try {
                _llmState.emit(LLMState.LLMResponseLoading)
                var currentLLMResponseId: String? = _chatState.value.createLLMLoadingMessage()
                LLMTask.getInstance(context).generateResponse(_chatState.value.fullPrompt)
                LLMTask.getInstance(context).partialResults
                    .collectIndexed { index, (partialResult, done) ->
                        currentLLMResponseId?.let { id ->
                            if (index == 0) {
                                _chatState.value.appendFirstLLMResponse(id, partialResult)
                            } else {
                                _chatState.value.appendLLMResponse(id, partialResult, done)
                            }
                            if (done) {
                                _llmState.emit(LLMState.LLMResponseLoaded)
                                currentLLMResponseId = null
                            }
                        }
                    }
            } catch (e: Exception) {
                _chatState.value.addErrorLLMResponse(e)
            }
        }
    }
}
```
**å…³é”®åŠŸèƒ½ï¼š**

1. **initLLMModel**ï¼šåˆå§‹åŒ– LLM æ¨¡å‹å¹¶ç›¸åº”åœ°æ›´æ–°çŠ¶æ€ã€‚
2. **sendMessage**ï¼šå¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œç”Ÿæˆ LLM å“åº”ï¼Œå¹¶ä½¿ç”¨éƒ¨åˆ†å’Œæœ€ç»ˆç»“æœæ›´æ–°èŠå¤©çŠ¶æ€ã€‚

**åœ¨èŠå¤©ç•Œé¢ä¸­çš„æ•´åˆ**

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5o4UFNFWN0B0_OzSZi9Bqg.gif)

# ä¼˜åŠ¿

**éšç§ä¿æŠ¤ï¼š** é€šè¿‡åœ¨è®¾å¤‡æœ¬åœ°å¤„ç†æ•°æ®ï¼Œè®¾å¤‡ç«¯å¤§å‹è¯­è¨€æ¨¡å‹å‡å°‘äº†å°†æ•æ„Ÿä¿¡æ¯é€šè¿‡äº’è”ç½‘ä¼ è¾“çš„éœ€æ±‚ï¼Œå¢å¼ºäº†ç”¨æˆ·éšç§ä¿æŠ¤ã€‚

**ç¦»çº¿åŠŸèƒ½ï¼š** è®¾å¤‡ç«¯å¤§å‹è¯­è¨€æ¨¡å‹æ— éœ€äº’è”ç½‘è¿æ¥å³å¯è¿è¡Œï¼Œä½¿ç”¨æˆ·å³ä½¿åœ¨ç¦»çº¿ç¯å¢ƒä¸‹ä¹Ÿèƒ½è®¿é—®è¯­è¨€å¤„ç†åŠŸèƒ½ã€‚

**ä½å»¶è¿Ÿï¼š** æœ¬åœ°å¤„ç†æ•°æ®å‡å°‘äº†å°†æ•°æ®å‘é€åˆ°è¿œç¨‹æœåŠ¡å™¨è¿›è¡Œå¤„ç†æ—¶çš„å»¶è¿Ÿï¼Œä»è€Œå®ç°æ›´å¿«çš„å“åº”æ—¶é—´ã€‚

**é™ä½æ•°æ®æˆæœ¬ï¼š** ç”¨æˆ·å¯ä»¥é¿å…å› å°†æ•°æ®å‘é€åˆ°è¿œç¨‹æœåŠ¡å™¨è¿›è¡Œå¤„ç†è€Œäº§ç”Ÿçš„æ•°æ®è´¹ç”¨ã€‚

**å®šåˆ¶åŒ–ï¼š** è®¾å¤‡ç«¯å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥æ ¹æ®ç‰¹å®šç”¨ä¾‹æˆ–è®¾å¤‡è¿›è¡Œå®šåˆ¶å’Œä¼˜åŒ–ï¼Œæä¾›æ›´å¤§çš„çµæ´»æ€§å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

**æˆæœ¬æ•ˆç›Šï¼š** é•¿æœŸæ¥çœ‹ï¼Œè®¾å¤‡ç«¯å¤„ç†æ›´å…·æˆæœ¬æ•ˆç›Šï¼Œå› ä¸ºå®ƒå‡å°‘äº†æ˜‚è´µæœåŠ¡å™¨åŸºç¡€è®¾æ–½å’Œæ•°æ®ä¼ è¾“æˆæœ¬çš„éœ€æ±‚ã€‚

# ç¼ºç‚¹

**æ¨¡å‹è§„æ¨¡ä¸å¤æ‚æ€§ï¼š** è®¾å¤‡ç«¯å¤„ç†è¦æ±‚å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜å‚¨åœ¨æœ¬åœ°è®¾å¤‡ä¸Šï¼Œç”±äºç°ä»£LLMçš„è§„æ¨¡å’Œå¤æ‚æ€§ï¼Œè¿™å¯èƒ½é¢‡å…·æŒ‘æˆ˜ã€‚è¾ƒå¤§çš„æ¨¡å‹éœ€è¦æ›´å¤šçš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—èµ„æºï¼Œè¿™å¯èƒ½å¯¹ä½ç«¯è®¾å¤‡é€ æˆå‹åŠ›ã€‚

**èµ„æºå¯†é›†å‹ï¼š** åœ¨è®¾å¤‡ä¸Šè¿è¡ŒLLMå¯èƒ½ä¼šæ¶ˆè€—å¤§é‡èµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤æ‚æ¨¡å‹æˆ–é•¿åºåˆ—ã€‚è¿™å¯èƒ½å¯¼è‡´ç”µæ± æ¶ˆè€—å¢åŠ å’Œæ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒæ—§æˆ–æ€§èƒ½è¾ƒå¼±çš„è®¾å¤‡ä¸Šã€‚

**æ¨¡å‹æ›´æ–°ï¼š** è·Ÿä¸ŠLLMæ¨¡å‹çš„æœ€æ–°è¿›å±•å’Œæ”¹è¿›å¯èƒ½é¢‡å…·æŒ‘æˆ˜ã€‚æ›´æ–°æ¨¡å‹éœ€è¦æ›´æ–°åº”ç”¨ç¨‹åºï¼Œè¿™å¯¹ç”¨æˆ·æ¥è¯´å¯èƒ½å¹¶éæ€»æ˜¯å¯è¡Œæˆ–å®é™…ã€‚

ğŸš€ å–œæ¬¢æˆ‘åœ¨æœ€æ–°Mediumæ–‡ç« ä¸­çš„è§è§£å—ï¼Ÿå¦‚æœä½ è§‰å¾—æœ‰å¸®åŠ©ï¼Œ**è¯·è€ƒè™‘ç»™äºˆæŒå£°ï¼ˆğŸ‘ï¼‰** å¹¶ä¸ä½ çš„ç½‘ç»œåˆ†äº«ã€‚

**åˆ«å¿˜äº†ç‚¹å‡»â€˜å…³æ³¨â€™æŒ‰é’®ã€‚** ğŸ“š è®©æˆ‘ä»¬ä¿æŒè”ç³»ï¼Œä¸€èµ·æ¢ç´¢æ›´å¤šï¼ ğŸš€ğŸ“Œ
