
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8Xi_ETkNJQz1-dA1oevO0A.jpeg
date: '2024-07-13 20:06:07'
tags:
  - 深度学习
  - 大型语言模型
  - 专家混合架构
title: 谷歌DeepMind的新研究利用数百万专家混合构建大规模LLMs

---


## 深入探讨百万专家混合（MoME）架构的开发，该架构在性能和计算效率上前所未有地超越了传统大型语言模型（LLM）



我们正身处一场大型语言模型（LLM）的竞争之中。

这可能并不显而易见，但所有大型科技公司都在争相开发超越现有模型的更优秀LLM。

[增加模型规模、数据集大小](https://arxiv.org/abs/2203.15556)以及计算量，[你就能得到比以往更优秀的模型](https://arxiv.org/abs/2001.08361)。

[基于这一规模化定律](https://arxiv.org/abs/2001.08361)，谷歌DeepMind的研究人员发现，以特定方式调整模型架构也能显著提升其性能和训练效率。

他们的洞察源于这样一个事实：作为LLM核心的Transformer架构，[其大部分事实知识存储](https://arxiv.org/abs/2104.08696)在密集的前馈（FFW）层中。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*00bT1Vxj3rxgX2SJvR4Wmg.png)

这些层占据了Transformer中三分之二的参数。

*而这正是一个问题。*

它大幅增加了传统Transformer的计算足迹，因为其计算量**与所含参数数量成线性正比**。

为解决这一问题并受[专家混合架构（MoE）](https://arxiv.org/abs/1701.06538)启发，研究人员开发了**PEER（参数高效专家检索）**，这是一种能融入现有Transformer架构并以前所未有的方式对其进行改进的新型层。

这一层随后可用于由**超过百万个微型专家**组成的专家混合LLM中！

他们在[ArXiv上的论文](https://arxiv.org/pdf/2407.04153)展示了这种**百万专家混合（MoME）**架构如何在保持计算效率的同时，超越传统LLM及先前参数较少但规模较大的MoE模型。

以下是深入探讨这一新型架构如何运作、其开发过程以及与先前LLM架构相比性能表现的故事。

# 让我们从混合专家模型开始

将计算成本与模型参数数量解耦的探索并非新鲜事。

2017年，[混合专家（Mixture-of-Experts，MoE）模型](https://arxiv.org/abs/1701.06538)被引入并普及。

[该模型](https://levelup.gitconnected.com/a-squad-of-open-source-llms-can-now-beat-the-closed-source-gpt-4o-86ebed788102#7580)结合了多个被称为**专家**的神经网络，这些专家专长于不同的技能集。

一个门控网络负责根据给定的查询选择性激活这些专家，并结合它们的加权输出以产生结果。

得益于这项研究，发现使用稀疏激活的专家模块而非密集的前馈网络（FFWs）更为高效。

随后，在2024年初，[针对这些MoE模型的缩放法则](https://arxiv.org/abs/2402.07871)被提出。

它们在数学上描述如下——

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_05hN8rMtmpY6cfBGNN9hg.png)

其中：

* `L(N,D,G)` 是MoE模型的最终测试损失
* `N` 是总的非嵌入参数数量
* `D` 是训练令牌的数量
* `G` 是粒度，即活跃专家的数量
* `a`, `b`, `g`, `γ`, `α`, `β` 是常数

根据这些法则，**增加模型大小、更多训练令牌以及更高的粒度（活跃专家数量）** 影响MoE模型，导致损失降低并使其更高效。

这揭示了一个重要洞见——

> *我们需要使用众多小型专家来扩展MoE架构，而非像之前那样使用少量大型专家。*

*那数百万个呢？*

# 向数百万专家路由的难题

将输入查询路由到百万级别的专家是一项艰巨的任务。

路由或从总数为 `N` 的专家中选择与给定查询相关的专家的过程如下：

首先，计算输入查询向量与每个专家的关键向量之间的内积。

这些得分决定了该专家对给定输入查询的相关性。

然后对这些得分进行排序，并选取前 `k` 个得分进行路由。

这一过程在计算上开销巨大，其复杂度随着专家总数 (`N`) 以及关键和查询向量的维度 (`d`) 的增长而增加。

研究人员通过引入一种新的路由层架构解决了这一问题。

接下来让我们详细探讨这一架构。

# 参数高效专家检索（PEER）层

Google DeepMind的研究人员通过构建一种名为**参数高效专家检索（PEER）**的新层架构，解决了路由效率低下的问题。

PEER层可以插入到Transformer主干架构中，或用于替换其前馈层。

这一构成百万专家混合（MoME）架构基础的层包含三个组成部分：

1. 专家池
2. 一组产品键（每个专家对应一个键），每个键决定每个专家对给定输入查询的相关性
3. 一个查询网络，功能是将输入向量映射到查询向量（用于与产品键比较，选择最相关的专家）

给定一个输入向量`x`，PEER层的运作方式如下：

1. 输入向量`x`传递到一个查询网络，生成查询向量`q(x)`
2. 计算此查询向量与产品键（维度均为`d`）的内积
3. 选择内积最高的`k`个专家

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZPK2lDclqCNX2pTxQr8CXQ.png)

4. 对这些前`k`个专家的内积应用非线性激活函数（Softmax或Sigmoid），生成**路由分数**，确定每个选定专家在最终输出中的重要性。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZL7iUn5F4Di7H-y2FV5Oiw.png)

5. 每个专家的输出乘以其对应的路由分数作为权重，这些值线性组合生成PEER层的最终输出。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1ReBadGNv6Ykn1p5tWh0Og.png)

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rb-PEc_X04TK5gC98qmjWg.png)

**现在，看步骤3的方程，我们计算top-k索引。这就是所有低效所在。**

这种方法的复杂度为`O(Nd)`，其中`d`是向量的维度，`N`是专家总数。

这一问题在[2019年发表于ArXiv的研究《具有产品键的大型记忆层》](https://arxiv.org/abs/1907.05242)中得到解决，研究人员设计了一种结构化记忆，可融入基于transformer的架构，提高其性能并减少推理时间。

该研究引入了高效产品键检索技术，这些技术被借鉴来构建PEER层。

其工作原理如下：

不计算`N`个不同的`d`维键向量，而是通过连接两组子键的向量来创建这些键。

这两组子键用`C`和`C'`表示，每个子键是`d/2`维，包含`√N`个子键。

这导致产品键集合`K`形成[笛卡尔积](https://en.wikipedia.org/wiki/Cartesian_product)结构。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n5omvOKFtvRoNuZOsLqAMw.png)

同样，查询向量`q(x)`被分成两个子查询向量`q(1)`和`q(2)`，每个子查询向量维度为`d/2`。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WElWFg7v3BoOkj9xC_vQaQ.png)

接下来，通过计算这些子键和子查询向量之间的内积来选择top-k专家。

这产生两组top-k子键：`I(C)`和`I(C')`。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jJouWJgj41DdT4TMegih0w.png)

然后，从这些集合中组合top-k子键，得到`k²`个候选键，这从数学上保证了与查询向量`q(x)`最相似的top `k`个键是该候选集的一部分。

此外，候选键与查询向量`q(x)`之间的内积简单地是子键和子查询之间内积的总和。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P9V6zR8XKqF-Z8pCqoui7A.png)

因此，可以再次对这些内积应用top-k操作符，从原始产品键集合(`K`)中获得top-k匹配键。

总的来说，在这种方法中，我们使用**笛卡尔积**，其中`N`个键从`√N`个子键高效生成。

由于现在只需与`√N`个子键而不是`N`个完整键比较，这显著将top-k选择过程的复杂度从`O(Nd)`降低到`O((√N + k²)d)`。

# 扩展PEER层至多头部

在传统的MoE（专家混合）模型中，每个专家的隐藏层大小与其他前馈层相同。

而百万专家混合（MoME）架构则采用了不同的方法。

在这里，每个专家是一个**单一隐藏层的MLP**，即它仅有一个隐藏层和一个神经元。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l05J2YFJr69qONVrcmIxlA.png)

为了提高粒度（*记住MoE的扩展定律*），MoME将单个专家的大小保持在最小，并通过多头部检索来增加其表达能力。

这与Transformer中的[多头注意力机制](https://arxiv.org/abs/1706.03762)类似。

不同于仅使用一个查询网络，多个独立的查询网络（头部）从共享的专家池（具有相同的产品键集合）中计算各自的查询，并检索一组不同的top-k专家。

这些头部的结果随后被汇总以产生最终输出。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Sl8hSXkRyu8G8JeFyMMvWw.png)

# PEER层前向传播的伪代码实现

让我们通过[原始研究论文](https://arxiv.org/pdf/2407.04153)中的描述来理解PEER层前向传播的伪代码。

这将帮助我们回顾步骤，更好地理解其工作原理。

```python
def peer_forward (self , x):
    # 存储所有专家的下/上投影权重的嵌入层
    self . w_down_embed = nn. Embed ( num_embeddings = self . n_experts , features = self . d_model )
    self . w_up_embed = nn. Embed ( num_embeddings = self . n_experts , features = self . d_model )
    
    # 使用乘积键检索匹配度最高的专家的权重
    # indices和scores的形状为'bthk'，其中h是头的数量
    indices , scores = self . get_indices ( self . query_proj (x), self . sub_keys , top_k = self .k)
    w_down = self . w_down_embed ( indices )
    w_up = self . w_up_embed ( indices )
    
    # 计算专家输出的加权平均
    x = jnp . einsum (’btd , bthkd -> bthk ’, x, w_down )
    x = self . activation (x)
    x = x * nn. softmax ( scores )
    x = jnp . einsum (’bthk , bthkd -> btd ’, x, w_up )
    return x

# bthkd表示batch_size, time_steps, num_heads, top_k, dimension
# einsum是爱因斯坦求和约定
```
`peer_forward`函数通过初始化存储所有专家下和上投影权重的嵌入层来工作。

（下投影权重将输入投影到较低维空间，而上投影权重将其投影回较高维空间。）

对于给定的输入，它使用查询网络找到匹配度最高的专家的索引和路由器分数。

然后将这些专家的投影权重存储到`w_down`和`w_up`中。

接着，输入通过[`ein`sum操作](https://mathworld.wolfram.com/EinsteinSummation.html)与下投影权重结合，并对结果应用非线性激活函数。

该结果乘以[Softmax](https://en.wikipedia.org/wiki/Softmax_function)归一化的路由器分数，然后通过另一个`einsum`（爱因斯坦求和）操作与上投影权重结合。

最后，这些处理后的输出相加，产生最终结果。

最后，让我们了解这种架构与其传统同类架构相比的性能表现。

# MoME架构的性能表现

MoME架构的性能表现相当出色，这在多项任务中得到了验证。

## 预训练 IsoFLOP 分析

[预训练](https://klu.ai/glossary/llm-pretraining) 指的是在大量未标记文本上对大型语言模型（LLM）进行初始训练的阶段。

通过此分析，研究人员在等量计算资源（因此称为 iso-[FLOP](https://en.wikipedia.org/wiki/FLOPS)）下，使用 [C4 数据集](https://huggingface.co/datasets/allenai/c4) 比较了 MoME 预训练性能与其他基准的表现。

比较的四种不同模型包括：

* 带有密集前馈层（FFW）的 LLM
* [带有 128 个专家和专家选择路由的混合专家模型（MoE）](https://arxiv.org/abs/2202.09368)（专家选择 top-k 令牌而非令牌选择 top-k 专家）
* [带有产品键记忆层（PKM）的 LLM](https://arxiv.org/abs/1907.05242)，具有 1024² 记忆
* 带有 PEER 层的 LLM（MoME 架构），具有 1024²（即 1,048,576）个专家

为了创建可比较的模型，首先选择了不同层数、注意力头和维度的密集模型。

然后，将 Transformer 中间块中的前馈层替换为 MoE、PKM 和 PEER 层。

结果显示，与其他密集 FFW 基准相比，其他模型将 isoFLOP 曲线**向右下方移动**。

这种向右下方的移动起初可能看似违反直觉，因为人们可能预期增加更多参数会增加计算成本（或将曲线向上和向右推）。

然而，事实并非如此，因为尽管这些稀疏模型引入了更多的总参数，它们仍然激活并利用其中的一小部分。

结果显示在下方的图中，可以看出**在相同的计算预算下，带有 PEER 层的 MoME 架构达到了最低的计算最优[困惑度](https://huggingface.co/docs/transformers/en/perplexity)**。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XswJWrGhOFJudQlhLBGUcA.png)

## 语言建模数据集上的性能表现

在根据isoFLOP分析选出计算最优模型后，这些模型的性能在多个流行的语言建模数据集上进行了评估。

结果显示，MoME架构再次相较于其他模型实现了最低的困惑度。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*s1Biw10w-ID_a2JslyfClw.png)

## MoME架构变体的性能表现

研究者接下来探讨了总专家数和活跃专家数的变化对困惑度的影响。

研究发现，单纯增加总专家数能提升性能，且无需额外成本。

然而，在保持总专家数不变的情况下改变活跃专家数，性能会在一定程度内提升，随后达到饱和并增加内存消耗。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*loB2Bd1MX3MsSkyS8bBGxg.png)

## 查询批量归一化的性能差异

研究者最终探讨了查询[批量归一化](https://en.wikipedia.org/wiki/Batch_normalization)（即在查询向量使用前对一批输入进行归一化处理）对模型困惑度的影响。

本分析中使用了两个额外指标：

1. **专家使用率**：指推理过程中活跃专家的数量。
2. [**KL散度**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)** / 不均匀性**：衡量给定概率分布与[均匀分布](https://en.wikipedia.org/wiki/Continuous_uniform_distribution)的偏离程度。

结果显示，使用查询批量归一化能更均衡地利用专家，并获得更低的困惑度。

值得注意的是，即使专家数量超过百万，模型的专家使用率仍接近100%。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1xRcwn-us6tA_jXcZGytVw.png)

此外，isoFLOP曲线表明，模型在使用查询批量归一化时，通常能实现更低的困惑度（尤其是在isoFLOP最优区域附近）。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HirusvvIDJsQMSQv2NEbnA.png)

***这篇研究论文的结果非常出色，我对这将如何积极改变LLM的未来感到非常乐观。***

***对此你有何看法？欢迎在下方评论区留言。***

# 深入阅读

* [*题为‘百万专家混合模型’的研究论文，发表于ArXiv*](https://arxiv.org/abs/2407.04153)
* [*题为‘带乘积键的大型记忆层’的研究论文，发表于ArXiv*](https://arxiv.org/abs/1907.05242)
* [*题为‘异常庞大的神经网络：稀疏门控专家混合层’的研究论文，发表于ArXiv*](https://arxiv.org/abs/1701.06538)
* [*题为‘采用专家选择路由的专家混合模型’的研究论文，发表于ArXiv*](https://arxiv.org/abs/2202.09368)
* [*题为‘细粒度专家混合模型的扩展定律’的研究论文，发表于ArXiv*](https://arxiv.org/abs/2402.07871)

## 以下是我的邮件列表链接，如果您想持续关注我的工作 ——
