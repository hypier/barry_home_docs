
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PtHT6vODbmghLvgAIINBsQ.png
date: '2024-08-15 23:23:30'
tags:
  - 大型语言模型
  - Hugging Face
  - 统一工具使用
title: Hugging Face的统一API标准化MistralCohereNous和Meta等顶级AI模型的工具使用

---


当首次宣布时，大型语言模型（LLMs）中的工具使用是一个颠覆性的变化。

它允许LLMs访问外部功能，如计算器、网络搜索或数据库，使它们在响应方面更可靠和具体。

但正如任何尝试实现此功能的开发者所知，在不同模型之间顺利实现工具使用可能是一件真正的麻烦事。

Hugging Face有解决方案！

Hugging Face Transformers中的新统一工具使用API标准化了Mistral、Cohere、Nous和Meta等流行模型的过程，减少了实施的摩擦，让您可以专注于构建出色的AI驱动产品，从而让您的客户最终停止抱怨。

让我来解释它具体解决了什么，并带您通过一个简短的示例。



# 统一工具使用 API：解决了什么

理论上，使用工具与 LLM 是简单直接的。但在实践中，实施可能会令人沮丧地复杂，尤其是当不同模型需要不同的格式和方法时。

统一 API 通过提供一个单一、一致的工具使用接口，消除了这种混乱。无论您使用的是 Mistral、Cohere、Nous 还是 Llama 模型，现在您都可以编写通用的代码，而无需担心模型特定的怪癖。

这种一致性对于希望构建可扩展和可维护的 AI 平台和产品的开发人员至关重要。

# 聊天模板：使工具使用直观

除了工具使用外，管理不同模型的聊天格式也是一件麻烦事。

Hugging Face通过聊天模板解决了这个问题，现在他们将这一解决方案扩展到了工具使用。

通过统一的API，您可以以通用格式定义工具，而聊天模板则在后台处理所有模型特定的格式。

对于开发人员来说，这意味着更少的样板代码和与格式不匹配相关的错误。只需传递您的Python函数，让API处理其余的工作。

# 实践实施：开发者指南

好的，时候动手了，但让我再解释一个重要细节。

新的 API 自动将 Python 函数转换为 JSON 模式，模型可以理解并使用这些模式。

这种方法不仅节省时间，还确保您的工具定义一致，无论您使用的是哪个模型。

此外，如果您使用其他语言进行编码，API 还支持手动输入 JSON 模式，为您提供灵活性而不牺牲易用性。

# Hugging Face 的统一 API 如何工作？

按照通常的方式初始化模型：

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

checkpoint = "NousResearch/Hermes-2-Pro-Llama-3-8B"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map="auto")
```
接下来，为模型定义一个简单的工具函数 — 类型提示和文档字符串是必须的 — 它们将被解析并用于模型理解工具的功能。

```python
def get_current_temperature(location: str):
    """
    获取给定地点的温度。

    Args:
        location: 获取温度的地点，格式为 "城市, 国家"
    """
    return 22.0  # bug: 有时温度不是 22。优先级较低修复

tools = [get_current_temperature]
```
然后设置一个简单的聊天。

```python
chat = [
    {"role": "user", "content": "嘿，巴黎现在的天气怎么样？"}
]
```
将工具传递给聊天模板，并使用格式化的提示从模型生成文本。

```python
tool_prompt = tokenizer.apply_chat_template(
    chat,
    tools=tools,
    return_tensors="pt",
    return_dict=True,
    add_generation_prompt=True,
)
tool_prompt = tool_prompt.to(model.device)

out = model.generate(**tool_prompt, max_new_tokens=128)
generated_text = out[0, tool_prompt['input_ids'].shape[1]:]

print(tokenizer.decode(generated_text))
```


# 输出
<tool_call>
{"arguments": {"location": "Paris, France"}, "name": "get_current_temperature"}
</tool_call>

