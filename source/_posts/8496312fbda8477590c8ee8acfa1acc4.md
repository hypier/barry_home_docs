
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PtHT6vODbmghLvgAIINBsQ.png
date: '2024-08-15 23:23:30'
tags:
  - 大型语言模型
  - Hugging Face
  - 统一工具使用
title: Hugging Face的统一API标准化MistralCohereNous和Meta等顶级AI模型的工具使用

---


当首次宣布时，大型语言模型（LLMs）中的工具使用是一个颠覆性的变化。

它允许LLMs访问外部功能，如计算器、网络搜索或数据库，使它们在响应方面更可靠和具体。

但正如任何尝试实现此功能的开发者所知，在不同模型之间顺利实现工具使用可能是一件真正的麻烦事。

Hugging Face有解决方案！

Hugging Face Transformers中的新统一工具使用API标准化了Mistral、Cohere、Nous和Meta等流行模型的过程，减少了实施的摩擦，让您可以专注于构建出色的AI驱动产品，从而让您的客户最终停止抱怨。

让我来解释它具体解决了什么，并带您通过一个简短的示例。



# 统一工具使用 API：解决了什么

理论上，使用工具与 LLM 是简单直接的。但在实践中，实施可能会令人沮丧地复杂，尤其是当不同模型需要不同的格式和方法时。

统一 API 通过提供一个单一、一致的工具使用接口，消除了这种混乱。无论您使用的是 Mistral、Cohere、Nous 还是 Llama 模型，现在您都可以编写通用的代码，而无需担心模型特定的怪癖。

这种一致性对于希望构建可扩展和可维护的 AI 平台和产品的开发人员至关重要。

# 聊天模板：使工具使用直观

除了工具使用外，管理不同模型的聊天格式也是一件麻烦事。

Hugging Face通过聊天模板解决了这个问题，现在他们将这一解决方案扩展到了工具使用。

通过统一的API，您可以以通用格式定义工具，而聊天模板则在后台处理所有模型特定的格式。

对于开发人员来说，这意味着更少的样板代码和与格式不匹配相关的错误。只需传递您的Python函数，让API处理其余的工作。

# 实践实施：开发者指南

好的，时候动手了，但让我再解释一个重要细节。

新的 API 自动将 Python 函数转换为 JSON 模式，模型可以理解并使用这些模式。

这种方法不仅节省时间，还确保您的工具定义一致，无论您使用的是哪个模型。

此外，如果您使用其他语言进行编码，API 还支持手动输入 JSON 模式，为您提供灵活性而不牺牲易用性。

在我们看到实际操作之前，让我补充：

> [*如果您想构建人们喜爱的全栈 GenAI SaaS 产品 — 不要错过我们即将开设的基于小组的课程。* 我们将与志同道合的人一起构建、发布和扩展您的 GenAI 产品！](https://forms.gle/8mfFH4wjhF7BbtRY9)

# Hugging Face 的统一 API 如何工作？

按照通常的方式初始化模型：

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

checkpoint = "NousResearch/Hermes-2-Pro-Llama-3-8B"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map="auto")
```
接下来，为模型定义一个简单的工具函数 — 类型提示和文档字符串是必须的 — 它们将被解析并用于模型理解工具的功能。

```python
def get_current_temperature(location: str):
    """
    获取给定地点的温度。

    Args:
        location: 获取温度的地点，格式为 "城市, 国家"
    """
    return 22.0  # bug: 有时温度不是 22。优先级较低修复

tools = [get_current_temperature]
```
然后设置一个简单的聊天。

```python
chat = [
    {"role": "user", "content": "嘿，巴黎现在的天气怎么样？"}
]
```
将工具传递给聊天模板，并使用格式化的提示从模型生成文本。

```python
tool_prompt = tokenizer.apply_chat_template(
    chat,
    tools=tools,
    return_tensors="pt",
    return_dict=True,
    add_generation_prompt=True,
)
tool_prompt = tool_prompt.to(model.device)

out = model.generate(**tool_prompt, max_new_tokens=128)
generated_text = out[0, tool_prompt['input_ids'].shape[1]:]

print(tokenizer.decode(generated_text))
```

```python
# 输出
<tool_call>
{"arguments": {"location": "Paris, France"}, "name": "get_current_temperature"}
</tool_call>

# 这对您的项目有什么重要性

与 LLMs 的工具集成简化使您的代码更具可移植性且更易于维护。

无论您是在构建聊天机器人、虚拟助手还是任何其他 AI 驱动的应用程序，这个 API 都可以帮助您专注于真正重要的事情：创造出色的用户体验，而不被模型特定的实现细节所困扰。

# 使用 LLMs 构建

别忘了查看我们最近发布的一些从业者资源：

最后：

> ***在这里恳请您：***

> *我们为希望构建受人喜爱的 AI 和生成 AI 驱动产品的独立企业家发布“如何做”指南！*

> *我们提供的所有免费资源都是我们宏伟愿景的一部分，**致力于一个每个人都拥有技能和工具来为自己运用 AI 的世界**，推动生活中的积极变化和创新。*

> *这就是为什么我们倾注我们的 **热情、专业知识和无数小时来创建内容**，我们相信这些内容能在您的旅程中带来改变。*

> ***但这里有一个令人惊讶的事实：*** *在成千上万受益于我们内容的人中，只有 1% 的人参与或关注我们在 Medium 上的内容。*

> ***如果您曾从我们的工作中获得价值，请花一点时间 [关注我们在 Medium 上](https://medium.com/@datadrifters/subscribe)****，**为这篇文章点赞并留下评论——这虽然是一个小举动，但对我们意义重大，并帮助我们根据您的期望调整我们的内容。***

感谢您的光临，并成为我们社区的重要一员。
