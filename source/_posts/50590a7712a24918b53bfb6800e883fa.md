
---
cover: http://placehold.it/16x9
date: '2024-05-26 22:46:18'
tags:
  - 自我纠正
  - 语言模型
  - 信息检索
title: 解密 PDF 解析 03无 OCR 的小型模型方法

---
想过如何让你的大型语言模型（LLMs）更聪明、更可靠吗？想象一个不仅能检索信息，还能自我纠正以提供准确响应的系统。欢迎来到具有自我纠正功能的高级检索增强生成（RAG）的世界。

在本文中，我们将探讨使用 LangGraph 构建自适应和自我反思 RAG 应用程序的过程。目标是构建一个能够自我批评以防止幻觉的系统，集成强大的代理，并利用 GROQ 减少 LLM 响应的延迟。

有关动手教程，请查看我的 YouTube 视频，我将深入讲解每个步骤：[https://youtu.be/GXRveOki4kE](https://youtu.be/GXRveOki4kE)

代码：[https://github.com/Eduardovasquezn/advanced-rag-app?tab=readme-ov-file](https://github.com/Eduardovasquezn/advanced-rag-app?tab=readme-ov-file)



## 理解 RAG

RAG 将信息检索的能力与文本生成相结合，增强了 LLM 的功能。

想象一下一个 LLM，它可以与您的自定义数据进行交互并检索任何类型的信息。无论是查询您的 SQL 数据库、从 PDF 文档中提取数据，还是从网络中筛选非结构化数据，RAG 都使这一切成为可能。

## 此项目的技术栈：

**LangGraph:** 用于构建自适应 RAG 的框架。

**Mixtral-8x7b**: 用于生成响应的 LLM。

**GROQ:** 一种通过减少延迟来加速 LLM 响应推理的技术。

**Tavily:** 一种基于代理的动态信息检索工具，用于执行网络搜索。

**Google Embeddings:** 用于生成文本嵌入。

**Chroma:** 一个用于高效数据管理的向量存储数据库。

## 工作流程

此流程图提供了一个具有自我纠正能力的自适应RAG系统的详细可视化。以下是每个组件及整体过程的逐步解释：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GpM1E4OnaaaWRLU9cydleQ.png)

**1\. 问题路由节点**

系统以问题路由节点开始，根据问题与存储在Chroma中的数据的相关性来指引初始问题。如果问题与我们数据库中的文档相关，则沿着检索路径进行；否则，启动网络搜索。

为此，我利用了模型包装器来格式化LLM的输出，以匹配指定的模式。通过利用Pydantic类，我创建了一个RouteQuery类，以确保路由决策清晰且结构化。以下是类定义：

```
class RouteQuery(BaseModel):
    """将用户查询路由到最相关的数据源。"""
    datasource: Literal["vectorstore", "websearch"] = Field(
        ...,
        description="根据用户问题选择将其路由到网络搜索或向量存储。",
    )
llm = ChatGroq(temperature=0)
structured_llm_router = llm.with_structured_output(RouteQuery)
```

**2\. 检索器节点**

我将与代理相关的非结构化数据插入到我的Chroma数据库中，并将所有文本转换为嵌入。这使得每当我提出问题时，我可以从数据库中检索到最相似的文档。此设置允许系统根据所提问题有效地检索相关文档，利用嵌入在数据库中找到最上下文相似的信息。

初始问题被转换为文本嵌入并发送到检索器。检索器访问向量存储，这本质上是一个已经存储嵌入的数据库，以寻找相关文档。它返回问题以及与查询匹配的过滤文档。

```
vectorstore = Chroma.from_documents(
documents=doc_splits,
collection_name="adv-rag-chroma",
embedding=embeddings)
retriever = vectorstore.as_retriever()
```

**3\. 文档评分节点**

检索到的文档随后由LLM进行评分，以评估其相关性。此评分过程产生一组精炼的文档，并确定是否需要进行网络搜索。如果文档被认为不相关，则将进行网络搜索。从网络搜索中检索到的信息将被插入到向量存储数据库中。

此包装器设置在LLM上，以提供一个二进制评分，指示检索文档与给定问题的相关性。此类中的二进制评分属性指示文档是否相关（“是”）或不相关（“否”）。

```
class GradeDocuments(BaseModel):
"""用于检查检索文档相关性的二进制评分。"""
binary_score: str = Field(description="文档与问题相关，'是'或'否'")
structured_llm_grader_docs = llm.with_structured_output(GradeDocuments)
```

**4\. 生成答案节点**

如果评分的文档被认为足够，LLM将基于检索到的信息生成答案。

我构建了一个链，以根据前一个节点提供的文档和提示生成答案。此外，输出被呈现为字符串。

```
prompt = ChatPromptTemplate.from_template(
"""你是一个问答任务的助手。使用以下检索到的上下文来回答问题。如果你不知道答案，只需说你不知道。最多使用三句话，并保持答案简洁。
问题: {question}
上下文: {context}
答案:""")

rag_chain = prompt | llm | StrOutputParser()
```

**5\. 自我纠正机制**

**检查幻觉：** 生成的答案会进行幻觉检查，即不准确或无意义的信息。

- **没有幻觉：** 如果答案准确，则继续进行下一个检查。
- **有幻觉：** 如果检测到幻觉，系统将返回生成节点以生成新答案。

此类用于结构化LLM的输出，具体关注生成答案中是否存在幻觉。

```
class GradeHallucinations(BaseModel):
"""用于检查生成答案中是否存在幻觉的二进制评分。"""
binary_score: str = Field(description="不要考虑调用外部API以获取额外信息。答案由事实支持，'是'或'否'。")
structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations)
```

**6\. 验证最终答案**

**是否回答了问题？**

- **是：** 如果答案正确地解决了问题，则将其提供给用户。
- **否：** 如果答案不充分，系统将进行网络搜索以获取额外信息。

我在LLM上设置了这个包装器，以结构化响应的输出。LLM将用户提出的问题和生成的答案作为输入。

```
class GradeAnswer(BaseModel):
"""用于评估答案是否解决问题的二进制评分。"""
binary_score: str = Field(description="答案解决了问题，'是'或'否'")
structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)
```

**7\. 网络搜索集成**

**网络搜索路径：** 如果问题与存储在Chroma中的数据无关，初始检索未能提供足够的信息，或检测到幻觉，系统将执行网络搜索。

网络搜索返回额外文档，这些文档随后被集成到向量存储数据库中，从而提升系统的未来性能。

此代码用于使用Tavily搜索获取3个结果

```
web_search_tool = TavilySearchResults(k=3)
```

使用LangGraph，我设计了自我反思RAG的工作流程。在这里，我定义了节点、边和条件语句，以将初始问题路由到网络搜索或检索器。同时检查LLM是否存在幻觉，以及最终答案是否回应了初始问题。

```
workflow = StateGraph(GraphState)

workflow.add_node("websearch", web_search)
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_edge("websearch", "generate")
workflow.add_edge("retrieve", "grade_documents")

workflow.set_conditional_entry_point(
route_question,
{
"websearch": "websearch",
"vectorstore": "retrieve",
})
workflow.add_conditional_edges(
"grade_documents",
decide_to_generate,
{
"websearch": "websearch",
"generate": "generate",
})
workflow.add_conditional_edges(
"generate",
grade_generation_v_documents_and_question,
{
"not supported": "generate",
"useful": END,
"not useful": "websearch",
})

app = workflow.compile()
```

示例：

```

from pprint import pprint
inputs = {"question": "2023年NBA选秀中，哪位球员被选为首选？"}
for output in app.stream(inputs):
for key, value in output.items():
pprint(f"完成运行: {key}:")
pprint(value["generation"])
```

```
- -路由问题 - -
 - -将问题路由到网络搜索 - -
 - -网络搜索。附加到向量存储数据库 - -
'完成运行: websearch:'
 - -生成答案 - -
 - -检查幻觉 - -
 - -决策：生成基于文档 - -
 - -评分生成与问题 - -
 - -决策：生成解决了问题 - -
'完成运行: generate:'
('根据提供的上下文，维克托·温班亚马被选为2023年NBA选秀的首选。')
```

## 结论

总之，RAG 应用通过允许自定义数据的信息检索与文本生成，增强了 LLM 的能力。利用 LangGraph、Tavily、Mixtral-8x7b、GROQ 和 Chroma 等工具，我构建了一个强大的自适应 RAG 应用框架。该框架使 LLM 能够动态检索多样化的数据源，自主优化响应，并通过自我纠正机制确保可靠性。通过无缝集成网络搜索功能，该系统持续学习和演变，为更可靠的 AI 驱动应用奠定了基础。
