
---
categories: 人工智能
cover: https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mc1s1BZr_HKp-az5
date: '2024-08-14 01:01:33'
tags:
  - AI推理
  - Grokking
  - 语言模型
title: Grokking，一种新的推理方式

---
Grokking，一种新的推理形式



这里有一个大胆的声明：这篇文章将使你变得更加“AI聪明”，同时揭示当今前沿AI模型的粗糙现实：**它们其实相当愚蠢。**

你可能已经厌倦了关于大型语言模型（LLMs）有多智能的无数声明，但这都是一种被夸大的谬论。虽然它们可以非常有用，**但它们并不是新闻所描述的聪明角色。**

但一种叫做grokking的技术可能会通过训练小型玩具LLM，使其比花费数亿美元训练的前沿AI模型聪明得多，从而更为显著地改变这一现状。

*但这怎么可能呢？*


> **厌倦了无意义的炒作？**


> 这篇文章，以及每周更多内容，最初发布在我的通讯中，这是AI高管和分析师们想要了解炒作背后真相、发现趋势并利用它们的地方。

# 将记忆与推理混淆

AI可以推理的方式有四种。

## 四种选择，残酷的现实

1. **隐性推理：** 当模型建立了内部推理电路，使其能够进行“天生推理”。这类似于驾驶汽车；你在无意识中执行智能行为；这些对你来说是自然的。
2. **显性推理：** 当我们引导模型明确地表达推理时，比如要求它“逐步解决问题”。让模型“解释它的想法”会导致更好的推理。

> 有一个有趣的事实。Anthropic 的 Claude 模型通过系统提示生成“思考”。这些是包含在条款中的响应用户的部分，对用户隐藏，但有助于它更好地回应。

**3. 少样本推理：** 当我们提供推理链作为上下文时。例如，当我们在提示中给出“如何推理”的示例。通过提供模仿的示例，这正是模型将要复制的内容。

**4. 主动搜索推理** 发生在我们允许模型生成和验证多个可能的解决方案，直到它们确定最佳方案时。

今天，我们的大部分努力围绕第二和第三个选项。然而，在这两种情况下，结果相当平庸，正如我们稍后将看到的，**明显劣于我们今天所看到的方法**。

另一个有前景的方向是搜索增强推理，这是一种“给模型思考时间”的方式。

长期以来被视为 LLM 的圣杯，[我在我的通讯中提供了更多细节](https://thetechoasis.beehiiv.com/p/ai-truly-intelligent)，但这个想法是通过允许模型探索可能解决方案的空间，它们模仿我们人类在思考问题时所做的相同过程，心理学家称之为“系统 2 思维”或“有意识思维”（有意识是指“深思熟虑”，而不是它们是有意识的存在）。

然而，这种范式所需的计算和内存要求的巨大规模使得它今天仍然是一个遥不可及的梦想。

> 此外，**这些模型存在验证问题**。虽然它们在每个想法可以自动验证的领域提供了极具前景的结果（例如，如果我试图解决一个可以运行测试来验证生成的代码片段的代码问题），但在开放性问题上它们大多未被探索。

> 一些人（OpenAI/Google）测试使用其他 LLM 来“验证想法”，[但正如 Google Deepmind 自身证明的那样](https://arxiv.org/pdf/2407.21787)，这种方法很快就会达到瓶颈。

> 例如，使用搜索创建文本的最佳摘要。这种主观验证并不容易解决。

因此，考虑到第二和第三个选项非常有限，而第四个选项过于复杂或昂贵，这使我们只剩下第一个选项：**隐性推理**。

## AI是否能够隐式推理？

根据它们的回答，你可能会觉得它们已经是很好的推理者。但事实远非如此。

虽然通过搜索接近系统2无疑是最终需要发生的事情，**我们仍然尚未训练出好的“内在推理者”，** 即那些本质上聪明的模型，或者更具体地说，能够有效地对其*“已知的已知”*进行推理的模型。

> 区分系统1和系统2思维的一种方式是，系统1是在过去的经验和知识上执行智能的、有理由的行动，而系统2则需要主动搜索推理，以找到对你来说并不自然或自动的答案。

我有很多证据表明当前的模型无法对其知识进行推理。事实上，你可以自己检查一下。

例如，它们总是失败的一种常见推理类型是**组合推理**，当提供两个相关事实时，模型必须推断出第三个事实。

例如：

1. 巴拉克·奥巴马的妻子是米歇尔。
2. 米歇尔出生于1964年。

看到这一点，如果我们问模型推断巴拉克的妻子出生于何时（1964年），只有少数模型能答对。但如果我们提高难度，情况就会变得相当糟糕。

这是几乎所有当前LLM在第一次尝试时都失败的提示：

*“爱丽丝有10个兄弟和10个姐妹。任何一个兄弟有多少个姐妹？”* 再次，这是一道组合练习。模型有两个事实，必须基于这两个事实推断出第三个事实。

这是GPT-4o给我的答案：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CFOMAzjLsvDbGmcQ)

你明白发生了什么，对吧？模型无法得出爱丽丝也必须算作姐妹组的一部分的结论。

接下来，让我们试着通过让它采取逐步的方法来引导它得出答案，也就是“口头推理”方法（经典的思维链方法）：

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-Artmv6zldyHanTn)

注意到这个模式了吗？模型没有意识到爱丽丝也是姐妹之一，尽管根据我给出的事实这显而易见。

虽然Claude 3.5 Sonnet答对了（如下），但几天前，当模型无法判断“9.11”是大于还是小于“9.9”时，引起了轰动。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qGW5FZxkzXIBO7beP8fWvA.png)

但是在更复杂的组合测试中，所有LLM都毫无例外地失败。因此，如果它们无法进行如此简单的推理，我们很快就会意识到**LLM只是在重复东西，而并不真正理解它们**。

因此，*我们是否一直被科技巨头误导，以为LLM比实际更聪明？*

长话短说，是的，但这并不意味着我们不能通过一种叫做**grokking**的技术来解决这个问题。

# 理解，与数据融为一体

就像其他任何人工智能一样，LLMs 也使用标准程序进行训练。

## 十年的方法

1. 我们尽可能多地收集数据，并将其分为两类：训练和测试。
2. 我们在训练集上训练模型，然后在“未见过”的测试数据上进行测试。如果模型在测试数据上表现良好，说明它从训练中学到了有用的东西，使其能够在“未见过的数据”上表现良好。这个想法被称为“泛化”。

> 这里有一个关键点需要注意：泛化有不同的层次。

> 通常，测试数据几乎总是来自与训练数据相同的分布（即训练数据是狗的照片，测试数据是新的狗）。

> 分布外泛化，即模型学习到的先验不仅能转移到未见过的数据，还能转移到截然不同的数据（举例来说，训练狗，测试猫），在今天的人工智能中仍然是一个未解决的问题，即使对于大型语言模型（LLMs）也是如此。

传统上，在这个过程中，关键是要避免“过拟合”，这是一种现象，当**我们过度延长训练，使模型记住训练数据**，使其无法对测试集进行泛化。

> 通过在数据上训练过久，会引导模型得出荒谬的具体结论。例如，如果模型只见过哈士奇的照片并且过拟合，它将只接受其他哈士奇作为“狗”。

因此，历史上，研究人员旨在找到欠拟合和过拟合之间的最佳平衡。再次以狗为例，我们的目标是找到模型学习到一般狗属性（四条腿、多毛、爪子……）作为所有狗的特征的时刻，而不是假设所有狗都像哈士奇那样。

> 这个问题被称为“方差/偏差权衡”。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4msEQpI_amRodM6A)

但有一个问题。如果我们只是轻微地让模型接触数据以防止过拟合，模型似乎只是在学习数据的“整体”。

而这就是“grokking”出现的地方。这是今天人工智能中最反直觉的想法之一，**但它可能是突破性的。**

## 理解的艺术

理解完全打破了之前的方法，通过**将训练延伸到过拟合点之外。**

起初，这可能看起来像是一种罪过，但它隐藏着一个神奇的秘密：通过足够的理解，模型将学会在深度上进行泛化，远远超过未理解模型的水平。

正如我们在下面的图像中看到的，当我们对模型进行过拟合时（红色曲线达到100%准确率的时刻），测试集上的准确率非常低。

然而，如果我们继续在过拟合的数据上进行训练，突然间，模型开始在测试集上表现良好，并且在推理比较问题的情况下（右侧），模型甚至学会了进行分布外比较（稍后会详细讨论）。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oUmEZCQfmBDykKnu)

**而这种训练的过度延伸就是我们所称之为理解**。起初，这可能没有意义。*一个已经记住训练数据的模型如何在不同数据上表现良好？*

最简短的回答是**模型已经与数据“融为一体”。** *但这是什么意思？*

## 给模型学习的时间

就像让 LLMs 在回答之前寻找解决问题的不同方法类似于给模型 *‘思考的时间，’* grokking 可以被视为给模型 *‘学习的时间。’*

传统训练方法的问题在于，如果我们达到过拟合并停止训练，模型确实已经记住了训练数据。因此，我们只是简单地假设过拟合是需要避免的事情。

但是如果我们延长训练，类似于让模型 *‘不断撞击它已经知道的数据，’* 会发生一些神奇的事情。随着时间的推移，**模型学习到更简单的方法来得出相同的结论。**

更技术性的解释是，今天大多数 AI 训练围绕正则化展开，类似于奥卡姆剃刀。通俗地说，解决问题的方案越简单（所需假设越少），该方案就越好，**因为模型只学习保留决定解决方案的关键元素。**

与所有事物一样，grokking 中发生的事情通过一个例子更容易理解。假设你训练一个模型来识别照片中的猫，为此你提供：

* 99 张深色猫的照片，它们是四条腿的，有毛的，有胡须，眼睛呈裂缝状，还有尾巴，
* 以及 1 张无毛猫的照片（如斯芬克斯猫）。

如果模型对数据过拟合，它将假设所有猫都是深色的，四条腿的，有毛的，并且有胡须、裂缝状的眼睛和尾巴。

*而斯芬克斯猫呢？* 它已经对数据过拟合，所以它仍然会争辩说它也是一只猫，但没有比“仅仅因为”更好的解释。

但是如果我们继续在这些数据上训练模型更长时间，模型最终会找到一个更简单的答案来回答“*什么是猫？*”

例如，**它可能得出结论，拥有胡须、裂缝状的眼睛和尾巴是将动物标记为猫的充分条件，** 而四条腿、有毛和特定深色是常见特征，但并不具有普遍性，因为并不是所有猫都是深色或有毛的。

由于这个解决方案更简单但足够，它更有可能很好地泛化。确实，尽管斯芬克斯猫不是有毛的或深色的，**在模型在 grokking 过程中达到的更简单定义下，斯芬克斯猫也是一只猫。**

简而言之，通过重复查看相同的数据，就像一个人多次阅读这篇 [研究论文](https://arxiv.org/pdf/2405.15071)（这就是我，哈哈），模型最终会对数据形成更全面、合理和简单的理解。

因此，通过 grokking，**模型从记忆数据转向理解数据。**

*但是它们是如何做到的？*

## 机械解释

在变换器中，输入标记（发送到模型的单词）经过一系列更新：

* 注意机制使用其周围的上下文（序列中的其他单词）更新每个标记（假设一个标记是一个单词）
* MLP 层将模型的核心知识嵌入到单词中（即，如果单词是“Michael Jordan”，模型将提供篮球信息，即使序列中没有“篮球”这个词）。

这一过程重复多次，直到模型对序列中的下一个单词有了良好的理解。

例如，在下面的图像中，我们可以看到一个“拒绝电路”，显示 LLM 如何意识到输入序列是一个危险请求，因此应该拒绝回答。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A9rk_QrGLgHIg7YbcfF_NQ.png)

在前沿 AI 模型中，这种“更新”进行数十次，构建出更复杂、更强大的特征，如上图所示。

> 在今天的案例中，他们使用了一个仅有 8 层的 GPT-2 级变换器。

有趣的是，您可以“干预”这些模型，并观察每个单词的含义如何随着每次新更新而“变化”，或者用更“AI 风格”的术语来说，我们可以观察在任何给定时刻每个单词指向（或关注）其他哪些单词或语义。

让我们通过一个组合示例来看看这一点。对于两个给定的明确事实，模型必须能够推断出第三个事实。例如：

**问题：** 阿尔伯特·爱因斯坦在 1904 年还活着吗？

知道：

1. 阿尔伯特·爱因斯坦是汉斯·阿尔伯特·爱因斯坦的父亲。
2. 汉斯·阿尔伯特·爱因斯坦于 1904 年出生。

回答：是的，阿尔伯特·爱因斯坦在 1904 年还活着。

在这里，我们有：

* 两个事实（如上所列）
* 事实之间的桥梁（汉斯·阿尔伯特·爱因斯坦）
* 推断出的事实是，基于与他儿子的关系，阿尔伯特确实在 1904 年还活着。

知道这一点后，研究人员观察了模型在整个训练过程中的内部电路。当模型第一次过拟合时，他们很快意识到它在记忆这些序列。

*但他们是怎么知道的？*

具体来说，**模型直接避免了桥梁（汉斯）**，并在训练期间每当看到前两个事实时自动预测推断出的事实。

如果模型已经见过这个组合，这不是问题（在这种情况下，我们无法分辨模型是否记住了这个组合），但当面对类似但新的组合时，模型无法预测新的推断事实，因为它完全忽略了连接两个事实并使第三个事实（这种关系的结果）的桥梁。

换句话说，它记住了这三个事实之间的关系，而不是理解第三个事实是基于前两个事实之间的关系推断出来的。

但是当他们扩展训练（理解）时，情况发生了变化。以下图像作为参考：

* 在早期层中，模型检索到第一个事实（阿尔伯特是汉斯的父亲）和桥梁（汉斯，连接两个事实）。
* 在第 5 层中，第一个事实（r1）的值已更新为直接指向桥梁。

简单来说，第一个事实中的单词直接指向（或关注）汉斯。直观上，模型基于提供的信息意识到“汉斯”是连接两个事实的桥梁。

> 请记住，在理解之前，桥梁是完全被避免的。

* 重要的是，事实 2（r2，或汉斯于 1904 年出生的事实）的更新值继续指向自身，确保模型不会忘记第二个事实。
* 最后，在后面的层中，模型检索第二个事实，并使用桥梁指向推断出的事实（表示为“t”）。换句话说，在最后的层中，模型不是记住第三个事实，而是主动推断它。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa_oGuV7bfyTCDVOhHWC9g.png)

*那么这一复杂过程意味着什么？*

简而言之，**模型已经：**

* 构建了一个可以接受两个事实的组合推理电路，
* 识别它们之间的关系（桥梁），
* 并利用这个连接推断出第三个事实，**即使在它从未见过的情况下。**

换句话说，它不是在记忆关系，而是在理解它。

因此，它不会跳过步骤（与它记住数据时不同，因为它不需要桥梁来找到推断出的事实，因为它已经记住了）。

现在，它不仅内化了推理过程（它期待桥梁），**而且遵循与人类相同的步骤**（找到桥梁，从而推断出第三个事实），模仿推理过程。

*但是理解是否“言行一致”？* 当然是的。

## 令人印象深刻的结果

与前沿的AI模型不同，一个简单的grokked GPT-2级模型（一个来自2019年的模型，按今天的标准来看是史前的）**完全击败了GPT-4 Turbo和Gemini 1.5 Pro**，尽管这两者都使用了检索增强生成来改善结果。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZyKlmMPKs-lNcycjH-DNXw.png)

尽管他们没有发布评估数据集，但任务与下面的任务类似，模型必须连接多个实体并识别复杂的关系，比如*“谁更老，Rick还是John？”*

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DO7HaED5nZYgsO5FARZKzQ.png)


> 有趣的是，GPT-4o和Claude 3.5 Sonnet都能正确回答这个问题，但在更复杂的问题上表现不佳，而grokked transformer则轻松通过了测试。

这些数据不容忽视，我预计grokked模型将很快成为一个普遍主题，这是我们都将很快听到的一个关键趋势。

# 它会变得普遍吗？

从纯技术的角度来看，这是一个非常相关的成就——不是在使用的架构上，而是在我们如何训练模型上。

看到一个经过理解的 GPT-2 级别模型（15 亿参数）在推理问题上击败了前沿模型，后者的规模大到千倍，我**不会感到惊讶，如果许多实验室很快发布 20 到 100 亿参数范围内的理解模型。**

> 考虑到埃隆·马斯克的模型被称为“Grok”，明显是对这种方法的引用，埃隆是否在准备一次盛大的亮相？

至于市场，它们并没有将理解模型的价值纳入考虑。理解模型可能是最终的压缩因子，**创造出比前沿模型小几个数量级的强大推理器，因此运行成本也更低。**

如果这一范式实现，许多投资者将质疑大型科技公司在 AI 硬件和数据中心土地及设备上每年花费的 2100 亿美元的资本支出（根据上个季度的数字预测）是否值得。

![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*grPM0hAM8i0BQhQG)

然而，这些公司也可以辩称，训练理解模型需要更长的训练时间，因此仍然有理由进行辩护。

话虽如此，有一点是肯定的：世界上没有任何投资者现在在问这个问题，这可能仅仅通过阅读这篇文章就能给你带来优势。

> **有关 AI 战略或分析的商业咨询，请联系 nacho@thewhitebox.ai**

> 如果你喜欢这篇文章，我在我的 [LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/) 上以更全面和简化的方式免费分享类似的想法。

> 如果你愿意，可以通过 [X](https://twitter.com/TheTechOasis1) 与我联系。
